{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "asian-contrary",
   "metadata": {},
   "source": [
    "May 4, 2021\n",
    "\n",
    "Example of Baseline vs IB-GAN using the UCR dataset SpokenArabicDigits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "regular-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "equal-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sustained-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from numpy import zeros, ones, expand_dims\n",
    "from numpy.random import randn, randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding, Dense, LSTM, Multiply, Add, Lambda\n",
    "from keras.layers import Reshape, Flatten, Activation, Concatenate\n",
    "from keras.layers import Conv1D, ZeroPadding1D, MaxPooling1D, Conv2D, Conv2DTranspose, TimeDistributed\n",
    "from keras.layers import BatchNormalization, Dropout, LeakyReLU, RepeatVector, ReLU, GlobalAveragePooling1D\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    "from keras.metrics import Precision, Recall, AUC\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, auc, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score, accuracy_score, f1_score\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-organizer",
   "metadata": {},
   "source": [
    "# Load & Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "accessory-panama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6599, 93, 13) (6599, 1) (2199, 93, 13) (2199, 1)\n"
     ]
    }
   ],
   "source": [
    "#State UCR dataset\n",
    "filefolder = \"SpokenArabicDigits\"\n",
    "trainx_key = 'UCR/' + filefolder + '/X_train.npy'\n",
    "trainy_key ='UCR/' + filefolder + '/y_train.npy'\n",
    "testx_key ='UCR/' + filefolder + '/X_test.npy'\n",
    "testy_key ='UCR/' + filefolder + '/y_test.npy'\n",
    "\n",
    "x_train = np.load(trainx_key)\n",
    "y_train = np.load(trainy_key)\n",
    "x_test = np.load(testx_key)\n",
    "y_test = np.load(testy_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "persistent-reproduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([660, 660, 660, 660, 660, 660, 660, 660, 660, 659]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See class frequencies \n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fifth-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduce class imbalance\n",
    "np.random.seed(2021)\n",
    "minority_index = [k for k in range(len(y_train)) if y_train[k] in [0, 1, 2, 3, 4]]\n",
    "toRemove = [i for i in minority_index if np.random.rand(1) < 0.75]\n",
    "x_train = np.delete(x_train, toRemove, axis=0)\n",
    "y_train = np.delete(y_train, toRemove, axis=0)\n",
    "\n",
    "minority_index = [k for k in range(len(y_test)) if y_test[k] in [0, 1, 2, 3, 4]]\n",
    "toRemove = [i for i in minority_index if np.random.rand(1) < 0.75]\n",
    "x_test = np.delete(x_test, toRemove, axis=0)\n",
    "y_test = np.delete(y_test, toRemove, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "steady-interview",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([158, 175, 162, 176, 176, 660, 660, 660, 660, 659]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See class frequencies after imbalance\n",
    "uniq_vals, uniq_counts = np.unique(y_train, return_counts=True)\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "seeing-librarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2.6240506329113926, 1: 2.369142857142857, 2: 2.5592592592592593, 3: 2.355681818181818, 4: 2.355681818181818, 5: 0.6281818181818182, 6: 0.6281818181818182, 7: 0.6281818181818182, 8: 0.6281818181818182, 9: 0.6291350531107739}\n"
     ]
    }
   ],
   "source": [
    "#Set class weights\n",
    "weights = np.zeros(len(uniq_vals))\n",
    "class_weight = {}\n",
    "for i in list(range(len(uniq_vals))):\n",
    "    weights[i] = y_train.shape[0]/(len(uniq_vals) * uniq_counts[i])\n",
    "    class_weight.update({uniq_vals[i] : weights[i]})\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "extra-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameters\n",
    "n_classes = len(uniq_vals)\n",
    "n_epochs= 10\n",
    "n_samples = max(int(x_train.shape[0]/5), 2*n_classes)\n",
    "p_missing = 0.1\n",
    "\n",
    "\n",
    "#Set parameters of data\n",
    "max_sequence_length = x_train.shape[1]\n",
    "n_features = x_train.shape[2]\n",
    "k = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "illegal-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create onehot\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-civilian",
   "metadata": {},
   "source": [
    "# 1. Baseline Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "miniature-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "developed-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_Classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 93, 13)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 93, 26)            702       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 93, 26)            104       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 46, 26)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 46, 1)             53        \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 46, 1)             4         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 23, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                480       \n",
      "_________________________________________________________________\n",
      "cnn_labels (Dense)           (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,553\n",
      "Trainable params: 1,499\n",
      "Non-trainable params: 54\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def single_CNN(ts_shape = (max_sequence_length, n_features)):\n",
    "    # time series input\n",
    "    in_ts = Input(shape=ts_shape)\n",
    "    filter1 = n_features * 2\n",
    "    fmaps = n_classes*2\n",
    "    fe = Conv1D(filter1, kernel_size=k, padding='same', input_shape = (max_sequence_length, n_features))(in_ts)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = MaxPooling1D(pool_size=(2))(fe)\n",
    "    fe = Conv1D(1, kernel_size=k, padding='same', input_shape = (max_sequence_length, n_features))(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = MaxPooling1D(pool_size=(2))(fe)\n",
    "    #Fully connected layers\n",
    "    fe = Flatten()(fe)\n",
    "    fe = Dense(fmaps)(fe)\n",
    "    out_labels = Dense(n_classes, activation='softmax', name=\"cnn_labels\")(fe)\n",
    "\n",
    "    model = Model(inputs=in_ts, outputs=out_labels, name=\"CNN_Classifier\")\n",
    "    opt = Adam()\n",
    "    losses = {\"cnn_labels\":'categorical_crossentropy'}\n",
    "    model.compile(loss=losses, optimizer=opt, metrics = ['categorical_accuracy']) \n",
    "    model.summary() #prints out layers of model\n",
    "    return model\n",
    "\n",
    "baseline_classifier = single_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "alert-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_Classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 93, 13)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 93, 26)            702       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 93, 26)            104       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 46, 26)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 46, 1)             53        \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 46, 1)             4         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 23, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                480       \n",
      "_________________________________________________________________\n",
      "cnn_labels (Dense)           (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,553\n",
      "Trainable params: 1,499\n",
      "Non-trainable params: 54\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "4146/4146 [==============================] - 3s 835us/step - loss: 1.7960 - categorical_accuracy: 0.3970\n",
      "              ClassifierType  Accuracy  Precision    Recall  BalancedAccuracy  \\\n",
      "0  Baseline No Class Weights  0.669798   0.485344  0.471718          0.471718   \n",
      "\n",
      "   F1-score  \n",
      "0  0.455928  \n"
     ]
    }
   ],
   "source": [
    "#Run one iteration of baseline classifier\n",
    "tf.keras.backend.clear_session()\n",
    "random.seed(123)\n",
    "baseline_classifier = single_CNN()\n",
    "baseline_classifier.fit(x_train, y_train_onehot,verbose=1)\n",
    "pred_cnn = baseline_classifier.predict(x_test)\n",
    "rounded_cnn_labels= np.argmax(pred_cnn, axis=1)\n",
    "\n",
    "baseline_metrics = pd.DataFrame({'ClassifierType': \"Baseline No Class Weights\",\n",
    "                            'Accuracy': accuracy_score(y_test, rounded_cnn_labels), \n",
    "                             'Precision': precision_score(y_test, rounded_cnn_labels, average=\"macro\"),\n",
    "                             'Recall': recall_score(y_test, rounded_cnn_labels, average=\"macro\"),\n",
    "                             'BalancedAccuracy': balanced_accuracy_score(y_true=y_test, y_pred = rounded_cnn_labels),\n",
    "                             'F1-score': f1_score(y_true=y_test, y_pred=rounded_cnn_labels, average=\"macro\")\n",
    "                            }, index=[0])\n",
    "print(baseline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "wound-knowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_Classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 93, 13)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 93, 26)            702       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 93, 26)            104       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 46, 26)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 46, 1)             53        \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 46, 1)             4         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 23, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                480       \n",
      "_________________________________________________________________\n",
      "cnn_labels (Dense)           (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,553\n",
      "Trainable params: 1,499\n",
      "Non-trainable params: 54\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "4146/4146 [==============================] - 4s 850us/step - loss: 1.9024 - categorical_accuracy: 0.3763\n",
      "                ClassifierType  Accuracy  Precision    Recall  \\\n",
      "0  Baseline with Class Weights  0.611272   0.551179  0.569021   \n",
      "\n",
      "   BalancedAccuracy  F1-score  \n",
      "0          0.569021  0.526474  \n"
     ]
    }
   ],
   "source": [
    "#Run one iteration of baseline classifier\n",
    "tf.keras.backend.clear_session()\n",
    "random.seed(123)\n",
    "baseline_classifier = single_CNN()\n",
    "baseline_classifier.fit(x_train, y_train_onehot,class_weight = class_weight, verbose=1)\n",
    "pred_cnn = baseline_classifier.predict(x_test)\n",
    "rounded_cnn_labels= np.argmax(pred_cnn, axis=1)\n",
    "\n",
    "baseline_metrics = pd.DataFrame({'ClassifierType': \"Baseline with Class Weights\",\n",
    "                            'Accuracy': accuracy_score(y_test, rounded_cnn_labels), \n",
    "                             'Precision': precision_score(y_test, rounded_cnn_labels, average=\"macro\"),\n",
    "                             'Recall': recall_score(y_test, rounded_cnn_labels, average=\"macro\"),\n",
    "                             'BalancedAccuracy': balanced_accuracy_score(y_true=y_test, y_pred = rounded_cnn_labels),\n",
    "                             'F1-score': f1_score(y_true=y_test, y_pred=rounded_cnn_labels, average=\"macro\")\n",
    "                            }, index=[0])\n",
    "print(baseline_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-shopping",
   "metadata": {},
   "source": [
    "# 2. Imputation Balanced GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "noble-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_classifier(ts_shape=(max_sequence_length, n_features),\n",
    "                           hint_shape = (max_sequence_length, n_features)):\n",
    "    # time series input\n",
    "    in_ts = Input(shape=ts_shape)\n",
    "    in_hint = Input(shape= hint_shape)\n",
    "    filter1 = n_features * 2\n",
    "    fmaps = n_classes*2\n",
    "    fe = Conv1D(filter1, kernel_size=k, padding='same', input_shape = (max_sequence_length, n_features))(in_ts)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = MaxPooling1D(pool_size=(2))(fe)\n",
    "    fe = Conv1D(1, kernel_size=k, padding='same', input_shape = (max_sequence_length, n_features))(fe)\n",
    "    fe = BatchNormalization()(fe)\n",
    "    fe = MaxPooling1D(pool_size=(2))(fe)\n",
    "    #Fully connected layers\n",
    "    fe = Flatten()(fe)\n",
    "    fe = Dense(fmaps)(fe)\n",
    "    out_labels = Dense(n_classes, activation='softmax', name=\"labels_output\")(fe)\n",
    "    model = Model(inputs=[in_ts, in_hint], outputs=out_labels, name=\"Labels_Classifier\")\n",
    "    opt = Adam()\n",
    "    losses = {\"labels_output\":'categorical_crossentropy'}\n",
    "    model.compile(loss=losses, optimizer=opt, metrics = ['categorical_accuracy']) \n",
    "    model.summary() #prints out layers of model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "subsequent-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(ts_shape=(max_sequence_length, n_features), \n",
    "                           hint_shape = (max_sequence_length, n_features)):\n",
    "    # time series input\n",
    "    in_ts = Input(shape=ts_shape)\n",
    "    in_hint = Input(shape= hint_shape)\n",
    "    in_merged = Concatenate(axis=2)([in_ts, in_hint])\n",
    "    filter1 = n_features * 2\n",
    "    fe = Conv1D(filter1, kernel_size=k, padding=\"same\", activation='relu', \n",
    "                input_shape = (max_sequence_length, n_features))(in_merged)\n",
    "    fe = Conv1D(n_features, kernel_size=k, padding=\"same\",\n",
    "                input_shape = (max_sequence_length, n_features))(fe)\n",
    "    # real/fake output\n",
    "    out_realfake = Activation('sigmoid', name=\"realfake_output\")(fe)\n",
    "    model = Model(inputs=[in_ts, in_hint], outputs=out_realfake, name=\"RealFake_Discriminator\")\n",
    "    # compile model\n",
    "    opt = Adam()\n",
    "    losses = {\"realfake_output\":'binary_crossentropy'}\n",
    "    model.compile(loss=losses, optimizer=opt, metrics = ['binary_accuracy']) \n",
    "    model.summary() #prints out layers of model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "returning-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator_impute(missing_shape = (max_sequence_length, n_features),\n",
    "                            mask_shape = (max_sequence_length, n_features),\n",
    "                            labels_shape = (n_classes, ),\n",
    "                            hint_shape = (max_sequence_length, n_features)):\n",
    "    # time series input\n",
    "    in_ts = Input(shape=missing_shape)\n",
    "    in_hint = Input(shape= hint_shape)\n",
    "\n",
    "    #Other inputs\n",
    "    in_labels = Input(shape = labels_shape)\n",
    "    labels_tile = RepeatVector(max_sequence_length)(in_labels)\n",
    "    in_mask = Input(shape=mask_shape)\n",
    "    in_merged = Concatenate(axis=2)([in_ts, in_hint, labels_tile])\n",
    "    \n",
    "    filter1 = n_features * 2\n",
    "    fe = Conv1D(filter1, kernel_size=k, padding=\"same\", activation='relu', \n",
    "                input_shape = (max_sequence_length, n_features))(in_merged)\n",
    "    fe = Conv1D(n_features, kernel_size=k, padding=\"same\",\n",
    "                input_shape = (max_sequence_length, n_features))(fe)\n",
    "    fake_ts = fe \n",
    "    Masked_Data = Multiply()([in_ts, in_mask])\n",
    "    Reversed_Mask = Lambda(lambda x: 1. - x)(in_mask)\n",
    "    Imputed_Vals = Multiply()([fake_ts, Reversed_Mask])\n",
    "    imputed_ts = Add(name = \"fakets_output\")([Masked_Data, Imputed_Vals]) #Note that we replaced noise with values from our imputed copy\n",
    "    # define model\n",
    "    model = Model(inputs = [in_ts, in_hint, in_mask, in_labels], outputs = [imputed_ts, in_hint], name=\"Generator\") \n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "alpine-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model, c_model):\n",
    "    # make weights in the discriminator AND classifier not trainable\n",
    "    for layer in d_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False  \n",
    "    discriminator_output = d_model(g_model.output)\n",
    "    classifier_output = c_model(g_model.output)\n",
    "    model = Model(inputs = g_model.input, output = [classifier_output, discriminator_output])\n",
    "    # compile model\n",
    "    opt = Adam()\n",
    "    twolosses = {\"Labels_Classifier\":'binary_crossentropy', \"RealFake_Discriminator\":'binary_crossentropy'}\n",
    "    twolossesweights = {\"Labels_Classifier\": 1, \"RealFake_Discriminator\": 1}\n",
    "    model.compile(loss=twolosses, optimizer=opt, loss_weights = twolossesweights)\n",
    "    model.summary()    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "downtown-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_toImpute(x_train, y_train, p_missing, n_samples=n_samples, \n",
    "                             n_features = n_features, max_sequence_length=max_sequence_length):\n",
    "    #1. Reweighted Sampling\n",
    "    uniq_classes, uniq_counts = np.unique(y_train, return_counts=True)\n",
    "    class_probs = uniq_counts/len(y_train)\n",
    "    inv_probs = (1/class_probs)/(sum(1/class_probs))\n",
    "    sample_counts = np.round(inv_probs * n_samples)\n",
    "    sample_counts[len(sample_counts) - 1] = n_samples - sum(sample_counts[0:(len(sample_counts) - 1)])                           \n",
    "    ix_bal = []\n",
    "    for i in uniq_classes:\n",
    "        index_class = [k for k in range(len(y_train)) if y_train[k] == i]\n",
    "        index_sample = random.choices(index_class, k=int(sample_counts[int(i)]))\n",
    "        ix_bal.append(index_sample)\n",
    "    \n",
    "    ix_bal = list(chain(*ix_bal))\n",
    "    \n",
    "    impute_y_label = y_train[ix_bal]\n",
    "    impute_y_label = tf.keras.utils.to_categorical(impute_y_label, num_classes=n_classes)\n",
    "    \n",
    "    #2. Random Masking under MCAR scheme\n",
    "    #Create mask matrix: M_{ij} = 1 mean data is real\n",
    "    impute_mask = np.random.rand(max_sequence_length * n_features * n_samples) > p_missing\n",
    "    impute_mask = 1*impute_mask.reshape(n_samples, max_sequence_length, n_features)\n",
    "    \n",
    "    #Mask the true data with noise values \n",
    "    impute_noise = np.random.rand(max_sequence_length * n_features * n_samples)\n",
    "    impute_noise = impute_noise.reshape(n_samples, max_sequence_length, n_features)\n",
    "    impute_x_train = x_train[ix_bal]\n",
    "    impute_z_input = impute_x_train * (impute_mask) + impute_noise * (1-impute_mask)\n",
    "    \n",
    "    return impute_z_input, impute_mask, impute_y_label, impute_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "inner-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hint_matrix(mask_matrix, p_hint=0.8):\n",
    "    hints = np.random.rand(mask_matrix.shape[0] * mask_matrix.shape[1] * mask_matrix.shape[2]) > p_missing\n",
    "    hints = 1*hints.reshape(mask_matrix.shape[0], mask_matrix.shape[1], mask_matrix.shape[2])\n",
    "    hint_matrix = hints*mask_matrix\n",
    "    return(hint_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fancy-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_minibatch(X_both, labels_both, hint_both):\n",
    "    p = np.random.permutation(X_both.shape[0])\n",
    "    X_both = X_both[p]\n",
    "    labels_both = labels_both[p]\n",
    "    hint_both = hint_both[p]\n",
    "    return X_both, labels_both, hint_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "entitled-third",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 24 µs, total: 43 µs\n",
      "Wall time: 75.6 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def train(g_model, d_model, c_model, gan_model, \n",
    "          x_train, y_train, x_test, n_epochs, p_missing,\n",
    "          n_features = n_features, max_sequence_length = max_sequence_length, n_samples=n_samples):\n",
    "\n",
    "    n_steps = int((x_train.shape[0]/n_samples) * n_epochs)\n",
    "    n_burn = 10\n",
    "    # manually enumerate epochs\n",
    "    print(\"Number of steps total: \", n_steps)\n",
    "    for i in range(n_steps):  \n",
    "        \n",
    "        # Select a random batch of true data\n",
    "        idx = np.random.randint(0, x_train.shape[0], n_samples)\n",
    "        true_samples = x_train[idx]\n",
    "\n",
    "        true_labels = y_train[idx]\n",
    "        true_labels = tf.keras.utils.to_categorical(true_labels, num_classes=n_classes)\n",
    "        \n",
    "        #Select generator input:\n",
    "        z_input, z_mask, z_labels, z_train = sample_toImpute(x_train=x_train, y_train=y_train,\n",
    "                                                             p_missing = p_missing,n_samples=n_samples, \n",
    "                                                             n_features = n_features,\n",
    "                                                             max_sequence_length=max_sequence_length)\n",
    "        #Discriminator ground truths\n",
    "        true_M = np.ones((n_samples, max_sequence_length, n_features))\n",
    "        fake_M = z_mask\n",
    "        true_hint = sample_hint_matrix(true_M, p_hint=0.5)\n",
    "        z_hint = sample_hint_matrix(z_mask, p_hint=0.5)\n",
    "\n",
    "        #Get generator prediction\n",
    "        gen_samples, gen_hint = generator.predict([z_input, z_hint, z_mask, z_labels])\n",
    "        \n",
    "        # Update discriminator\n",
    "        d_loss_real = discriminator.train_on_batch([true_samples, true_hint], true_M)\n",
    "        d_loss_fake = discriminator.train_on_batch([gen_samples, gen_hint], fake_M)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        \n",
    "        # Train the classifier after burning the first n iterations\n",
    "        if i <= n_burn:\n",
    "            c_loss = classifier.train_on_batch([true_samples, true_hint], true_labels)\n",
    "        if i > n_burn:\n",
    "            samples_both = np.concatenate((true_samples, gen_samples), axis=0)\n",
    "            labels_both = np.concatenate((true_labels, z_labels), axis=0)\n",
    "            hint_both = np.concatenate((true_hint, z_hint), axis=0)\n",
    "            samples_both, labels_both, hint_both = shuffle_minibatch(samples_both, labels_both, hint_both)\n",
    "            c_loss = classifier.train_on_batch([samples_both, hint_both], labels_both)\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = gan_model.train_on_batch([z_input, z_hint, z_mask, z_labels], [z_labels, true_M])\n",
    "        \n",
    "        # State losses\n",
    "        print (\"%d [D loss: %f] [G loss: %f] [C loss: %f]\" % (i, d_loss[0], g_loss[0], c_loss[0]))\n",
    "\n",
    "        if i  == (n_steps -1):\n",
    "            test_M = np.ones((x_test.shape[0], max_sequence_length, n_features))\n",
    "            test_hint = sample_hint_matrix(test_M, p_hint = 0.5)\n",
    "            pred_realfake = d_model.predict([x_test, test_hint])\n",
    "            pred_labels = c_model.predict([x_test, test_hint])\n",
    "    return pred_realfake, pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-sustainability",
   "metadata": {},
   "source": [
    "#### Train IB-GAN for one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "extra-moscow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Labels_Classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 93, 13)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 93, 26)            702       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 93, 26)            104       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 46, 26)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 46, 1)             53        \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 46, 1)             4         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 23, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                480       \n",
      "_________________________________________________________________\n",
      "labels_output (Dense)        (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,553\n",
      "Trainable params: 1,499\n",
      "Non-trainable params: 54\n",
      "_________________________________________________________________\n",
      "Model: \"RealFake_Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 93, 26)       0           input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 93, 26)       1378        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 93, 13)       689         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "realfake_output (Activation)    (None, 93, 13)       0           conv1d_4[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,067\n",
      "Trainable params: 2,067\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 93, 10)       0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 93, 36)       0           input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 93, 26)       1898        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 93, 13)       689         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 93, 13)       0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 93, 13)       0           input_5[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 93, 13)       0           conv1d_6[0][0]                   \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fakets_output (Add)             (None, 93, 13)       0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,587\n",
      "Trainable params: 2,587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 93, 10)       0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 93, 36)       0           input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 93, 26)       1898        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 93, 13)       689         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 93, 13)       0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 93, 13)       0           input_5[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 93, 13)       0           conv1d_6[0][0]                   \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fakets_output (Add)             (None, 93, 13)       0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Labels_Classifier (Model)       (None, 10)           1553        fakets_output[0][0]              \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "RealFake_Discriminator (Model)  (None, 93, 13)       2067        fakets_output[0][0]              \n",
      "                                                                 input_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,207\n",
      "Trainable params: 4,086\n",
      "Non-trainable params: 2,121\n",
      "__________________________________________________________________________________________________\n",
      "Number of steps total:  500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.749327] [G loss: 1.129638] [C loss: 2.777773]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.720249] [G loss: 1.089097] [C loss: 2.769699]\n",
      "2 [D loss: 0.690112] [G loss: 1.038612] [C loss: 2.671327]\n",
      "3 [D loss: 0.660841] [G loss: 1.003426] [C loss: 2.617783]\n",
      "4 [D loss: 0.631245] [G loss: 0.969167] [C loss: 2.556081]\n",
      "5 [D loss: 0.602717] [G loss: 0.927545] [C loss: 2.584717]\n",
      "6 [D loss: 0.575001] [G loss: 0.885251] [C loss: 2.528491]\n",
      "7 [D loss: 0.547903] [G loss: 0.849937] [C loss: 2.411498]\n",
      "8 [D loss: 0.522560] [G loss: 0.817405] [C loss: 2.410905]\n",
      "9 [D loss: 0.496404] [G loss: 0.779333] [C loss: 2.355428]\n",
      "10 [D loss: 0.472498] [G loss: 0.756427] [C loss: 2.339181]\n",
      "11 [D loss: 0.448339] [G loss: 0.717516] [C loss: 2.331836]\n",
      "12 [D loss: 0.424922] [G loss: 0.679316] [C loss: 2.241491]\n",
      "13 [D loss: 0.402016] [G loss: 0.654098] [C loss: 2.241865]\n",
      "14 [D loss: 0.381279] [G loss: 0.616325] [C loss: 2.218970]\n",
      "15 [D loss: 0.360904] [G loss: 0.576536] [C loss: 2.143676]\n",
      "16 [D loss: 0.341720] [G loss: 0.557010] [C loss: 2.134474]\n",
      "17 [D loss: 0.324679] [G loss: 0.529212] [C loss: 2.112467]\n",
      "18 [D loss: 0.307246] [G loss: 0.505032] [C loss: 2.114999]\n",
      "19 [D loss: 0.292885] [G loss: 0.478755] [C loss: 2.062670]\n",
      "20 [D loss: 0.278579] [G loss: 0.457633] [C loss: 2.037954]\n",
      "21 [D loss: 0.267828] [G loss: 0.434000] [C loss: 1.999915]\n",
      "22 [D loss: 0.257481] [G loss: 0.424494] [C loss: 2.009230]\n",
      "23 [D loss: 0.249096] [G loss: 0.403486] [C loss: 2.011390]\n",
      "24 [D loss: 0.242336] [G loss: 0.381468] [C loss: 1.947167]\n",
      "25 [D loss: 0.235433] [G loss: 0.368143] [C loss: 1.944830]\n",
      "26 [D loss: 0.231108] [G loss: 0.363101] [C loss: 1.935854]\n",
      "27 [D loss: 0.225715] [G loss: 0.345234] [C loss: 1.887245]\n",
      "28 [D loss: 0.222508] [G loss: 0.338456] [C loss: 1.903852]\n",
      "29 [D loss: 0.221114] [G loss: 0.331138] [C loss: 1.868458]\n",
      "30 [D loss: 0.217754] [G loss: 0.322834] [C loss: 1.868093]\n",
      "31 [D loss: 0.216377] [G loss: 0.316819] [C loss: 1.859424]\n",
      "32 [D loss: 0.213773] [G loss: 0.318219] [C loss: 1.863580]\n",
      "33 [D loss: 0.212682] [G loss: 0.305853] [C loss: 1.820045]\n",
      "34 [D loss: 0.210636] [G loss: 0.304717] [C loss: 1.814980]\n",
      "35 [D loss: 0.209587] [G loss: 0.297806] [C loss: 1.776172]\n",
      "36 [D loss: 0.208574] [G loss: 0.300267] [C loss: 1.815082]\n",
      "37 [D loss: 0.207216] [G loss: 0.289736] [C loss: 1.730042]\n",
      "38 [D loss: 0.206983] [G loss: 0.287916] [C loss: 1.739667]\n",
      "39 [D loss: 0.206518] [G loss: 0.293822] [C loss: 1.736337]\n",
      "40 [D loss: 0.204750] [G loss: 0.283863] [C loss: 1.723723]\n",
      "41 [D loss: 0.203501] [G loss: 0.285734] [C loss: 1.705986]\n",
      "42 [D loss: 0.203489] [G loss: 0.285154] [C loss: 1.695617]\n",
      "43 [D loss: 0.202521] [G loss: 0.282103] [C loss: 1.680332]\n",
      "44 [D loss: 0.202112] [G loss: 0.277943] [C loss: 1.658547]\n",
      "45 [D loss: 0.201333] [G loss: 0.282180] [C loss: 1.649156]\n",
      "46 [D loss: 0.200757] [G loss: 0.276057] [C loss: 1.653194]\n",
      "47 [D loss: 0.199514] [G loss: 0.276913] [C loss: 1.613546]\n",
      "48 [D loss: 0.200910] [G loss: 0.275775] [C loss: 1.630388]\n",
      "49 [D loss: 0.200018] [G loss: 0.275978] [C loss: 1.603397]\n",
      "50 [D loss: 0.198472] [G loss: 0.270424] [C loss: 1.611222]\n",
      "51 [D loss: 0.198388] [G loss: 0.270316] [C loss: 1.556144]\n",
      "52 [D loss: 0.198766] [G loss: 0.268963] [C loss: 1.551993]\n",
      "53 [D loss: 0.197588] [G loss: 0.268475] [C loss: 1.544537]\n",
      "54 [D loss: 0.197139] [G loss: 0.258229] [C loss: 1.509210]\n",
      "55 [D loss: 0.197361] [G loss: 0.261479] [C loss: 1.516686]\n",
      "56 [D loss: 0.196439] [G loss: 0.260000] [C loss: 1.504520]\n",
      "57 [D loss: 0.195577] [G loss: 0.258256] [C loss: 1.493366]\n",
      "58 [D loss: 0.195555] [G loss: 0.256613] [C loss: 1.492111]\n",
      "59 [D loss: 0.195045] [G loss: 0.254774] [C loss: 1.474097]\n",
      "60 [D loss: 0.194692] [G loss: 0.256616] [C loss: 1.478250]\n",
      "61 [D loss: 0.194618] [G loss: 0.258478] [C loss: 1.466148]\n",
      "62 [D loss: 0.194118] [G loss: 0.253488] [C loss: 1.448416]\n",
      "63 [D loss: 0.193687] [G loss: 0.244559] [C loss: 1.404549]\n",
      "64 [D loss: 0.193370] [G loss: 0.249792] [C loss: 1.414852]\n",
      "65 [D loss: 0.193701] [G loss: 0.251338] [C loss: 1.417920]\n",
      "66 [D loss: 0.192501] [G loss: 0.253564] [C loss: 1.433859]\n",
      "67 [D loss: 0.191889] [G loss: 0.240371] [C loss: 1.374201]\n",
      "68 [D loss: 0.191507] [G loss: 0.239940] [C loss: 1.369113]\n",
      "69 [D loss: 0.190796] [G loss: 0.239310] [C loss: 1.329606]\n",
      "70 [D loss: 0.190741] [G loss: 0.236677] [C loss: 1.346017]\n",
      "71 [D loss: 0.190515] [G loss: 0.243922] [C loss: 1.362133]\n",
      "72 [D loss: 0.190956] [G loss: 0.235142] [C loss: 1.294998]\n",
      "73 [D loss: 0.190042] [G loss: 0.235809] [C loss: 1.311256]\n",
      "74 [D loss: 0.190668] [G loss: 0.236417] [C loss: 1.319718]\n",
      "75 [D loss: 0.189437] [G loss: 0.227448] [C loss: 1.275292]\n",
      "76 [D loss: 0.189201] [G loss: 0.235158] [C loss: 1.300816]\n",
      "77 [D loss: 0.188780] [G loss: 0.224383] [C loss: 1.261249]\n",
      "78 [D loss: 0.189230] [G loss: 0.228397] [C loss: 1.238369]\n",
      "79 [D loss: 0.188568] [G loss: 0.223384] [C loss: 1.277677]\n",
      "80 [D loss: 0.188610] [G loss: 0.223068] [C loss: 1.224334]\n",
      "81 [D loss: 0.188099] [G loss: 0.224878] [C loss: 1.224978]\n",
      "82 [D loss: 0.188005] [G loss: 0.219105] [C loss: 1.201961]\n",
      "83 [D loss: 0.187015] [G loss: 0.221905] [C loss: 1.207389]\n",
      "84 [D loss: 0.187879] [G loss: 0.216867] [C loss: 1.193099]\n",
      "85 [D loss: 0.187639] [G loss: 0.211514] [C loss: 1.199582]\n",
      "86 [D loss: 0.188307] [G loss: 0.213598] [C loss: 1.184957]\n",
      "87 [D loss: 0.187375] [G loss: 0.215322] [C loss: 1.184178]\n",
      "88 [D loss: 0.187803] [G loss: 0.205990] [C loss: 1.149968]\n",
      "89 [D loss: 0.187782] [G loss: 0.205912] [C loss: 1.145260]\n",
      "90 [D loss: 0.188160] [G loss: 0.205480] [C loss: 1.107517]\n",
      "91 [D loss: 0.188179] [G loss: 0.207477] [C loss: 1.148033]\n",
      "92 [D loss: 0.188207] [G loss: 0.204754] [C loss: 1.103195]\n",
      "93 [D loss: 0.187460] [G loss: 0.201503] [C loss: 1.112269]\n",
      "94 [D loss: 0.187418] [G loss: 0.201977] [C loss: 1.136270]\n",
      "95 [D loss: 0.187247] [G loss: 0.198843] [C loss: 1.085439]\n",
      "96 [D loss: 0.187584] [G loss: 0.196530] [C loss: 1.089748]\n",
      "97 [D loss: 0.187942] [G loss: 0.191932] [C loss: 1.057891]\n",
      "98 [D loss: 0.188555] [G loss: 0.189566] [C loss: 1.048963]\n",
      "99 [D loss: 0.187706] [G loss: 0.196668] [C loss: 1.101174]\n",
      "100 [D loss: 0.187978] [G loss: 0.189792] [C loss: 1.053389]\n",
      "101 [D loss: 0.187376] [G loss: 0.192675] [C loss: 1.059909]\n",
      "102 [D loss: 0.187213] [G loss: 0.182419] [C loss: 1.043453]\n",
      "103 [D loss: 0.187534] [G loss: 0.184844] [C loss: 1.047774]\n",
      "104 [D loss: 0.188554] [G loss: 0.182961] [C loss: 1.007977]\n",
      "105 [D loss: 0.188434] [G loss: 0.178532] [C loss: 1.011802]\n",
      "106 [D loss: 0.188236] [G loss: 0.174179] [C loss: 0.994199]\n",
      "107 [D loss: 0.188896] [G loss: 0.167845] [C loss: 0.998164]\n",
      "108 [D loss: 0.188262] [G loss: 0.174879] [C loss: 1.006457]\n",
      "109 [D loss: 0.187663] [G loss: 0.169672] [C loss: 0.986542]\n",
      "110 [D loss: 0.188665] [G loss: 0.167697] [C loss: 0.943795]\n",
      "111 [D loss: 0.188472] [G loss: 0.168516] [C loss: 0.978926]\n",
      "112 [D loss: 0.189972] [G loss: 0.166114] [C loss: 0.974665]\n",
      "113 [D loss: 0.188701] [G loss: 0.166084] [C loss: 0.988418]\n",
      "114 [D loss: 0.188258] [G loss: 0.163376] [C loss: 0.972594]\n",
      "115 [D loss: 0.189142] [G loss: 0.160573] [C loss: 0.928403]\n",
      "116 [D loss: 0.190171] [G loss: 0.160569] [C loss: 0.921766]\n",
      "117 [D loss: 0.189218] [G loss: 0.159124] [C loss: 0.910404]\n",
      "118 [D loss: 0.189239] [G loss: 0.164907] [C loss: 0.930686]\n",
      "119 [D loss: 0.189460] [G loss: 0.157567] [C loss: 0.902844]\n",
      "120 [D loss: 0.190352] [G loss: 0.155778] [C loss: 0.887605]\n",
      "121 [D loss: 0.189775] [G loss: 0.157427] [C loss: 0.894252]\n",
      "122 [D loss: 0.189581] [G loss: 0.155331] [C loss: 0.904550]\n",
      "123 [D loss: 0.190022] [G loss: 0.154392] [C loss: 0.876235]\n",
      "124 [D loss: 0.190415] [G loss: 0.155928] [C loss: 0.898498]\n",
      "125 [D loss: 0.189801] [G loss: 0.147810] [C loss: 0.870938]\n",
      "126 [D loss: 0.189706] [G loss: 0.155093] [C loss: 0.879436]\n",
      "127 [D loss: 0.190147] [G loss: 0.151423] [C loss: 0.863437]\n",
      "128 [D loss: 0.190410] [G loss: 0.155851] [C loss: 0.874880]\n",
      "129 [D loss: 0.189568] [G loss: 0.144700] [C loss: 0.833816]\n",
      "130 [D loss: 0.189850] [G loss: 0.150813] [C loss: 0.854269]\n",
      "131 [D loss: 0.189825] [G loss: 0.146867] [C loss: 0.837469]\n",
      "132 [D loss: 0.190471] [G loss: 0.145119] [C loss: 0.849404]\n",
      "133 [D loss: 0.189159] [G loss: 0.144788] [C loss: 0.839446]\n",
      "134 [D loss: 0.190292] [G loss: 0.151032] [C loss: 0.867841]\n",
      "135 [D loss: 0.189693] [G loss: 0.148917] [C loss: 0.846240]\n",
      "136 [D loss: 0.189940] [G loss: 0.146359] [C loss: 0.840395]\n",
      "137 [D loss: 0.189956] [G loss: 0.140663] [C loss: 0.818056]\n",
      "138 [D loss: 0.191043] [G loss: 0.140104] [C loss: 0.829961]\n",
      "139 [D loss: 0.190471] [G loss: 0.141022] [C loss: 0.800281]\n",
      "140 [D loss: 0.190068] [G loss: 0.142010] [C loss: 0.819918]\n",
      "141 [D loss: 0.190432] [G loss: 0.138582] [C loss: 0.837952]\n",
      "142 [D loss: 0.190341] [G loss: 0.144595] [C loss: 0.818272]\n",
      "143 [D loss: 0.190178] [G loss: 0.142641] [C loss: 0.826052]\n",
      "144 [D loss: 0.189712] [G loss: 0.140976] [C loss: 0.822500]\n",
      "145 [D loss: 0.190864] [G loss: 0.138680] [C loss: 0.785453]\n",
      "146 [D loss: 0.189815] [G loss: 0.141570] [C loss: 0.818171]\n",
      "147 [D loss: 0.189105] [G loss: 0.137547] [C loss: 0.791673]\n",
      "148 [D loss: 0.190535] [G loss: 0.135684] [C loss: 0.782488]\n",
      "149 [D loss: 0.189468] [G loss: 0.131019] [C loss: 0.768561]\n",
      "150 [D loss: 0.189700] [G loss: 0.135064] [C loss: 0.766658]\n",
      "151 [D loss: 0.189303] [G loss: 0.137994] [C loss: 0.822597]\n",
      "152 [D loss: 0.189696] [G loss: 0.135511] [C loss: 0.765854]\n",
      "153 [D loss: 0.190166] [G loss: 0.130555] [C loss: 0.775559]\n",
      "154 [D loss: 0.189904] [G loss: 0.132436] [C loss: 0.767445]\n",
      "155 [D loss: 0.189984] [G loss: 0.132304] [C loss: 0.755319]\n",
      "156 [D loss: 0.189745] [G loss: 0.131268] [C loss: 0.736877]\n",
      "157 [D loss: 0.189578] [G loss: 0.133568] [C loss: 0.749112]\n",
      "158 [D loss: 0.189517] [G loss: 0.129510] [C loss: 0.725624]\n",
      "159 [D loss: 0.188901] [G loss: 0.129964] [C loss: 0.742927]\n",
      "160 [D loss: 0.188870] [G loss: 0.132104] [C loss: 0.754242]\n",
      "161 [D loss: 0.189636] [G loss: 0.131365] [C loss: 0.763597]\n",
      "162 [D loss: 0.189704] [G loss: 0.128541] [C loss: 0.730011]\n",
      "163 [D loss: 0.189065] [G loss: 0.130451] [C loss: 0.741403]\n",
      "164 [D loss: 0.189699] [G loss: 0.128242] [C loss: 0.723165]\n",
      "165 [D loss: 0.190341] [G loss: 0.126899] [C loss: 0.716735]\n",
      "166 [D loss: 0.189821] [G loss: 0.133053] [C loss: 0.748354]\n",
      "167 [D loss: 0.189345] [G loss: 0.129830] [C loss: 0.747211]\n",
      "168 [D loss: 0.188740] [G loss: 0.121490] [C loss: 0.690312]\n",
      "169 [D loss: 0.189344] [G loss: 0.129835] [C loss: 0.723116]\n",
      "170 [D loss: 0.189596] [G loss: 0.127435] [C loss: 0.684601]\n",
      "171 [D loss: 0.189645] [G loss: 0.123050] [C loss: 0.672275]\n",
      "172 [D loss: 0.188526] [G loss: 0.119352] [C loss: 0.689862]\n",
      "173 [D loss: 0.189653] [G loss: 0.127672] [C loss: 0.705590]\n",
      "174 [D loss: 0.189063] [G loss: 0.123721] [C loss: 0.692223]\n",
      "175 [D loss: 0.190709] [G loss: 0.120269] [C loss: 0.687451]\n",
      "176 [D loss: 0.190386] [G loss: 0.119618] [C loss: 0.640057]\n",
      "177 [D loss: 0.190192] [G loss: 0.120406] [C loss: 0.665483]\n",
      "178 [D loss: 0.189837] [G loss: 0.115319] [C loss: 0.676449]\n",
      "179 [D loss: 0.189818] [G loss: 0.120840] [C loss: 0.663007]\n",
      "180 [D loss: 0.190409] [G loss: 0.119929] [C loss: 0.697066]\n",
      "181 [D loss: 0.189957] [G loss: 0.119767] [C loss: 0.691621]\n",
      "182 [D loss: 0.189276] [G loss: 0.118833] [C loss: 0.665510]\n",
      "183 [D loss: 0.189384] [G loss: 0.119700] [C loss: 0.648255]\n",
      "184 [D loss: 0.189206] [G loss: 0.119288] [C loss: 0.641305]\n",
      "185 [D loss: 0.189288] [G loss: 0.113077] [C loss: 0.611664]\n",
      "186 [D loss: 0.189837] [G loss: 0.113337] [C loss: 0.625377]\n",
      "187 [D loss: 0.189703] [G loss: 0.118084] [C loss: 0.626410]\n",
      "188 [D loss: 0.189547] [G loss: 0.116688] [C loss: 0.669524]\n",
      "189 [D loss: 0.188950] [G loss: 0.118755] [C loss: 0.656049]\n",
      "190 [D loss: 0.189128] [G loss: 0.112012] [C loss: 0.636111]\n",
      "191 [D loss: 0.188941] [G loss: 0.114251] [C loss: 0.627707]\n",
      "192 [D loss: 0.188706] [G loss: 0.121042] [C loss: 0.635100]\n",
      "193 [D loss: 0.189257] [G loss: 0.110984] [C loss: 0.636592]\n",
      "194 [D loss: 0.190004] [G loss: 0.111471] [C loss: 0.657880]\n",
      "195 [D loss: 0.189541] [G loss: 0.112370] [C loss: 0.610472]\n",
      "196 [D loss: 0.188796] [G loss: 0.113321] [C loss: 0.616306]\n",
      "197 [D loss: 0.189327] [G loss: 0.111545] [C loss: 0.610455]\n",
      "198 [D loss: 0.189344] [G loss: 0.115240] [C loss: 0.656751]\n",
      "199 [D loss: 0.188878] [G loss: 0.107125] [C loss: 0.582739]\n",
      "200 [D loss: 0.188079] [G loss: 0.107367] [C loss: 0.582306]\n",
      "201 [D loss: 0.189311] [G loss: 0.105574] [C loss: 0.609886]\n",
      "202 [D loss: 0.189012] [G loss: 0.107205] [C loss: 0.568305]\n",
      "203 [D loss: 0.188852] [G loss: 0.108296] [C loss: 0.590383]\n",
      "204 [D loss: 0.188542] [G loss: 0.104024] [C loss: 0.583732]\n",
      "205 [D loss: 0.188365] [G loss: 0.105453] [C loss: 0.577678]\n",
      "206 [D loss: 0.188416] [G loss: 0.106927] [C loss: 0.563071]\n",
      "207 [D loss: 0.188095] [G loss: 0.106550] [C loss: 0.601272]\n",
      "208 [D loss: 0.187582] [G loss: 0.103316] [C loss: 0.581120]\n",
      "209 [D loss: 0.188760] [G loss: 0.107740] [C loss: 0.583579]\n",
      "210 [D loss: 0.188367] [G loss: 0.106607] [C loss: 0.568322]\n",
      "211 [D loss: 0.186571] [G loss: 0.102558] [C loss: 0.604537]\n",
      "212 [D loss: 0.187530] [G loss: 0.104645] [C loss: 0.547086]\n",
      "213 [D loss: 0.188284] [G loss: 0.109353] [C loss: 0.592118]\n",
      "214 [D loss: 0.187395] [G loss: 0.105336] [C loss: 0.594651]\n",
      "215 [D loss: 0.187789] [G loss: 0.099711] [C loss: 0.555195]\n",
      "216 [D loss: 0.187619] [G loss: 0.106359] [C loss: 0.593313]\n",
      "217 [D loss: 0.186817] [G loss: 0.099390] [C loss: 0.518677]\n",
      "218 [D loss: 0.187266] [G loss: 0.102119] [C loss: 0.577565]\n",
      "219 [D loss: 0.187388] [G loss: 0.096310] [C loss: 0.556707]\n",
      "220 [D loss: 0.187625] [G loss: 0.101591] [C loss: 0.552850]\n",
      "221 [D loss: 0.188112] [G loss: 0.096840] [C loss: 0.540643]\n",
      "222 [D loss: 0.187388] [G loss: 0.098283] [C loss: 0.558628]\n",
      "223 [D loss: 0.186680] [G loss: 0.097248] [C loss: 0.527946]\n",
      "224 [D loss: 0.186904] [G loss: 0.095698] [C loss: 0.528011]\n",
      "225 [D loss: 0.186763] [G loss: 0.096374] [C loss: 0.536803]\n",
      "226 [D loss: 0.186380] [G loss: 0.095679] [C loss: 0.505188]\n",
      "227 [D loss: 0.186713] [G loss: 0.098365] [C loss: 0.585701]\n",
      "228 [D loss: 0.186740] [G loss: 0.099598] [C loss: 0.536114]\n",
      "229 [D loss: 0.186189] [G loss: 0.100966] [C loss: 0.527466]\n",
      "230 [D loss: 0.186123] [G loss: 0.099936] [C loss: 0.534005]\n",
      "231 [D loss: 0.186546] [G loss: 0.097942] [C loss: 0.540361]\n",
      "232 [D loss: 0.186183] [G loss: 0.095407] [C loss: 0.513742]\n",
      "233 [D loss: 0.186040] [G loss: 0.096570] [C loss: 0.535058]\n",
      "234 [D loss: 0.185501] [G loss: 0.097344] [C loss: 0.513272]\n",
      "235 [D loss: 0.185275] [G loss: 0.096667] [C loss: 0.504335]\n",
      "236 [D loss: 0.185490] [G loss: 0.102605] [C loss: 0.573577]\n",
      "237 [D loss: 0.184702] [G loss: 0.093545] [C loss: 0.530693]\n",
      "238 [D loss: 0.185706] [G loss: 0.097134] [C loss: 0.511946]\n",
      "239 [D loss: 0.184783] [G loss: 0.088538] [C loss: 0.479599]\n",
      "240 [D loss: 0.185036] [G loss: 0.089988] [C loss: 0.522652]\n",
      "241 [D loss: 0.184727] [G loss: 0.088807] [C loss: 0.495544]\n",
      "242 [D loss: 0.185017] [G loss: 0.093046] [C loss: 0.489541]\n",
      "243 [D loss: 0.185459] [G loss: 0.098259] [C loss: 0.539317]\n",
      "244 [D loss: 0.184437] [G loss: 0.091243] [C loss: 0.498046]\n",
      "245 [D loss: 0.184326] [G loss: 0.096218] [C loss: 0.516845]\n",
      "246 [D loss: 0.184710] [G loss: 0.095434] [C loss: 0.479978]\n",
      "247 [D loss: 0.184932] [G loss: 0.094334] [C loss: 0.552445]\n",
      "248 [D loss: 0.184178] [G loss: 0.090753] [C loss: 0.516333]\n",
      "249 [D loss: 0.184386] [G loss: 0.092406] [C loss: 0.482722]\n",
      "250 [D loss: 0.184211] [G loss: 0.091728] [C loss: 0.484554]\n",
      "251 [D loss: 0.184139] [G loss: 0.092998] [C loss: 0.478623]\n",
      "252 [D loss: 0.183693] [G loss: 0.093197] [C loss: 0.481894]\n",
      "253 [D loss: 0.183430] [G loss: 0.089454] [C loss: 0.493684]\n",
      "254 [D loss: 0.183379] [G loss: 0.095805] [C loss: 0.492080]\n",
      "255 [D loss: 0.182683] [G loss: 0.093291] [C loss: 0.489603]\n",
      "256 [D loss: 0.183974] [G loss: 0.089686] [C loss: 0.530910]\n",
      "257 [D loss: 0.183451] [G loss: 0.092994] [C loss: 0.498855]\n",
      "258 [D loss: 0.183245] [G loss: 0.090754] [C loss: 0.485031]\n",
      "259 [D loss: 0.182635] [G loss: 0.091330] [C loss: 0.489378]\n",
      "260 [D loss: 0.183072] [G loss: 0.089560] [C loss: 0.493287]\n",
      "261 [D loss: 0.182754] [G loss: 0.089003] [C loss: 0.523040]\n",
      "262 [D loss: 0.183120] [G loss: 0.093233] [C loss: 0.503576]\n",
      "263 [D loss: 0.183024] [G loss: 0.084514] [C loss: 0.448282]\n",
      "264 [D loss: 0.182630] [G loss: 0.092355] [C loss: 0.473696]\n",
      "265 [D loss: 0.182317] [G loss: 0.090031] [C loss: 0.456275]\n",
      "266 [D loss: 0.182628] [G loss: 0.091435] [C loss: 0.496624]\n",
      "267 [D loss: 0.182446] [G loss: 0.084710] [C loss: 0.499839]\n",
      "268 [D loss: 0.182528] [G loss: 0.086846] [C loss: 0.481495]\n",
      "269 [D loss: 0.182473] [G loss: 0.087000] [C loss: 0.474916]\n",
      "270 [D loss: 0.181741] [G loss: 0.083227] [C loss: 0.450936]\n",
      "271 [D loss: 0.181635] [G loss: 0.088260] [C loss: 0.471143]\n",
      "272 [D loss: 0.182309] [G loss: 0.088116] [C loss: 0.480334]\n",
      "273 [D loss: 0.181681] [G loss: 0.085003] [C loss: 0.465501]\n",
      "274 [D loss: 0.181422] [G loss: 0.084521] [C loss: 0.449328]\n",
      "275 [D loss: 0.181523] [G loss: 0.087709] [C loss: 0.475901]\n",
      "276 [D loss: 0.182551] [G loss: 0.092162] [C loss: 0.451089]\n",
      "277 [D loss: 0.181632] [G loss: 0.084639] [C loss: 0.392874]\n",
      "278 [D loss: 0.181509] [G loss: 0.093642] [C loss: 0.530939]\n",
      "279 [D loss: 0.181078] [G loss: 0.085689] [C loss: 0.451596]\n",
      "280 [D loss: 0.181250] [G loss: 0.085191] [C loss: 0.446519]\n",
      "281 [D loss: 0.182251] [G loss: 0.085529] [C loss: 0.432249]\n",
      "282 [D loss: 0.181450] [G loss: 0.085058] [C loss: 0.476050]\n",
      "283 [D loss: 0.182544] [G loss: 0.085552] [C loss: 0.459547]\n",
      "284 [D loss: 0.181222] [G loss: 0.082055] [C loss: 0.454415]\n",
      "285 [D loss: 0.181244] [G loss: 0.086740] [C loss: 0.475545]\n",
      "286 [D loss: 0.181778] [G loss: 0.085168] [C loss: 0.455420]\n",
      "287 [D loss: 0.181173] [G loss: 0.091824] [C loss: 0.506262]\n",
      "288 [D loss: 0.182003] [G loss: 0.086263] [C loss: 0.457206]\n",
      "289 [D loss: 0.181250] [G loss: 0.085314] [C loss: 0.452211]\n",
      "290 [D loss: 0.181548] [G loss: 0.086399] [C loss: 0.473715]\n",
      "291 [D loss: 0.181048] [G loss: 0.085779] [C loss: 0.420115]\n",
      "292 [D loss: 0.180547] [G loss: 0.083827] [C loss: 0.463928]\n",
      "293 [D loss: 0.180912] [G loss: 0.082134] [C loss: 0.449334]\n",
      "294 [D loss: 0.180508] [G loss: 0.084000] [C loss: 0.428660]\n",
      "295 [D loss: 0.181065] [G loss: 0.082324] [C loss: 0.437317]\n",
      "296 [D loss: 0.180611] [G loss: 0.084180] [C loss: 0.456202]\n",
      "297 [D loss: 0.180458] [G loss: 0.086400] [C loss: 0.462535]\n",
      "298 [D loss: 0.180839] [G loss: 0.080897] [C loss: 0.421683]\n",
      "299 [D loss: 0.180504] [G loss: 0.084072] [C loss: 0.423636]\n",
      "300 [D loss: 0.179406] [G loss: 0.086189] [C loss: 0.469129]\n",
      "301 [D loss: 0.180817] [G loss: 0.083670] [C loss: 0.436794]\n",
      "302 [D loss: 0.180045] [G loss: 0.083927] [C loss: 0.432077]\n",
      "303 [D loss: 0.179473] [G loss: 0.082180] [C loss: 0.405267]\n",
      "304 [D loss: 0.179275] [G loss: 0.081082] [C loss: 0.419867]\n",
      "305 [D loss: 0.179689] [G loss: 0.084440] [C loss: 0.426828]\n",
      "306 [D loss: 0.179304] [G loss: 0.086977] [C loss: 0.447237]\n",
      "307 [D loss: 0.178968] [G loss: 0.080972] [C loss: 0.423559]\n",
      "308 [D loss: 0.179976] [G loss: 0.080928] [C loss: 0.425762]\n",
      "309 [D loss: 0.178772] [G loss: 0.079228] [C loss: 0.401176]\n",
      "310 [D loss: 0.178735] [G loss: 0.085682] [C loss: 0.418884]\n",
      "311 [D loss: 0.179138] [G loss: 0.085644] [C loss: 0.439179]\n",
      "312 [D loss: 0.178722] [G loss: 0.087590] [C loss: 0.418476]\n",
      "313 [D loss: 0.178277] [G loss: 0.084100] [C loss: 0.421863]\n",
      "314 [D loss: 0.178225] [G loss: 0.079459] [C loss: 0.394315]\n",
      "315 [D loss: 0.178937] [G loss: 0.080800] [C loss: 0.378678]\n",
      "316 [D loss: 0.177995] [G loss: 0.081933] [C loss: 0.404567]\n",
      "317 [D loss: 0.177997] [G loss: 0.082942] [C loss: 0.443657]\n",
      "318 [D loss: 0.177496] [G loss: 0.083458] [C loss: 0.393309]\n",
      "319 [D loss: 0.178154] [G loss: 0.080801] [C loss: 0.419041]\n",
      "320 [D loss: 0.177368] [G loss: 0.079560] [C loss: 0.432365]\n",
      "321 [D loss: 0.177778] [G loss: 0.081670] [C loss: 0.407023]\n",
      "322 [D loss: 0.177308] [G loss: 0.079959] [C loss: 0.394223]\n",
      "323 [D loss: 0.177287] [G loss: 0.081252] [C loss: 0.427974]\n",
      "324 [D loss: 0.177533] [G loss: 0.079819] [C loss: 0.416767]\n",
      "325 [D loss: 0.177289] [G loss: 0.079459] [C loss: 0.421669]\n",
      "326 [D loss: 0.177490] [G loss: 0.079425] [C loss: 0.428611]\n",
      "327 [D loss: 0.176860] [G loss: 0.079349] [C loss: 0.389616]\n",
      "328 [D loss: 0.177609] [G loss: 0.079029] [C loss: 0.401853]\n",
      "329 [D loss: 0.177228] [G loss: 0.079474] [C loss: 0.395999]\n",
      "330 [D loss: 0.178004] [G loss: 0.076857] [C loss: 0.377094]\n",
      "331 [D loss: 0.176765] [G loss: 0.082396] [C loss: 0.431629]\n",
      "332 [D loss: 0.176739] [G loss: 0.083438] [C loss: 0.402297]\n",
      "333 [D loss: 0.177153] [G loss: 0.080903] [C loss: 0.396817]\n",
      "334 [D loss: 0.176232] [G loss: 0.078608] [C loss: 0.402440]\n",
      "335 [D loss: 0.176233] [G loss: 0.079873] [C loss: 0.404969]\n",
      "336 [D loss: 0.176404] [G loss: 0.077496] [C loss: 0.399836]\n",
      "337 [D loss: 0.176373] [G loss: 0.078658] [C loss: 0.402053]\n",
      "338 [D loss: 0.176350] [G loss: 0.078749] [C loss: 0.419956]\n",
      "339 [D loss: 0.175987] [G loss: 0.076396] [C loss: 0.386325]\n",
      "340 [D loss: 0.175481] [G loss: 0.079446] [C loss: 0.376162]\n",
      "341 [D loss: 0.176074] [G loss: 0.078903] [C loss: 0.420268]\n",
      "342 [D loss: 0.175681] [G loss: 0.079908] [C loss: 0.412884]\n",
      "343 [D loss: 0.175965] [G loss: 0.079168] [C loss: 0.388616]\n",
      "344 [D loss: 0.175482] [G loss: 0.075121] [C loss: 0.390178]\n",
      "345 [D loss: 0.175467] [G loss: 0.077658] [C loss: 0.410121]\n",
      "346 [D loss: 0.174420] [G loss: 0.078016] [C loss: 0.439537]\n",
      "347 [D loss: 0.175698] [G loss: 0.079772] [C loss: 0.393921]\n",
      "348 [D loss: 0.175259] [G loss: 0.077276] [C loss: 0.373591]\n",
      "349 [D loss: 0.175241] [G loss: 0.079218] [C loss: 0.406540]\n",
      "350 [D loss: 0.175170] [G loss: 0.078408] [C loss: 0.385699]\n",
      "351 [D loss: 0.175207] [G loss: 0.077355] [C loss: 0.384381]\n",
      "352 [D loss: 0.174561] [G loss: 0.081080] [C loss: 0.367443]\n",
      "353 [D loss: 0.174472] [G loss: 0.078798] [C loss: 0.386548]\n",
      "354 [D loss: 0.174371] [G loss: 0.073977] [C loss: 0.357993]\n",
      "355 [D loss: 0.174496] [G loss: 0.077523] [C loss: 0.372205]\n",
      "356 [D loss: 0.174125] [G loss: 0.075511] [C loss: 0.375037]\n",
      "357 [D loss: 0.174502] [G loss: 0.077203] [C loss: 0.413750]\n",
      "358 [D loss: 0.174584] [G loss: 0.075804] [C loss: 0.385672]\n",
      "359 [D loss: 0.174213] [G loss: 0.076364] [C loss: 0.415080]\n",
      "360 [D loss: 0.173890] [G loss: 0.077949] [C loss: 0.392121]\n",
      "361 [D loss: 0.174442] [G loss: 0.078341] [C loss: 0.379327]\n",
      "362 [D loss: 0.174348] [G loss: 0.074211] [C loss: 0.355551]\n",
      "363 [D loss: 0.173769] [G loss: 0.075701] [C loss: 0.381827]\n",
      "364 [D loss: 0.173682] [G loss: 0.078431] [C loss: 0.369195]\n",
      "365 [D loss: 0.174534] [G loss: 0.080744] [C loss: 0.425177]\n",
      "366 [D loss: 0.173852] [G loss: 0.074631] [C loss: 0.355212]\n",
      "367 [D loss: 0.173680] [G loss: 0.074556] [C loss: 0.391744]\n",
      "368 [D loss: 0.174843] [G loss: 0.073973] [C loss: 0.353270]\n",
      "369 [D loss: 0.174856] [G loss: 0.078563] [C loss: 0.398414]\n",
      "370 [D loss: 0.175097] [G loss: 0.073814] [C loss: 0.336489]\n",
      "371 [D loss: 0.174137] [G loss: 0.074920] [C loss: 0.355131]\n",
      "372 [D loss: 0.174489] [G loss: 0.076643] [C loss: 0.384958]\n",
      "373 [D loss: 0.174522] [G loss: 0.076764] [C loss: 0.354151]\n",
      "374 [D loss: 0.174603] [G loss: 0.075559] [C loss: 0.394111]\n",
      "375 [D loss: 0.174920] [G loss: 0.075809] [C loss: 0.381950]\n",
      "376 [D loss: 0.174534] [G loss: 0.078459] [C loss: 0.381465]\n",
      "377 [D loss: 0.174085] [G loss: 0.073786] [C loss: 0.344230]\n",
      "378 [D loss: 0.173826] [G loss: 0.073713] [C loss: 0.314063]\n",
      "379 [D loss: 0.174123] [G loss: 0.077014] [C loss: 0.335590]\n",
      "380 [D loss: 0.174315] [G loss: 0.071961] [C loss: 0.360266]\n",
      "381 [D loss: 0.173392] [G loss: 0.074566] [C loss: 0.355155]\n",
      "382 [D loss: 0.174449] [G loss: 0.074364] [C loss: 0.365765]\n",
      "383 [D loss: 0.174080] [G loss: 0.074051] [C loss: 0.416198]\n",
      "384 [D loss: 0.175101] [G loss: 0.075136] [C loss: 0.383475]\n",
      "385 [D loss: 0.174870] [G loss: 0.076725] [C loss: 0.346693]\n",
      "386 [D loss: 0.175050] [G loss: 0.073697] [C loss: 0.346929]\n",
      "387 [D loss: 0.174197] [G loss: 0.072318] [C loss: 0.316158]\n",
      "388 [D loss: 0.174349] [G loss: 0.070788] [C loss: 0.385362]\n",
      "389 [D loss: 0.174414] [G loss: 0.073700] [C loss: 0.344597]\n",
      "390 [D loss: 0.174519] [G loss: 0.075661] [C loss: 0.392263]\n",
      "391 [D loss: 0.173650] [G loss: 0.075373] [C loss: 0.357889]\n",
      "392 [D loss: 0.174279] [G loss: 0.077721] [C loss: 0.371113]\n",
      "393 [D loss: 0.173721] [G loss: 0.076188] [C loss: 0.363464]\n",
      "394 [D loss: 0.173445] [G loss: 0.077426] [C loss: 0.362647]\n",
      "395 [D loss: 0.173422] [G loss: 0.074772] [C loss: 0.340444]\n",
      "396 [D loss: 0.174135] [G loss: 0.073272] [C loss: 0.364521]\n",
      "397 [D loss: 0.173729] [G loss: 0.073837] [C loss: 0.361567]\n",
      "398 [D loss: 0.173193] [G loss: 0.075687] [C loss: 0.354061]\n",
      "399 [D loss: 0.173480] [G loss: 0.076454] [C loss: 0.374491]\n",
      "400 [D loss: 0.173373] [G loss: 0.076703] [C loss: 0.388041]\n",
      "401 [D loss: 0.173107] [G loss: 0.071056] [C loss: 0.329484]\n",
      "402 [D loss: 0.172524] [G loss: 0.073317] [C loss: 0.353641]\n",
      "403 [D loss: 0.172715] [G loss: 0.073908] [C loss: 0.353367]\n",
      "404 [D loss: 0.172390] [G loss: 0.074970] [C loss: 0.348692]\n",
      "405 [D loss: 0.172273] [G loss: 0.075542] [C loss: 0.373615]\n",
      "406 [D loss: 0.172356] [G loss: 0.075094] [C loss: 0.354225]\n",
      "407 [D loss: 0.172390] [G loss: 0.074365] [C loss: 0.415981]\n",
      "408 [D loss: 0.171856] [G loss: 0.073250] [C loss: 0.378660]\n",
      "409 [D loss: 0.171598] [G loss: 0.072120] [C loss: 0.394552]\n",
      "410 [D loss: 0.172110] [G loss: 0.076324] [C loss: 0.369782]\n",
      "411 [D loss: 0.172248] [G loss: 0.072761] [C loss: 0.361046]\n",
      "412 [D loss: 0.171429] [G loss: 0.076485] [C loss: 0.385235]\n",
      "413 [D loss: 0.171640] [G loss: 0.072372] [C loss: 0.371540]\n",
      "414 [D loss: 0.171242] [G loss: 0.073693] [C loss: 0.354438]\n",
      "415 [D loss: 0.170958] [G loss: 0.073126] [C loss: 0.339338]\n",
      "416 [D loss: 0.171161] [G loss: 0.073521] [C loss: 0.352261]\n",
      "417 [D loss: 0.170730] [G loss: 0.072055] [C loss: 0.389275]\n",
      "418 [D loss: 0.170674] [G loss: 0.075330] [C loss: 0.375233]\n",
      "419 [D loss: 0.170040] [G loss: 0.072649] [C loss: 0.372328]\n",
      "420 [D loss: 0.170209] [G loss: 0.075173] [C loss: 0.375111]\n",
      "421 [D loss: 0.169781] [G loss: 0.074770] [C loss: 0.338378]\n",
      "422 [D loss: 0.169982] [G loss: 0.073382] [C loss: 0.355080]\n",
      "423 [D loss: 0.169669] [G loss: 0.072048] [C loss: 0.344400]\n",
      "424 [D loss: 0.169873] [G loss: 0.073524] [C loss: 0.342142]\n",
      "425 [D loss: 0.169575] [G loss: 0.073232] [C loss: 0.352826]\n",
      "426 [D loss: 0.169448] [G loss: 0.076564] [C loss: 0.369116]\n",
      "427 [D loss: 0.170181] [G loss: 0.069759] [C loss: 0.343557]\n",
      "428 [D loss: 0.169177] [G loss: 0.073208] [C loss: 0.342691]\n",
      "429 [D loss: 0.169286] [G loss: 0.069422] [C loss: 0.328733]\n",
      "430 [D loss: 0.168769] [G loss: 0.071724] [C loss: 0.344845]\n",
      "431 [D loss: 0.169231] [G loss: 0.071782] [C loss: 0.346077]\n",
      "432 [D loss: 0.169672] [G loss: 0.077268] [C loss: 0.383921]\n",
      "433 [D loss: 0.168675] [G loss: 0.075222] [C loss: 0.337821]\n",
      "434 [D loss: 0.168507] [G loss: 0.071464] [C loss: 0.332333]\n",
      "435 [D loss: 0.168988] [G loss: 0.076259] [C loss: 0.374424]\n",
      "436 [D loss: 0.168689] [G loss: 0.074725] [C loss: 0.331870]\n",
      "437 [D loss: 0.168864] [G loss: 0.071452] [C loss: 0.371795]\n",
      "438 [D loss: 0.168649] [G loss: 0.072234] [C loss: 0.333239]\n",
      "439 [D loss: 0.168501] [G loss: 0.070865] [C loss: 0.363915]\n",
      "440 [D loss: 0.168363] [G loss: 0.074749] [C loss: 0.348841]\n",
      "441 [D loss: 0.168498] [G loss: 0.068905] [C loss: 0.365379]\n",
      "442 [D loss: 0.168643] [G loss: 0.073672] [C loss: 0.359328]\n",
      "443 [D loss: 0.168581] [G loss: 0.072147] [C loss: 0.384040]\n",
      "444 [D loss: 0.167836] [G loss: 0.069502] [C loss: 0.332500]\n",
      "445 [D loss: 0.167765] [G loss: 0.075266] [C loss: 0.358694]\n",
      "446 [D loss: 0.168221] [G loss: 0.074417] [C loss: 0.338698]\n",
      "447 [D loss: 0.168521] [G loss: 0.074653] [C loss: 0.366618]\n",
      "448 [D loss: 0.168063] [G loss: 0.072887] [C loss: 0.368452]\n",
      "449 [D loss: 0.168335] [G loss: 0.070758] [C loss: 0.362847]\n",
      "450 [D loss: 0.168037] [G loss: 0.074533] [C loss: 0.345579]\n",
      "451 [D loss: 0.167068] [G loss: 0.071745] [C loss: 0.348921]\n",
      "452 [D loss: 0.167106] [G loss: 0.073673] [C loss: 0.393515]\n",
      "453 [D loss: 0.167455] [G loss: 0.072313] [C loss: 0.373396]\n",
      "454 [D loss: 0.166759] [G loss: 0.073416] [C loss: 0.357117]\n",
      "455 [D loss: 0.166030] [G loss: 0.069976] [C loss: 0.313780]\n",
      "456 [D loss: 0.166786] [G loss: 0.073379] [C loss: 0.356903]\n",
      "457 [D loss: 0.166610] [G loss: 0.073602] [C loss: 0.355963]\n",
      "458 [D loss: 0.166046] [G loss: 0.074990] [C loss: 0.367084]\n",
      "459 [D loss: 0.166584] [G loss: 0.073524] [C loss: 0.350392]\n",
      "460 [D loss: 0.166233] [G loss: 0.072613] [C loss: 0.332348]\n",
      "461 [D loss: 0.166137] [G loss: 0.073788] [C loss: 0.308177]\n",
      "462 [D loss: 0.165986] [G loss: 0.074544] [C loss: 0.382607]\n",
      "463 [D loss: 0.166657] [G loss: 0.073164] [C loss: 0.327384]\n",
      "464 [D loss: 0.165837] [G loss: 0.069900] [C loss: 0.334960]\n",
      "465 [D loss: 0.165958] [G loss: 0.075053] [C loss: 0.365072]\n",
      "466 [D loss: 0.165749] [G loss: 0.075291] [C loss: 0.339135]\n",
      "467 [D loss: 0.166482] [G loss: 0.073694] [C loss: 0.362740]\n",
      "468 [D loss: 0.165961] [G loss: 0.073396] [C loss: 0.331908]\n",
      "469 [D loss: 0.165380] [G loss: 0.069732] [C loss: 0.350508]\n",
      "470 [D loss: 0.165695] [G loss: 0.073521] [C loss: 0.332681]\n",
      "471 [D loss: 0.166030] [G loss: 0.072915] [C loss: 0.365747]\n",
      "472 [D loss: 0.165365] [G loss: 0.070607] [C loss: 0.328361]\n",
      "473 [D loss: 0.165747] [G loss: 0.070518] [C loss: 0.321322]\n",
      "474 [D loss: 0.165701] [G loss: 0.072794] [C loss: 0.343529]\n",
      "475 [D loss: 0.165053] [G loss: 0.071131] [C loss: 0.362083]\n",
      "476 [D loss: 0.165817] [G loss: 0.071395] [C loss: 0.325185]\n",
      "477 [D loss: 0.165455] [G loss: 0.072440] [C loss: 0.403336]\n",
      "478 [D loss: 0.165644] [G loss: 0.074782] [C loss: 0.340585]\n",
      "479 [D loss: 0.166170] [G loss: 0.074367] [C loss: 0.337612]\n",
      "480 [D loss: 0.164933] [G loss: 0.072199] [C loss: 0.342712]\n",
      "481 [D loss: 0.165165] [G loss: 0.071154] [C loss: 0.350773]\n",
      "482 [D loss: 0.165483] [G loss: 0.075415] [C loss: 0.338582]\n",
      "483 [D loss: 0.165464] [G loss: 0.075353] [C loss: 0.375730]\n",
      "484 [D loss: 0.165241] [G loss: 0.072369] [C loss: 0.344653]\n",
      "485 [D loss: 0.165224] [G loss: 0.072539] [C loss: 0.333988]\n",
      "486 [D loss: 0.164437] [G loss: 0.072095] [C loss: 0.351642]\n",
      "487 [D loss: 0.165399] [G loss: 0.075725] [C loss: 0.361019]\n",
      "488 [D loss: 0.164495] [G loss: 0.072530] [C loss: 0.334005]\n",
      "489 [D loss: 0.166167] [G loss: 0.072329] [C loss: 0.315515]\n",
      "490 [D loss: 0.165266] [G loss: 0.073789] [C loss: 0.368205]\n",
      "491 [D loss: 0.165327] [G loss: 0.072770] [C loss: 0.333906]\n",
      "492 [D loss: 0.165380] [G loss: 0.070023] [C loss: 0.317185]\n",
      "493 [D loss: 0.165383] [G loss: 0.073094] [C loss: 0.306797]\n",
      "494 [D loss: 0.165492] [G loss: 0.072774] [C loss: 0.354218]\n",
      "495 [D loss: 0.165770] [G loss: 0.073910] [C loss: 0.363828]\n",
      "496 [D loss: 0.166555] [G loss: 0.072083] [C loss: 0.344010]\n",
      "497 [D loss: 0.165077] [G loss: 0.075856] [C loss: 0.343040]\n",
      "498 [D loss: 0.166500] [G loss: 0.073019] [C loss: 0.324429]\n",
      "499 [D loss: 0.165997] [G loss: 0.075498] [C loss: 0.333844]\n",
      "CPU times: user 1h 9min 2s, sys: 55min 36s, total: 2h 4min 39s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.keras.backend.clear_session()\n",
    "random.seed(123)\n",
    "#Hyperparameters\n",
    "n_epochs = 100\n",
    "p_missing = 0.1\n",
    "\n",
    "#Loop\n",
    "classifier = define_classifier()\n",
    "discriminator = define_discriminator()\n",
    "generator = define_generator_impute()\n",
    "gan_model = define_gan(g_model = generator, d_model = discriminator, c_model = classifier)\n",
    "pred_realfake, pred_labels = train(g_model = generator, d_model = discriminator, c_model = classifier, \n",
    "                               gan_model=gan_model, p_missing=p_missing,\n",
    "                               x_train = x_train, y_train = y_train,\n",
    "                               x_test = x_test, n_epochs=n_epochs)\n",
    "\n",
    "rounded_labels= np.argmax(pred_labels, axis = 1)\n",
    "\n",
    "pred_metrics = pd.DataFrame({'ClassifierType': \"IB-GAN\",\n",
    "                    'Accuracy': accuracy_score(y_test, rounded_labels), \n",
    "                     'Precision': precision_score(y_test, rounded_labels, average=\"macro\"),\n",
    "                     'Recall': recall_score(y_test, rounded_labels, average=\"macro\"),\n",
    "                     'BalancedAccuracy': balanced_accuracy_score(y_true=y_test, y_pred = rounded_labels),\n",
    "                     'F1-score': f1_score(y_true=y_test, y_pred=rounded_labels, average=\"macro\")\n",
    "                    }, index = [0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "respective-diagram",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClassifierType</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>BalancedAccuracy</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IB-GAN</td>\n",
       "      <td>0.790462</td>\n",
       "      <td>0.693231</td>\n",
       "      <td>0.672434</td>\n",
       "      <td>0.672434</td>\n",
       "      <td>0.65461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ClassifierType  Accuracy  Precision    Recall  BalancedAccuracy  F1-score\n",
       "0         IB-GAN  0.790462   0.693231  0.672434          0.672434   0.65461"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-armor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ahead-honduras",
   "metadata": {},
   "source": [
    "#### Compare with Naive GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "transparent-cookbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Labels_Classifier\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 93, 13)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 93, 26)            702       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 93, 26)            104       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 46, 26)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 46, 1)             53        \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 46, 1)             4         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 23, 1)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                480       \n",
      "_________________________________________________________________\n",
      "labels_output (Dense)        (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,553\n",
      "Trainable params: 1,499\n",
      "Non-trainable params: 54\n",
      "_________________________________________________________________\n",
      "Model: \"RealFake_Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 93, 26)       0           input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 93, 26)       1378        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 93, 13)       689         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "realfake_output (Activation)    (None, 93, 13)       0           conv1d_4[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,067\n",
      "Trainable params: 2,067\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 93, 10)       0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 93, 36)       0           input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 93, 26)       1898        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 93, 13)       689         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 93, 13)       0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 93, 13)       0           input_5[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 93, 13)       0           conv1d_6[0][0]                   \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fakets_output (Add)             (None, 93, 13)       0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,587\n",
      "Trainable params: 2,587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 93, 10)       0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 93, 36)       0           input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 93, 13)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 93, 26)       1898        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 93, 13)       689         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 93, 13)       0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 93, 13)       0           input_5[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 93, 13)       0           conv1d_6[0][0]                   \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fakets_output (Add)             (None, 93, 13)       0           multiply_1[0][0]                 \n",
      "                                                                 multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Labels_Classifier (Model)       (None, 10)           1553        fakets_output[0][0]              \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "RealFake_Discriminator (Model)  (None, 93, 13)       2067        fakets_output[0][0]              \n",
      "                                                                 input_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,207\n",
      "Trainable params: 4,086\n",
      "Non-trainable params: 2,121\n",
      "__________________________________________________________________________________________________\n",
      "Number of steps total:  500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.712780] [G loss: 1.088855] [C loss: 3.096802]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.708721] [G loss: 1.066500] [C loss: 3.016187]\n",
      "2 [D loss: 0.705123] [G loss: 1.051443] [C loss: 2.987961]\n",
      "3 [D loss: 0.701101] [G loss: 1.041341] [C loss: 2.940164]\n",
      "4 [D loss: 0.697602] [G loss: 1.027327] [C loss: 2.832797]\n",
      "5 [D loss: 0.694589] [G loss: 1.017469] [C loss: 2.755674]\n",
      "6 [D loss: 0.692299] [G loss: 1.011691] [C loss: 2.649664]\n",
      "7 [D loss: 0.689996] [G loss: 1.006182] [C loss: 2.728942]\n",
      "8 [D loss: 0.688435] [G loss: 0.999833] [C loss: 2.556787]\n",
      "9 [D loss: 0.686251] [G loss: 0.991040] [C loss: 2.517570]\n",
      "10 [D loss: 0.685541] [G loss: 0.984313] [C loss: 2.498852]\n",
      "11 [D loss: 0.684020] [G loss: 0.974604] [C loss: 2.316761]\n",
      "12 [D loss: 0.683122] [G loss: 0.968397] [C loss: 2.329162]\n",
      "13 [D loss: 0.682318] [G loss: 0.958148] [C loss: 2.313528]\n",
      "14 [D loss: 0.681456] [G loss: 0.949268] [C loss: 2.280632]\n",
      "15 [D loss: 0.681617] [G loss: 0.943822] [C loss: 2.259490]\n",
      "16 [D loss: 0.681176] [G loss: 0.935404] [C loss: 2.198006]\n",
      "17 [D loss: 0.680414] [G loss: 0.930516] [C loss: 2.199055]\n",
      "18 [D loss: 0.680432] [G loss: 0.922523] [C loss: 2.146766]\n",
      "19 [D loss: 0.680088] [G loss: 0.914588] [C loss: 2.164885]\n",
      "20 [D loss: 0.679853] [G loss: 0.907155] [C loss: 2.164095]\n",
      "21 [D loss: 0.679257] [G loss: 0.902350] [C loss: 2.181153]\n",
      "22 [D loss: 0.678373] [G loss: 0.895835] [C loss: 2.119389]\n",
      "23 [D loss: 0.677953] [G loss: 0.889602] [C loss: 2.111789]\n",
      "24 [D loss: 0.678321] [G loss: 0.885255] [C loss: 2.157844]\n",
      "25 [D loss: 0.678115] [G loss: 0.880350] [C loss: 2.097215]\n",
      "26 [D loss: 0.677807] [G loss: 0.876732] [C loss: 2.118954]\n",
      "27 [D loss: 0.675419] [G loss: 0.872463] [C loss: 2.120700]\n",
      "28 [D loss: 0.675852] [G loss: 0.870999] [C loss: 2.100066]\n",
      "29 [D loss: 0.676502] [G loss: 0.865342] [C loss: 2.095126]\n",
      "30 [D loss: 0.676408] [G loss: 0.861339] [C loss: 2.100292]\n",
      "31 [D loss: 0.675098] [G loss: 0.860009] [C loss: 2.094800]\n",
      "32 [D loss: 0.676037] [G loss: 0.858015] [C loss: 2.054413]\n",
      "33 [D loss: 0.673510] [G loss: 0.854638] [C loss: 2.048611]\n",
      "34 [D loss: 0.673724] [G loss: 0.854066] [C loss: 2.040458]\n",
      "35 [D loss: 0.672894] [G loss: 0.853625] [C loss: 2.002376]\n",
      "36 [D loss: 0.672252] [G loss: 0.851242] [C loss: 2.009372]\n",
      "37 [D loss: 0.671055] [G loss: 0.851224] [C loss: 1.975738]\n",
      "38 [D loss: 0.668318] [G loss: 0.852131] [C loss: 1.982361]\n",
      "39 [D loss: 0.665796] [G loss: 0.854430] [C loss: 1.932545]\n",
      "40 [D loss: 0.664299] [G loss: 0.855829] [C loss: 1.940344]\n",
      "41 [D loss: 0.662409] [G loss: 0.858249] [C loss: 1.886945]\n",
      "42 [D loss: 0.659172] [G loss: 0.859488] [C loss: 1.878097]\n",
      "43 [D loss: 0.655477] [G loss: 0.866097] [C loss: 1.845469]\n",
      "44 [D loss: 0.652596] [G loss: 0.869018] [C loss: 1.880659]\n",
      "45 [D loss: 0.649796] [G loss: 0.873689] [C loss: 1.837463]\n",
      "46 [D loss: 0.645141] [G loss: 0.879925] [C loss: 1.862247]\n",
      "47 [D loss: 0.642191] [G loss: 0.885433] [C loss: 1.823067]\n",
      "48 [D loss: 0.638183] [G loss: 0.892390] [C loss: 1.820923]\n",
      "49 [D loss: 0.636184] [G loss: 0.896967] [C loss: 1.834013]\n",
      "50 [D loss: 0.631420] [G loss: 0.902410] [C loss: 1.826785]\n",
      "51 [D loss: 0.629390] [G loss: 0.907706] [C loss: 1.860786]\n",
      "52 [D loss: 0.625809] [G loss: 0.915216] [C loss: 1.800303]\n",
      "53 [D loss: 0.624328] [G loss: 0.920064] [C loss: 1.779897]\n",
      "54 [D loss: 0.622656] [G loss: 0.924690] [C loss: 1.840673]\n",
      "55 [D loss: 0.621536] [G loss: 0.928512] [C loss: 1.744151]\n",
      "56 [D loss: 0.618146] [G loss: 0.932180] [C loss: 1.751847]\n",
      "57 [D loss: 0.617038] [G loss: 0.933721] [C loss: 1.718039]\n",
      "58 [D loss: 0.619076] [G loss: 0.935369] [C loss: 1.730640]\n",
      "59 [D loss: 0.618133] [G loss: 0.934094] [C loss: 1.723242]\n",
      "60 [D loss: 0.619178] [G loss: 0.933865] [C loss: 1.765856]\n",
      "61 [D loss: 0.621766] [G loss: 0.930033] [C loss: 1.747870]\n",
      "62 [D loss: 0.623258] [G loss: 0.925013] [C loss: 1.700778]\n",
      "63 [D loss: 0.626145] [G loss: 0.917961] [C loss: 1.699892]\n",
      "64 [D loss: 0.630181] [G loss: 0.909271] [C loss: 1.722153]\n",
      "65 [D loss: 0.635490] [G loss: 0.899280] [C loss: 1.708523]\n",
      "66 [D loss: 0.642300] [G loss: 0.886122] [C loss: 1.657522]\n",
      "67 [D loss: 0.649857] [G loss: 0.871165] [C loss: 1.650149]\n",
      "68 [D loss: 0.657775] [G loss: 0.853352] [C loss: 1.644099]\n",
      "69 [D loss: 0.667083] [G loss: 0.836144] [C loss: 1.656047]\n",
      "70 [D loss: 0.678385] [G loss: 0.815797] [C loss: 1.619762]\n",
      "71 [D loss: 0.691836] [G loss: 0.797379] [C loss: 1.623875]\n",
      "72 [D loss: 0.705016] [G loss: 0.774530] [C loss: 1.642861]\n",
      "73 [D loss: 0.719916] [G loss: 0.753305] [C loss: 1.628417]\n",
      "74 [D loss: 0.736718] [G loss: 0.732599] [C loss: 1.668583]\n",
      "75 [D loss: 0.752959] [G loss: 0.712451] [C loss: 1.588499]\n",
      "76 [D loss: 0.769448] [G loss: 0.693068] [C loss: 1.604058]\n",
      "77 [D loss: 0.786742] [G loss: 0.675767] [C loss: 1.568573]\n",
      "78 [D loss: 0.803180] [G loss: 0.658332] [C loss: 1.606893]\n",
      "79 [D loss: 0.818233] [G loss: 0.644807] [C loss: 1.569978]\n",
      "80 [D loss: 0.831469] [G loss: 0.633792] [C loss: 1.596752]\n",
      "81 [D loss: 0.845858] [G loss: 0.625748] [C loss: 1.573507]\n",
      "82 [D loss: 0.853166] [G loss: 0.621355] [C loss: 1.573385]\n",
      "83 [D loss: 0.861233] [G loss: 0.619706] [C loss: 1.562539]\n",
      "84 [D loss: 0.863139] [G loss: 0.619243] [C loss: 1.547606]\n",
      "85 [D loss: 0.865801] [G loss: 0.625640] [C loss: 1.551717]\n",
      "86 [D loss: 0.862658] [G loss: 0.632449] [C loss: 1.547791]\n",
      "87 [D loss: 0.856995] [G loss: 0.645191] [C loss: 1.521181]\n",
      "88 [D loss: 0.848422] [G loss: 0.659308] [C loss: 1.542569]\n",
      "89 [D loss: 0.837309] [G loss: 0.678374] [C loss: 1.520903]\n",
      "90 [D loss: 0.824521] [G loss: 0.700050] [C loss: 1.511187]\n",
      "91 [D loss: 0.808536] [G loss: 0.726504] [C loss: 1.475848]\n",
      "92 [D loss: 0.792908] [G loss: 0.755501] [C loss: 1.520876]\n",
      "93 [D loss: 0.773869] [G loss: 0.791087] [C loss: 1.489783]\n",
      "94 [D loss: 0.757403] [G loss: 0.828195] [C loss: 1.462170]\n",
      "95 [D loss: 0.736937] [G loss: 0.868365] [C loss: 1.481240]\n",
      "96 [D loss: 0.718262] [G loss: 0.911675] [C loss: 1.489297]\n",
      "97 [D loss: 0.700323] [G loss: 0.957089] [C loss: 1.471990]\n",
      "98 [D loss: 0.680339] [G loss: 1.004205] [C loss: 1.465895]\n",
      "99 [D loss: 0.664358] [G loss: 1.051753] [C loss: 1.436637]\n",
      "100 [D loss: 0.646832] [G loss: 1.098780] [C loss: 1.435453]\n",
      "101 [D loss: 0.631206] [G loss: 1.145910] [C loss: 1.468937]\n",
      "102 [D loss: 0.612121] [G loss: 1.191196] [C loss: 1.408164]\n",
      "103 [D loss: 0.599648] [G loss: 1.232989] [C loss: 1.474092]\n",
      "104 [D loss: 0.587649] [G loss: 1.271668] [C loss: 1.462850]\n",
      "105 [D loss: 0.575955] [G loss: 1.303630] [C loss: 1.453960]\n",
      "106 [D loss: 0.562648] [G loss: 1.331188] [C loss: 1.422918]\n",
      "107 [D loss: 0.553524] [G loss: 1.353219] [C loss: 1.428611]\n",
      "108 [D loss: 0.547083] [G loss: 1.370332] [C loss: 1.477220]\n",
      "109 [D loss: 0.535455] [G loss: 1.385705] [C loss: 1.462791]\n",
      "110 [D loss: 0.530395] [G loss: 1.398333] [C loss: 1.464591]\n",
      "111 [D loss: 0.523279] [G loss: 1.404668] [C loss: 1.469486]\n",
      "112 [D loss: 0.514301] [G loss: 1.411595] [C loss: 1.483497]\n",
      "113 [D loss: 0.509590] [G loss: 1.413758] [C loss: 1.539114]\n",
      "114 [D loss: 0.501248] [G loss: 1.416317] [C loss: 1.508346]\n",
      "115 [D loss: 0.496524] [G loss: 1.419061] [C loss: 1.550027]\n",
      "116 [D loss: 0.490237] [G loss: 1.416176] [C loss: 1.518229]\n",
      "117 [D loss: 0.486157] [G loss: 1.414471] [C loss: 1.563586]\n",
      "118 [D loss: 0.480524] [G loss: 1.409712] [C loss: 1.561775]\n",
      "119 [D loss: 0.476056] [G loss: 1.401771] [C loss: 1.549577]\n",
      "120 [D loss: 0.473688] [G loss: 1.393405] [C loss: 1.598964]\n",
      "121 [D loss: 0.471692] [G loss: 1.385271] [C loss: 1.630861]\n",
      "122 [D loss: 0.468537] [G loss: 1.376553] [C loss: 1.670914]\n",
      "123 [D loss: 0.469155] [G loss: 1.368072] [C loss: 1.691762]\n",
      "124 [D loss: 0.466564] [G loss: 1.359469] [C loss: 1.699678]\n",
      "125 [D loss: 0.465762] [G loss: 1.349200] [C loss: 1.716887]\n",
      "126 [D loss: 0.463359] [G loss: 1.339706] [C loss: 1.740984]\n",
      "127 [D loss: 0.464659] [G loss: 1.329340] [C loss: 1.764471]\n",
      "128 [D loss: 0.465179] [G loss: 1.319245] [C loss: 1.774817]\n",
      "129 [D loss: 0.461333] [G loss: 1.310043] [C loss: 1.820082]\n",
      "130 [D loss: 0.459661] [G loss: 1.302735] [C loss: 1.836640]\n",
      "131 [D loss: 0.459826] [G loss: 1.296155] [C loss: 1.864385]\n",
      "132 [D loss: 0.458777] [G loss: 1.289597] [C loss: 1.865237]\n",
      "133 [D loss: 0.456568] [G loss: 1.281751] [C loss: 1.927184]\n",
      "134 [D loss: 0.455117] [G loss: 1.275713] [C loss: 1.884888]\n",
      "135 [D loss: 0.456551] [G loss: 1.268078] [C loss: 1.928096]\n",
      "136 [D loss: 0.454321] [G loss: 1.260649] [C loss: 1.895754]\n",
      "137 [D loss: 0.454484] [G loss: 1.252170] [C loss: 1.948970]\n",
      "138 [D loss: 0.454395] [G loss: 1.240790] [C loss: 1.914176]\n",
      "139 [D loss: 0.454623] [G loss: 1.240104] [C loss: 1.948805]\n",
      "140 [D loss: 0.455579] [G loss: 1.229023] [C loss: 1.965616]\n",
      "141 [D loss: 0.453628] [G loss: 1.227049] [C loss: 1.922654]\n",
      "142 [D loss: 0.458176] [G loss: 1.218840] [C loss: 1.981808]\n",
      "143 [D loss: 0.459825] [G loss: 1.208872] [C loss: 1.946756]\n",
      "144 [D loss: 0.462351] [G loss: 1.200841] [C loss: 1.963799]\n",
      "145 [D loss: 0.464975] [G loss: 1.190985] [C loss: 1.940593]\n",
      "146 [D loss: 0.468870] [G loss: 1.183068] [C loss: 1.928723]\n",
      "147 [D loss: 0.470251] [G loss: 1.171489] [C loss: 1.885790]\n",
      "148 [D loss: 0.477225] [G loss: 1.156337] [C loss: 1.943983]\n",
      "149 [D loss: 0.486858] [G loss: 1.139636] [C loss: 1.895293]\n",
      "150 [D loss: 0.491966] [G loss: 1.119257] [C loss: 1.876680]\n",
      "151 [D loss: 0.504265] [G loss: 1.095408] [C loss: 1.878029]\n",
      "152 [D loss: 0.515889] [G loss: 1.070068] [C loss: 1.894417]\n",
      "153 [D loss: 0.529117] [G loss: 1.041800] [C loss: 1.842298]\n",
      "154 [D loss: 0.542468] [G loss: 1.015636] [C loss: 1.884283]\n",
      "155 [D loss: 0.562769] [G loss: 0.985654] [C loss: 1.826997]\n",
      "156 [D loss: 0.579112] [G loss: 0.958485] [C loss: 1.824477]\n",
      "157 [D loss: 0.597597] [G loss: 0.921825] [C loss: 1.760050]\n",
      "158 [D loss: 0.615820] [G loss: 0.892743] [C loss: 1.774784]\n",
      "159 [D loss: 0.642309] [G loss: 0.860312] [C loss: 1.748219]\n",
      "160 [D loss: 0.664304] [G loss: 0.829443] [C loss: 1.718014]\n",
      "161 [D loss: 0.684935] [G loss: 0.804219] [C loss: 1.733639]\n",
      "162 [D loss: 0.718322] [G loss: 0.775556] [C loss: 1.688050]\n",
      "163 [D loss: 0.741459] [G loss: 0.746508] [C loss: 1.611421]\n",
      "164 [D loss: 0.767831] [G loss: 0.722751] [C loss: 1.647352]\n",
      "165 [D loss: 0.796389] [G loss: 0.699043] [C loss: 1.613218]\n",
      "166 [D loss: 0.817213] [G loss: 0.679318] [C loss: 1.585147]\n",
      "167 [D loss: 0.835065] [G loss: 0.661133] [C loss: 1.551853]\n",
      "168 [D loss: 0.852827] [G loss: 0.646550] [C loss: 1.506791]\n",
      "169 [D loss: 0.871144] [G loss: 0.634399] [C loss: 1.492621]\n",
      "170 [D loss: 0.875645] [G loss: 0.630927] [C loss: 1.470641]\n",
      "171 [D loss: 0.892909] [G loss: 0.626321] [C loss: 1.464154]\n",
      "172 [D loss: 0.881298] [G loss: 0.629950] [C loss: 1.418936]\n",
      "173 [D loss: 0.892577] [G loss: 0.637767] [C loss: 1.421201]\n",
      "174 [D loss: 0.889410] [G loss: 0.646235] [C loss: 1.447942]\n",
      "175 [D loss: 0.880351] [G loss: 0.656457] [C loss: 1.410444]\n",
      "176 [D loss: 0.867116] [G loss: 0.671091] [C loss: 1.398384]\n",
      "177 [D loss: 0.856601] [G loss: 0.685450] [C loss: 1.409812]\n",
      "178 [D loss: 0.846040] [G loss: 0.703517] [C loss: 1.453885]\n",
      "179 [D loss: 0.828126] [G loss: 0.721756] [C loss: 1.382632]\n",
      "180 [D loss: 0.815572] [G loss: 0.744724] [C loss: 1.342845]\n",
      "181 [D loss: 0.793629] [G loss: 0.771129] [C loss: 1.363191]\n",
      "182 [D loss: 0.774806] [G loss: 0.801985] [C loss: 1.349534]\n",
      "183 [D loss: 0.750328] [G loss: 0.835253] [C loss: 1.331170]\n",
      "184 [D loss: 0.724163] [G loss: 0.877289] [C loss: 1.364050]\n",
      "185 [D loss: 0.696960] [G loss: 0.930910] [C loss: 1.282804]\n",
      "186 [D loss: 0.664358] [G loss: 0.996785] [C loss: 1.261210]\n",
      "187 [D loss: 0.627767] [G loss: 1.083713] [C loss: 1.281860]\n",
      "188 [D loss: 0.596898] [G loss: 1.188179] [C loss: 1.307573]\n",
      "189 [D loss: 0.554696] [G loss: 1.304995] [C loss: 1.282058]\n",
      "190 [D loss: 0.527215] [G loss: 1.430032] [C loss: 1.264868]\n",
      "191 [D loss: 0.499202] [G loss: 1.560369] [C loss: 1.255271]\n",
      "192 [D loss: 0.471284] [G loss: 1.687211] [C loss: 1.241239]\n",
      "193 [D loss: 0.447110] [G loss: 1.806984] [C loss: 1.261902]\n",
      "194 [D loss: 0.432425] [G loss: 1.910571] [C loss: 1.261581]\n",
      "195 [D loss: 0.414432] [G loss: 1.994238] [C loss: 1.238182]\n",
      "196 [D loss: 0.399925] [G loss: 2.059752] [C loss: 1.226334]\n",
      "197 [D loss: 0.390243] [G loss: 2.105446] [C loss: 1.233116]\n",
      "198 [D loss: 0.383807] [G loss: 2.133279] [C loss: 1.214660]\n",
      "199 [D loss: 0.377420] [G loss: 2.136789] [C loss: 1.246276]\n",
      "200 [D loss: 0.372377] [G loss: 2.121113] [C loss: 1.234869]\n",
      "201 [D loss: 0.373107] [G loss: 2.092133] [C loss: 1.294699]\n",
      "202 [D loss: 0.374361] [G loss: 2.055230] [C loss: 1.272851]\n",
      "203 [D loss: 0.373515] [G loss: 1.995899] [C loss: 1.294762]\n",
      "204 [D loss: 0.376797] [G loss: 1.934042] [C loss: 1.276787]\n",
      "205 [D loss: 0.380151] [G loss: 1.866229] [C loss: 1.293779]\n",
      "206 [D loss: 0.385058] [G loss: 1.801770] [C loss: 1.318431]\n",
      "207 [D loss: 0.393737] [G loss: 1.723459] [C loss: 1.301684]\n",
      "208 [D loss: 0.400402] [G loss: 1.653598] [C loss: 1.329520]\n",
      "209 [D loss: 0.408333] [G loss: 1.586929] [C loss: 1.336002]\n",
      "210 [D loss: 0.422394] [G loss: 1.512935] [C loss: 1.309131]\n",
      "211 [D loss: 0.431314] [G loss: 1.448656] [C loss: 1.359620]\n",
      "212 [D loss: 0.442534] [G loss: 1.385738] [C loss: 1.411113]\n",
      "213 [D loss: 0.455266] [G loss: 1.324166] [C loss: 1.363542]\n",
      "214 [D loss: 0.467482] [G loss: 1.271501] [C loss: 1.403890]\n",
      "215 [D loss: 0.480506] [G loss: 1.216204] [C loss: 1.369058]\n",
      "216 [D loss: 0.496197] [G loss: 1.165733] [C loss: 1.372371]\n",
      "217 [D loss: 0.508672] [G loss: 1.122454] [C loss: 1.367009]\n",
      "218 [D loss: 0.520779] [G loss: 1.083488] [C loss: 1.414120]\n",
      "219 [D loss: 0.533196] [G loss: 1.051351] [C loss: 1.433567]\n",
      "220 [D loss: 0.541376] [G loss: 1.022167] [C loss: 1.355493]\n",
      "221 [D loss: 0.548140] [G loss: 0.999589] [C loss: 1.380747]\n",
      "222 [D loss: 0.557977] [G loss: 0.984709] [C loss: 1.414555]\n",
      "223 [D loss: 0.557399] [G loss: 0.972328] [C loss: 1.424442]\n",
      "224 [D loss: 0.563492] [G loss: 0.963022] [C loss: 1.420192]\n",
      "225 [D loss: 0.566492] [G loss: 0.960089] [C loss: 1.463560]\n",
      "226 [D loss: 0.566875] [G loss: 0.961572] [C loss: 1.443153]\n",
      "227 [D loss: 0.571195] [G loss: 0.962948] [C loss: 1.478746]\n",
      "228 [D loss: 0.577001] [G loss: 0.966925] [C loss: 1.402003]\n",
      "229 [D loss: 0.568628] [G loss: 0.971434] [C loss: 1.440653]\n",
      "230 [D loss: 0.575601] [G loss: 0.979098] [C loss: 1.349114]\n",
      "231 [D loss: 0.569585] [G loss: 0.992067] [C loss: 1.380346]\n",
      "232 [D loss: 0.570101] [G loss: 0.998924] [C loss: 1.366080]\n",
      "233 [D loss: 0.563061] [G loss: 1.007033] [C loss: 1.302601]\n",
      "234 [D loss: 0.568497] [G loss: 1.016037] [C loss: 1.328018]\n",
      "235 [D loss: 0.560550] [G loss: 1.027515] [C loss: 1.408121]\n",
      "236 [D loss: 0.562240] [G loss: 1.037904] [C loss: 1.336867]\n",
      "237 [D loss: 0.560481] [G loss: 1.048773] [C loss: 1.341917]\n",
      "238 [D loss: 0.556266] [G loss: 1.059641] [C loss: 1.333052]\n",
      "239 [D loss: 0.557805] [G loss: 1.065788] [C loss: 1.323900]\n",
      "240 [D loss: 0.553726] [G loss: 1.071947] [C loss: 1.286061]\n",
      "241 [D loss: 0.544022] [G loss: 1.076631] [C loss: 1.260981]\n",
      "242 [D loss: 0.549770] [G loss: 1.082008] [C loss: 1.338495]\n",
      "243 [D loss: 0.554274] [G loss: 1.084655] [C loss: 1.294521]\n",
      "244 [D loss: 0.544412] [G loss: 1.087208] [C loss: 1.257572]\n",
      "245 [D loss: 0.555216] [G loss: 1.086220] [C loss: 1.317784]\n",
      "246 [D loss: 0.544033] [G loss: 1.087317] [C loss: 1.283507]\n",
      "247 [D loss: 0.539030] [G loss: 1.082654] [C loss: 1.223030]\n",
      "248 [D loss: 0.538202] [G loss: 1.078018] [C loss: 1.304695]\n",
      "249 [D loss: 0.553917] [G loss: 1.075184] [C loss: 1.349352]\n",
      "250 [D loss: 0.545058] [G loss: 1.067475] [C loss: 1.277950]\n",
      "251 [D loss: 0.546156] [G loss: 1.058732] [C loss: 1.279180]\n",
      "252 [D loss: 0.549546] [G loss: 1.050746] [C loss: 1.290114]\n",
      "253 [D loss: 0.548554] [G loss: 1.041262] [C loss: 1.310521]\n",
      "254 [D loss: 0.558622] [G loss: 1.026254] [C loss: 1.267960]\n",
      "255 [D loss: 0.557325] [G loss: 1.015834] [C loss: 1.337870]\n",
      "256 [D loss: 0.557260] [G loss: 1.004707] [C loss: 1.299486]\n",
      "257 [D loss: 0.556101] [G loss: 0.988010] [C loss: 1.300033]\n",
      "258 [D loss: 0.560487] [G loss: 0.968988] [C loss: 1.304881]\n",
      "259 [D loss: 0.562732] [G loss: 0.954027] [C loss: 1.255527]\n",
      "260 [D loss: 0.571771] [G loss: 0.935203] [C loss: 1.313264]\n",
      "261 [D loss: 0.581823] [G loss: 0.921516] [C loss: 1.262843]\n",
      "262 [D loss: 0.575485] [G loss: 0.902619] [C loss: 1.252185]\n",
      "263 [D loss: 0.583668] [G loss: 0.888287] [C loss: 1.285243]\n",
      "264 [D loss: 0.581189] [G loss: 0.877185] [C loss: 1.318570]\n",
      "265 [D loss: 0.581583] [G loss: 0.866742] [C loss: 1.262366]\n",
      "266 [D loss: 0.587517] [G loss: 0.860574] [C loss: 1.297949]\n",
      "267 [D loss: 0.580486] [G loss: 0.851642] [C loss: 1.300899]\n",
      "268 [D loss: 0.579481] [G loss: 0.843940] [C loss: 1.294173]\n",
      "269 [D loss: 0.574127] [G loss: 0.840629] [C loss: 1.261788]\n",
      "270 [D loss: 0.584159] [G loss: 0.837608] [C loss: 1.270208]\n",
      "271 [D loss: 0.578751] [G loss: 0.831888] [C loss: 1.263697]\n",
      "272 [D loss: 0.577195] [G loss: 0.828713] [C loss: 1.292125]\n",
      "273 [D loss: 0.573829] [G loss: 0.826619] [C loss: 1.242697]\n",
      "274 [D loss: 0.565150] [G loss: 0.823367] [C loss: 1.226056]\n",
      "275 [D loss: 0.569574] [G loss: 0.824318] [C loss: 1.242755]\n",
      "276 [D loss: 0.570950] [G loss: 0.818556] [C loss: 1.273590]\n",
      "277 [D loss: 0.571737] [G loss: 0.814708] [C loss: 1.258174]\n",
      "278 [D loss: 0.572253] [G loss: 0.813870] [C loss: 1.270208]\n",
      "279 [D loss: 0.568385] [G loss: 0.806728] [C loss: 1.314743]\n",
      "280 [D loss: 0.568914] [G loss: 0.801590] [C loss: 1.289279]\n",
      "281 [D loss: 0.575736] [G loss: 0.794394] [C loss: 1.303619]\n",
      "282 [D loss: 0.575410] [G loss: 0.789785] [C loss: 1.261539]\n",
      "283 [D loss: 0.572875] [G loss: 0.779245] [C loss: 1.295226]\n",
      "284 [D loss: 0.574473] [G loss: 0.770970] [C loss: 1.310623]\n",
      "285 [D loss: 0.583705] [G loss: 0.761383] [C loss: 1.284762]\n",
      "286 [D loss: 0.580444] [G loss: 0.750203] [C loss: 1.306575]\n",
      "287 [D loss: 0.590499] [G loss: 0.742154] [C loss: 1.273767]\n",
      "288 [D loss: 0.594036] [G loss: 0.735476] [C loss: 1.404617]\n",
      "289 [D loss: 0.593073] [G loss: 0.720642] [C loss: 1.335466]\n",
      "290 [D loss: 0.598314] [G loss: 0.713121] [C loss: 1.347941]\n",
      "291 [D loss: 0.602311] [G loss: 0.703679] [C loss: 1.323261]\n",
      "292 [D loss: 0.606653] [G loss: 0.697287] [C loss: 1.339555]\n",
      "293 [D loss: 0.610439] [G loss: 0.688375] [C loss: 1.331533]\n",
      "294 [D loss: 0.610179] [G loss: 0.684387] [C loss: 1.337270]\n",
      "295 [D loss: 0.613861] [G loss: 0.678131] [C loss: 1.334193]\n",
      "296 [D loss: 0.615488] [G loss: 0.673057] [C loss: 1.345624]\n",
      "297 [D loss: 0.618809] [G loss: 0.669864] [C loss: 1.377190]\n",
      "298 [D loss: 0.621401] [G loss: 0.667027] [C loss: 1.336461]\n",
      "299 [D loss: 0.620659] [G loss: 0.662526] [C loss: 1.303226]\n",
      "300 [D loss: 0.624495] [G loss: 0.660765] [C loss: 1.312449]\n",
      "301 [D loss: 0.626395] [G loss: 0.654293] [C loss: 1.338338]\n",
      "302 [D loss: 0.625773] [G loss: 0.651481] [C loss: 1.299974]\n",
      "303 [D loss: 0.632174] [G loss: 0.647003] [C loss: 1.363501]\n",
      "304 [D loss: 0.635544] [G loss: 0.645414] [C loss: 1.262896]\n",
      "305 [D loss: 0.634625] [G loss: 0.638831] [C loss: 1.337580]\n",
      "306 [D loss: 0.634830] [G loss: 0.636403] [C loss: 1.300687]\n",
      "307 [D loss: 0.639397] [G loss: 0.635347] [C loss: 1.311614]\n",
      "308 [D loss: 0.642490] [G loss: 0.629866] [C loss: 1.350265]\n",
      "309 [D loss: 0.640310] [G loss: 0.627643] [C loss: 1.305956]\n",
      "310 [D loss: 0.642870] [G loss: 0.625564] [C loss: 1.279482]\n",
      "311 [D loss: 0.647741] [G loss: 0.622999] [C loss: 1.315906]\n",
      "312 [D loss: 0.647681] [G loss: 0.618626] [C loss: 1.312561]\n",
      "313 [D loss: 0.647405] [G loss: 0.616288] [C loss: 1.280846]\n",
      "314 [D loss: 0.651300] [G loss: 0.616728] [C loss: 1.315682]\n",
      "315 [D loss: 0.654466] [G loss: 0.612338] [C loss: 1.237401]\n",
      "316 [D loss: 0.657372] [G loss: 0.614008] [C loss: 1.240431]\n",
      "317 [D loss: 0.656732] [G loss: 0.610616] [C loss: 1.241259]\n",
      "318 [D loss: 0.656377] [G loss: 0.608726] [C loss: 1.255120]\n",
      "319 [D loss: 0.660919] [G loss: 0.608732] [C loss: 1.228640]\n",
      "320 [D loss: 0.659256] [G loss: 0.604899] [C loss: 1.200456]\n",
      "321 [D loss: 0.665432] [G loss: 0.603073] [C loss: 1.201397]\n",
      "322 [D loss: 0.665152] [G loss: 0.600919] [C loss: 1.216283]\n",
      "323 [D loss: 0.666397] [G loss: 0.599716] [C loss: 1.167234]\n",
      "324 [D loss: 0.668410] [G loss: 0.599339] [C loss: 1.205704]\n",
      "325 [D loss: 0.671400] [G loss: 0.596441] [C loss: 1.196798]\n",
      "326 [D loss: 0.672211] [G loss: 0.595606] [C loss: 1.160612]\n",
      "327 [D loss: 0.674022] [G loss: 0.593863] [C loss: 1.168683]\n",
      "328 [D loss: 0.680138] [G loss: 0.591881] [C loss: 1.146165]\n",
      "329 [D loss: 0.680407] [G loss: 0.590059] [C loss: 1.151275]\n",
      "330 [D loss: 0.683071] [G loss: 0.585442] [C loss: 1.118537]\n",
      "331 [D loss: 0.686182] [G loss: 0.586352] [C loss: 1.104263]\n",
      "332 [D loss: 0.690968] [G loss: 0.583086] [C loss: 1.190459]\n",
      "333 [D loss: 0.693624] [G loss: 0.580543] [C loss: 1.146697]\n",
      "334 [D loss: 0.698532] [G loss: 0.577047] [C loss: 1.134804]\n",
      "335 [D loss: 0.700646] [G loss: 0.576298] [C loss: 1.139673]\n",
      "336 [D loss: 0.707187] [G loss: 0.571975] [C loss: 1.108895]\n",
      "337 [D loss: 0.709126] [G loss: 0.571459] [C loss: 1.095699]\n",
      "338 [D loss: 0.713283] [G loss: 0.567759] [C loss: 1.127021]\n",
      "339 [D loss: 0.716838] [G loss: 0.566553] [C loss: 1.157662]\n",
      "340 [D loss: 0.721864] [G loss: 0.564082] [C loss: 1.098218]\n",
      "341 [D loss: 0.723862] [G loss: 0.561580] [C loss: 1.071573]\n",
      "342 [D loss: 0.725810] [G loss: 0.564300] [C loss: 1.100285]\n",
      "343 [D loss: 0.728763] [G loss: 0.563907] [C loss: 1.117256]\n",
      "344 [D loss: 0.735617] [G loss: 0.564841] [C loss: 1.066418]\n",
      "345 [D loss: 0.735694] [G loss: 0.570229] [C loss: 1.067162]\n",
      "346 [D loss: 0.733590] [G loss: 0.573775] [C loss: 1.070505]\n",
      "347 [D loss: 0.737901] [G loss: 0.578082] [C loss: 1.084496]\n",
      "348 [D loss: 0.734007] [G loss: 0.586848] [C loss: 1.079662]\n",
      "349 [D loss: 0.736838] [G loss: 0.593440] [C loss: 1.124034]\n",
      "350 [D loss: 0.733149] [G loss: 0.604116] [C loss: 1.066433]\n",
      "351 [D loss: 0.731481] [G loss: 0.613823] [C loss: 1.062555]\n",
      "352 [D loss: 0.729259] [G loss: 0.627678] [C loss: 1.093757]\n",
      "353 [D loss: 0.730550] [G loss: 0.641474] [C loss: 1.132943]\n",
      "354 [D loss: 0.724785] [G loss: 0.656954] [C loss: 1.088838]\n",
      "355 [D loss: 0.717914] [G loss: 0.674007] [C loss: 1.020558]\n",
      "356 [D loss: 0.709883] [G loss: 0.694584] [C loss: 1.091569]\n",
      "357 [D loss: 0.707047] [G loss: 0.713518] [C loss: 1.090048]\n",
      "358 [D loss: 0.703658] [G loss: 0.734573] [C loss: 1.072255]\n",
      "359 [D loss: 0.697659] [G loss: 0.755896] [C loss: 1.032681]\n",
      "360 [D loss: 0.689837] [G loss: 0.780104] [C loss: 1.024897]\n",
      "361 [D loss: 0.685282] [G loss: 0.803833] [C loss: 0.996791]\n",
      "362 [D loss: 0.681361] [G loss: 0.824907] [C loss: 1.016857]\n",
      "363 [D loss: 0.674179] [G loss: 0.844686] [C loss: 1.046854]\n",
      "364 [D loss: 0.669524] [G loss: 0.868564] [C loss: 0.983855]\n",
      "365 [D loss: 0.669027] [G loss: 0.886621] [C loss: 0.999663]\n",
      "366 [D loss: 0.659245] [G loss: 0.904963] [C loss: 0.973261]\n",
      "367 [D loss: 0.658304] [G loss: 0.921018] [C loss: 1.006379]\n",
      "368 [D loss: 0.653724] [G loss: 0.934247] [C loss: 0.996597]\n",
      "369 [D loss: 0.648249] [G loss: 0.946433] [C loss: 0.950477]\n",
      "370 [D loss: 0.645944] [G loss: 0.954061] [C loss: 0.973208]\n",
      "371 [D loss: 0.643868] [G loss: 0.962121] [C loss: 0.945630]\n",
      "372 [D loss: 0.633765] [G loss: 0.968074] [C loss: 0.936031]\n",
      "373 [D loss: 0.632274] [G loss: 0.969887] [C loss: 1.013830]\n",
      "374 [D loss: 0.635298] [G loss: 0.970910] [C loss: 0.972389]\n",
      "375 [D loss: 0.635141] [G loss: 0.971880] [C loss: 0.943456]\n",
      "376 [D loss: 0.635093] [G loss: 0.968275] [C loss: 0.960348]\n",
      "377 [D loss: 0.629353] [G loss: 0.964078] [C loss: 0.952791]\n",
      "378 [D loss: 0.629961] [G loss: 0.960243] [C loss: 0.929899]\n",
      "379 [D loss: 0.628932] [G loss: 0.952806] [C loss: 0.924840]\n",
      "380 [D loss: 0.632523] [G loss: 0.945251] [C loss: 0.929524]\n",
      "381 [D loss: 0.631635] [G loss: 0.936016] [C loss: 0.924292]\n",
      "382 [D loss: 0.631493] [G loss: 0.927228] [C loss: 0.935987]\n",
      "383 [D loss: 0.632877] [G loss: 0.917781] [C loss: 0.920297]\n",
      "384 [D loss: 0.633764] [G loss: 0.905575] [C loss: 0.956270]\n",
      "385 [D loss: 0.637192] [G loss: 0.896926] [C loss: 0.904315]\n",
      "386 [D loss: 0.640345] [G loss: 0.884328] [C loss: 0.887683]\n",
      "387 [D loss: 0.644419] [G loss: 0.872325] [C loss: 0.897188]\n",
      "388 [D loss: 0.646072] [G loss: 0.859387] [C loss: 0.936875]\n",
      "389 [D loss: 0.643606] [G loss: 0.848264] [C loss: 0.848954]\n",
      "390 [D loss: 0.647800] [G loss: 0.835570] [C loss: 0.879760]\n",
      "391 [D loss: 0.651566] [G loss: 0.824242] [C loss: 0.914676]\n",
      "392 [D loss: 0.656607] [G loss: 0.812118] [C loss: 0.856943]\n",
      "393 [D loss: 0.662018] [G loss: 0.802143] [C loss: 0.826338]\n",
      "394 [D loss: 0.659914] [G loss: 0.792937] [C loss: 0.917928]\n",
      "395 [D loss: 0.664049] [G loss: 0.780243] [C loss: 0.864685]\n",
      "396 [D loss: 0.665953] [G loss: 0.770595] [C loss: 0.878401]\n",
      "397 [D loss: 0.668996] [G loss: 0.761965] [C loss: 0.804939]\n",
      "398 [D loss: 0.672563] [G loss: 0.752838] [C loss: 0.853347]\n",
      "399 [D loss: 0.672744] [G loss: 0.745694] [C loss: 0.860910]\n",
      "400 [D loss: 0.671580] [G loss: 0.738814] [C loss: 0.827442]\n",
      "401 [D loss: 0.676528] [G loss: 0.731447] [C loss: 0.866200]\n",
      "402 [D loss: 0.671331] [G loss: 0.727125] [C loss: 0.866792]\n",
      "403 [D loss: 0.671791] [G loss: 0.722637] [C loss: 0.880623]\n",
      "404 [D loss: 0.674291] [G loss: 0.719086] [C loss: 0.877528]\n",
      "405 [D loss: 0.674310] [G loss: 0.715745] [C loss: 0.830678]\n",
      "406 [D loss: 0.669836] [G loss: 0.712644] [C loss: 0.846994]\n",
      "407 [D loss: 0.667893] [G loss: 0.711763] [C loss: 0.867245]\n",
      "408 [D loss: 0.667098] [G loss: 0.710751] [C loss: 0.831513]\n",
      "409 [D loss: 0.668380] [G loss: 0.712705] [C loss: 0.773031]\n",
      "410 [D loss: 0.659548] [G loss: 0.712498] [C loss: 0.862305]\n",
      "411 [D loss: 0.658100] [G loss: 0.713632] [C loss: 0.840352]\n",
      "412 [D loss: 0.655183] [G loss: 0.715960] [C loss: 0.836270]\n",
      "413 [D loss: 0.653082] [G loss: 0.718467] [C loss: 0.816485]\n",
      "414 [D loss: 0.648566] [G loss: 0.721992] [C loss: 0.826310]\n",
      "415 [D loss: 0.647056] [G loss: 0.725709] [C loss: 0.848961]\n",
      "416 [D loss: 0.642063] [G loss: 0.728934] [C loss: 0.833987]\n",
      "417 [D loss: 0.638562] [G loss: 0.732574] [C loss: 0.836842]\n",
      "418 [D loss: 0.633921] [G loss: 0.738845] [C loss: 0.786036]\n",
      "419 [D loss: 0.626602] [G loss: 0.745130] [C loss: 0.832140]\n",
      "420 [D loss: 0.625185] [G loss: 0.751234] [C loss: 0.785473]\n",
      "421 [D loss: 0.621097] [G loss: 0.759116] [C loss: 0.805063]\n",
      "422 [D loss: 0.614366] [G loss: 0.765647] [C loss: 0.851754]\n",
      "423 [D loss: 0.608568] [G loss: 0.774470] [C loss: 0.790445]\n",
      "424 [D loss: 0.603455] [G loss: 0.783020] [C loss: 0.807104]\n",
      "425 [D loss: 0.595874] [G loss: 0.791988] [C loss: 0.829423]\n",
      "426 [D loss: 0.591922] [G loss: 0.801196] [C loss: 0.817162]\n",
      "427 [D loss: 0.587519] [G loss: 0.809878] [C loss: 0.767231]\n",
      "428 [D loss: 0.581410] [G loss: 0.818937] [C loss: 0.759765]\n",
      "429 [D loss: 0.576219] [G loss: 0.827711] [C loss: 0.760947]\n",
      "430 [D loss: 0.571851] [G loss: 0.837224] [C loss: 0.761382]\n",
      "431 [D loss: 0.567526] [G loss: 0.847205] [C loss: 0.774012]\n",
      "432 [D loss: 0.563757] [G loss: 0.855148] [C loss: 0.762134]\n",
      "433 [D loss: 0.557200] [G loss: 0.863061] [C loss: 0.743720]\n",
      "434 [D loss: 0.554155] [G loss: 0.869224] [C loss: 0.741509]\n",
      "435 [D loss: 0.546147] [G loss: 0.877378] [C loss: 0.761007]\n",
      "436 [D loss: 0.545939] [G loss: 0.885209] [C loss: 0.713668]\n",
      "437 [D loss: 0.542025] [G loss: 0.891379] [C loss: 0.767761]\n",
      "438 [D loss: 0.536804] [G loss: 0.895898] [C loss: 0.756741]\n",
      "439 [D loss: 0.535188] [G loss: 0.899063] [C loss: 0.745361]\n",
      "440 [D loss: 0.531396] [G loss: 0.903227] [C loss: 0.781499]\n",
      "441 [D loss: 0.529987] [G loss: 0.905178] [C loss: 0.728106]\n",
      "442 [D loss: 0.528795] [G loss: 0.907458] [C loss: 0.753653]\n",
      "443 [D loss: 0.526948] [G loss: 0.907910] [C loss: 0.741907]\n",
      "444 [D loss: 0.523953] [G loss: 0.907526] [C loss: 0.728959]\n",
      "445 [D loss: 0.522559] [G loss: 0.905569] [C loss: 0.746862]\n",
      "446 [D loss: 0.522319] [G loss: 0.903251] [C loss: 0.769321]\n",
      "447 [D loss: 0.524538] [G loss: 0.899671] [C loss: 0.740052]\n",
      "448 [D loss: 0.522200] [G loss: 0.897600] [C loss: 0.716469]\n",
      "449 [D loss: 0.521982] [G loss: 0.892676] [C loss: 0.740673]\n",
      "450 [D loss: 0.523629] [G loss: 0.887936] [C loss: 0.765174]\n",
      "451 [D loss: 0.521856] [G loss: 0.884430] [C loss: 0.763375]\n",
      "452 [D loss: 0.523709] [G loss: 0.877845] [C loss: 0.773001]\n",
      "453 [D loss: 0.524409] [G loss: 0.873289] [C loss: 0.756417]\n",
      "454 [D loss: 0.524739] [G loss: 0.868761] [C loss: 0.744042]\n",
      "455 [D loss: 0.523694] [G loss: 0.862704] [C loss: 0.756512]\n",
      "456 [D loss: 0.524000] [G loss: 0.857187] [C loss: 0.787874]\n",
      "457 [D loss: 0.525256] [G loss: 0.853099] [C loss: 0.793497]\n",
      "458 [D loss: 0.524990] [G loss: 0.849014] [C loss: 0.777118]\n",
      "459 [D loss: 0.523098] [G loss: 0.847993] [C loss: 0.782399]\n",
      "460 [D loss: 0.525792] [G loss: 0.845312] [C loss: 0.859029]\n",
      "461 [D loss: 0.524922] [G loss: 0.843734] [C loss: 0.808853]\n",
      "462 [D loss: 0.523260] [G loss: 0.840561] [C loss: 0.813558]\n",
      "463 [D loss: 0.522281] [G loss: 0.841258] [C loss: 0.823735]\n",
      "464 [D loss: 0.523183] [G loss: 0.840299] [C loss: 0.810245]\n",
      "465 [D loss: 0.521448] [G loss: 0.840856] [C loss: 0.896717]\n",
      "466 [D loss: 0.517697] [G loss: 0.842222] [C loss: 0.888709]\n",
      "467 [D loss: 0.516008] [G loss: 0.843268] [C loss: 0.860809]\n",
      "468 [D loss: 0.515622] [G loss: 0.845342] [C loss: 0.875386]\n",
      "469 [D loss: 0.511680] [G loss: 0.848221] [C loss: 0.880311]\n",
      "470 [D loss: 0.510230] [G loss: 0.851821] [C loss: 0.860987]\n",
      "471 [D loss: 0.508885] [G loss: 0.855294] [C loss: 0.892286]\n",
      "472 [D loss: 0.505088] [G loss: 0.860608] [C loss: 0.876286]\n",
      "473 [D loss: 0.503408] [G loss: 0.864334] [C loss: 0.889626]\n",
      "474 [D loss: 0.499694] [G loss: 0.871445] [C loss: 0.860754]\n",
      "475 [D loss: 0.497007] [G loss: 0.876175] [C loss: 0.817990]\n",
      "476 [D loss: 0.495950] [G loss: 0.882497] [C loss: 0.805221]\n",
      "477 [D loss: 0.492283] [G loss: 0.886371] [C loss: 0.830315]\n",
      "478 [D loss: 0.490671] [G loss: 0.892880] [C loss: 0.824673]\n",
      "479 [D loss: 0.487687] [G loss: 0.895154] [C loss: 0.842531]\n",
      "480 [D loss: 0.485712] [G loss: 0.898541] [C loss: 0.765097]\n",
      "481 [D loss: 0.486233] [G loss: 0.899704] [C loss: 0.767951]\n",
      "482 [D loss: 0.485596] [G loss: 0.897931] [C loss: 0.813246]\n",
      "483 [D loss: 0.485865] [G loss: 0.893485] [C loss: 0.827374]\n",
      "484 [D loss: 0.489324] [G loss: 0.886934] [C loss: 0.795524]\n",
      "485 [D loss: 0.491581] [G loss: 0.875142] [C loss: 0.779193]\n",
      "486 [D loss: 0.499938] [G loss: 0.860801] [C loss: 0.789145]\n",
      "487 [D loss: 0.508277] [G loss: 0.841901] [C loss: 0.786914]\n",
      "488 [D loss: 0.514701] [G loss: 0.820364] [C loss: 0.775944]\n",
      "489 [D loss: 0.526589] [G loss: 0.796245] [C loss: 0.771414]\n",
      "490 [D loss: 0.540568] [G loss: 0.770003] [C loss: 0.791250]\n",
      "491 [D loss: 0.553925] [G loss: 0.742248] [C loss: 0.772353]\n",
      "492 [D loss: 0.570362] [G loss: 0.713720] [C loss: 0.783167]\n",
      "493 [D loss: 0.585022] [G loss: 0.686082] [C loss: 0.811542]\n",
      "494 [D loss: 0.603628] [G loss: 0.660424] [C loss: 0.757830]\n",
      "495 [D loss: 0.619389] [G loss: 0.636635] [C loss: 0.811485]\n",
      "496 [D loss: 0.637909] [G loss: 0.615021] [C loss: 0.800207]\n",
      "497 [D loss: 0.653008] [G loss: 0.596743] [C loss: 0.855417]\n",
      "498 [D loss: 0.669021] [G loss: 0.583536] [C loss: 0.817628]\n",
      "499 [D loss: 0.679469] [G loss: 0.571720] [C loss: 0.839305]\n",
      "CPU times: user 1h 9min 5s, sys: 55min 39s, total: 2h 4min 44s\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf.keras.backend.clear_session()\n",
    "random.seed(123)\n",
    "#Hyperparameters\n",
    "n_epochs = 100\n",
    "p_missing = 1\n",
    "\n",
    "#Loop\n",
    "classifier = define_classifier()\n",
    "discriminator = define_discriminator()\n",
    "generator = define_generator_impute()\n",
    "gan_model = define_gan(g_model = generator, d_model = discriminator, c_model = classifier)\n",
    "pred_realfake, pred_labels = train(g_model = generator, d_model = discriminator, c_model = classifier, \n",
    "                               gan_model=gan_model, p_missing=p_missing,\n",
    "                               x_train = x_train, y_train = y_train,\n",
    "                               x_test = x_test, n_epochs=n_epochs)\n",
    "\n",
    "rounded_labels= np.argmax(pred_labels, axis = 1)\n",
    "\n",
    "pred_metrics = pd.DataFrame({'ClassifierType': \"Naive GAN\",\n",
    "                    'Accuracy': accuracy_score(y_test, rounded_labels), \n",
    "                     'Precision': precision_score(y_test, rounded_labels, average=\"macro\"),\n",
    "                     'Recall': recall_score(y_test, rounded_labels, average=\"macro\"),\n",
    "                     'BalancedAccuracy': balanced_accuracy_score(y_true=y_test, y_pred = rounded_labels),\n",
    "                     'F1-score': f1_score(y_true=y_test, y_pred=rounded_labels, average=\"macro\")\n",
    "                    }, index = [0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "separated-martin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClassifierType</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>BalancedAccuracy</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive GAN</td>\n",
       "      <td>0.688584</td>\n",
       "      <td>0.568496</td>\n",
       "      <td>0.488597</td>\n",
       "      <td>0.488597</td>\n",
       "      <td>0.488749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ClassifierType  Accuracy  Precision    Recall  BalancedAccuracy  F1-score\n",
       "0      Naive GAN  0.688584   0.568496  0.488597          0.488597  0.488749"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-shelter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-drink",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-principle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-timothy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
