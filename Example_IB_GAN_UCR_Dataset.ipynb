{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_tensorflow2_p36",
      "language": "python",
      "name": "conda_tensorflow2_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "Example IB-GAN - UCR Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asian-contrary"
      },
      "source": [
        "May 4, 2021\n",
        "\n",
        "### Example of Baseline vs IB-GAN using the UCR dataset SpokenArabicDigits"
      ],
      "id": "asian-contrary"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "regular-fireplace"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval"
      ],
      "id": "regular-fireplace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equal-baghdad"
      },
      "source": [
        "import keras\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support"
      ],
      "id": "equal-baghdad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sustained-recycling"
      },
      "source": [
        "import random\n",
        "from numpy import zeros, ones, expand_dims\n",
        "from numpy.random import randn, randint\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Embedding, Dense, LSTM, Multiply, Add, Lambda\n",
        "from keras.layers import Reshape, Flatten, Activation, Concatenate\n",
        "from keras.layers import Conv1D, ZeroPadding1D, MaxPooling1D, Conv2D, Conv2DTranspose, TimeDistributed\n",
        "from keras.layers import BatchNormalization, Dropout, LeakyReLU, RepeatVector, ReLU, GlobalAveragePooling1D\n",
        "from keras.initializers import RandomNormal\n",
        "from matplotlib import pyplot\n",
        "from keras.metrics import Precision, Recall, AUC\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, auc, confusion_matrix, classification_report, precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score, accuracy_score, f1_score\n",
        "from itertools import chain\n"
      ],
      "id": "sustained-recycling",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "covered-organizer"
      },
      "source": [
        "### Load & Process Data"
      ],
      "id": "covered-organizer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accessory-panama",
        "outputId": "6ab091bd-45a9-4099-8691-dd39c15c6962"
      },
      "source": [
        "#State UCR dataset\n",
        "filefolder = \"SpokenArabicDigits\"\n",
        "trainx_key = 'UCR/' + filefolder + '/X_train.npy'\n",
        "trainy_key ='UCR/' + filefolder + '/y_train.npy'\n",
        "testx_key ='UCR/' + filefolder + '/X_test.npy'\n",
        "testy_key ='UCR/' + filefolder + '/y_test.npy'\n",
        "\n",
        "x_train = np.load(trainx_key)\n",
        "y_train = np.load(trainy_key)\n",
        "x_test = np.load(testx_key)\n",
        "y_test = np.load(testy_key)\n"
      ],
      "id": "accessory-panama",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6599, 93, 13) (6599, 1) (2199, 93, 13) (2199, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "persistent-reproduction",
        "outputId": "f4300901-7612-49d7-987e-edfc16a422c9"
      },
      "source": [
        "#See class frequencies \n",
        "np.unique(y_train, return_counts=True)"
      ],
      "id": "persistent-reproduction",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
              " array([660, 660, 660, 660, 660, 660, 660, 660, 660, 659]))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fifth-aggregate"
      },
      "source": [
        "#Introduce class imbalance\n",
        "np.random.seed(2021)\n",
        "minority_index = [k for k in range(len(y_train)) if y_train[k] in [0, 1, 2, 3, 4]]\n",
        "toRemove = [i for i in minority_index if np.random.rand(1) < 0.75]\n",
        "x_train = np.delete(x_train, toRemove, axis=0)\n",
        "y_train = np.delete(y_train, toRemove, axis=0)\n",
        "\n",
        "minority_index = [k for k in range(len(y_test)) if y_test[k] in [0, 1, 2, 3, 4]]\n",
        "toRemove = [i for i in minority_index if np.random.rand(1) < 0.75]\n",
        "x_test = np.delete(x_test, toRemove, axis=0)\n",
        "y_test = np.delete(y_test, toRemove, axis=0)"
      ],
      "id": "fifth-aggregate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "steady-interview",
        "outputId": "1aeecea7-ab60-4de9-f2a1-28c8c18589d0"
      },
      "source": [
        "#See class frequencies after imbalance\n",
        "uniq_vals, uniq_counts = np.unique(y_train, return_counts=True)\n",
        "np.unique(y_train, return_counts=True)"
      ],
      "id": "steady-interview",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
              " array([158, 175, 162, 176, 176, 660, 660, 660, 660, 659]))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seeing-librarian",
        "outputId": "fbbd17b2-492a-4aee-bd19-e00da9f4e894"
      },
      "source": [
        "#Set class weights\n",
        "weights = np.zeros(len(uniq_vals))\n",
        "class_weight = {}\n",
        "for i in list(range(len(uniq_vals))):\n",
        "    weights[i] = y_train.shape[0]/(len(uniq_vals) * uniq_counts[i])\n",
        "    class_weight.update({uniq_vals[i] : weights[i]})\n",
        "print(class_weight)"
      ],
      "id": "seeing-librarian",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 2.6240506329113926, 1: 2.369142857142857, 2: 2.5592592592592593, 3: 2.355681818181818, 4: 2.355681818181818, 5: 0.6281818181818182, 6: 0.6281818181818182, 7: 0.6281818181818182, 8: 0.6281818181818182, 9: 0.6291350531107739}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extra-stadium"
      },
      "source": [
        "#Training parameters\n",
        "n_classes = len(uniq_vals)\n",
        "n_epochs= 10\n",
        "n_samples = max(int(x_train.shape[0]/5), 2*n_classes)\n",
        "p_missing = 0.1\n",
        "\n",
        "\n",
        "#Set parameters of data\n",
        "max_sequence_length = x_train.shape[1]\n",
        "n_features = x_train.shape[2]\n",
        "k = 2\n"
      ],
      "id": "extra-stadium",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "illegal-sugar"
      },
      "source": [
        "#Create onehot\n",
        "y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=n_classes)\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=n_classes)"
      ],
      "id": "illegal-sugar",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "explicit-civilian"
      },
      "source": [
        "### 1. Baseline Classifier"
      ],
      "id": "explicit-civilian"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miniature-california"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "id": "miniature-california",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "developed-millennium",
        "outputId": "83887e50-73f3-4749-b782-34af3c3693f0"
      },
      "source": [
        "def single_CNN(ts_shape = (max_sequence_length, n_features)):\n",
        "    # time series input\n",
        "    in_ts = Input(shape=ts_shape)\n",
        "    filter1 = n_features * 2\n",
        "    fmaps = n_classes*2\n",
        "    fe = Conv1D(filter1, kernel_size=k, padding='same', input_shape = (max_sequence_length, n_features))(in_ts)\n",
        "    fe = BatchNormalization()(fe)\n",
        "    fe = MaxPooling1D(pool_size=(2))(fe)\n",
        "    fe = Conv1D(1, kernel_size=k, padding='same', input_shape = (max_sequence_length, n_features))(fe)\n",
        "    fe = BatchNormalization()(fe)\n",
        "    fe = MaxPooling1D(pool_size=(2))(fe)\n",
        "    #Fully connected layers\n",
        "    fe = Flatten()(fe)\n",
        "    fe = Dense(fmaps)(fe)\n",
        "    out_labels = Dense(n_classes, activation='softmax', name=\"cnn_labels\")(fe)\n",
        "\n",
        "    model = Model(inputs=in_ts, outputs=out_labels, name=\"CNN_Classifier\")\n",
        "    opt = Adam()\n",
        "    losses = {\"cnn_labels\":'categorical_crossentropy'}\n",
        "    model.compile(loss=losses, optimizer=opt, metrics = ['categorical_accuracy']) \n",
        "    model.summary() #prints out layers of model\n",
        "    return model\n",
        "\n",
        "baseline_classifier = single_CNN()"
      ],
      "id": "developed-millennium",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"CNN_Classifier\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 93, 13)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 93, 26)            702       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 93, 26)            104       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 46, 26)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 46, 1)             53        \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 46, 1)             4         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 23, 1)             0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 23)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                480       \n",
            "_________________________________________________________________\n",
            "cnn_labels (Dense)           (None, 10)                210       \n",
            "=================================================================\n",
            "Total params: 1,553\n",
            "Trainable params: 1,499\n",
            "Non-trainable params: 54\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alert-tyler",
        "outputId": "b67b47d5-6464-4137-e26e-2855627743ab"
      },
      "source": [
        "#Run one iteration of baseline classifier\n",
        "tf.keras.backend.clear_session()\n",
        "random.seed(123)\n",
        "baseline_classifier = single_CNN()\n",
        "baseline_classifier.fit(x_train, y_train_onehot,verbose=1)\n",
        "pred_cnn = baseline_classifier.predict(x_test)\n",
        "rounded_cnn_labels= np.argmax(pred_cnn, axis=1)\n",
        "\n",
        "baseline_metrics = pd.DataFrame({'ClassifierType': \"Baseline No Class Weights\",\n",
        "                            'Accuracy': accuracy_score(y_test, rounded_cnn_labels), \n",
        "                             'Precision': precision_score(y_test, rounded_cnn_labels, average=\"macro\"),\n",
        "                             'Recall': recall_score(y_test, rounded_cnn_labels, average=\"macro\"),\n",
        "                             'BalancedAccuracy': balanced_accuracy_score(y_true=y_test, y_pred = rounded_cnn_labels),\n",
        "                             'F1-score': f1_score(y_true=y_test, y_pred=rounded_cnn_labels, average=\"macro\")\n",
        "                            }, index=[0])\n",
        "print(baseline_metrics)"
      ],
      "id": "alert-tyler",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"CNN_Classifier\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 93, 13)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 93, 26)            702       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 93, 26)            104       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 46, 26)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 46, 1)             53        \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 46, 1)             4         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 23, 1)             0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 23)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                480       \n",
            "_________________________________________________________________\n",
            "cnn_labels (Dense)           (None, 10)                210       \n",
            "=================================================================\n",
            "Total params: 1,553\n",
            "Trainable params: 1,499\n",
            "Non-trainable params: 54\n",
            "_________________________________________________________________\n",
            "Epoch 1/1\n",
            "4146/4146 [==============================] - 3s 835us/step - loss: 1.7960 - categorical_accuracy: 0.3970\n",
            "              ClassifierType  Accuracy  Precision    Recall  BalancedAccuracy  \\\n",
            "0  Baseline No Class Weights  0.669798   0.485344  0.471718          0.471718   \n",
            "\n",
            "   F1-score  \n",
            "0  0.455928  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wound-knowing",
        "outputId": "6c702b35-efae-4b62-c806-8cafe306dd47"
      },
      "source": [
        "#Run one iteration of baseline classifier with class weights\n",
        "tf.keras.backend.clear_session()\n",
        "random.seed(123)\n",
        "baseline_classifier = single_CNN()\n",
        "baseline_classifier.fit(x_train, y_train_onehot,class_weight = class_weight, verbose=1)\n",
        "pred_cnn = baseline_classifier.predict(x_test)\n",
        "rounded_cnn_labels= np.argmax(pred_cnn, axis=1)\n",
        "\n",
        "baseline_metrics = pd.DataFrame({'ClassifierType': \"Baseline with Class Weights\",\n",
        "                            'Accuracy': accuracy_score(y_test, rounded_cnn_labels), \n",
        "                             'Precision': precision_score(y_test, rounded_cnn_labels, average=\"macro\"),\n",
        "                             'Recall': recall_score(y_test, rounded_cnn_labels, average=\"macro\"),\n",
        "                             'BalancedAccuracy': balanced_accuracy_score(y_true=y_test, y_pred = rounded_cnn_labels),\n",
        "                             'F1-score': f1_score(y_true=y_test, y_pred=rounded_cnn_labels, average=\"macro\")\n",
        "                            }, index=[0])\n",
        "print(baseline_metrics)"
      ],
      "id": "wound-knowing",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"CNN_Classifier\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 93, 13)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 93, 26)            702       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 93, 26)            104       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 46, 26)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 46, 1)             53        \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 46, 1)             4         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 23, 1)             0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 23)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                480       \n",
            "_________________________________________________________________\n",
            "cnn_labels (Dense)           (None, 10)                210       \n",
            "=================================================================\n",
            "Total params: 1,553\n",
            "Trainable params: 1,499\n",
            "Non-trainable params: 54\n",
            "_________________________________________________________________\n",
            "Epoch 1/1\n",
            "4146/4146 [==============================] - 4s 850us/step - loss: 1.9024 - categorical_accuracy: 0.3763\n",
            "                ClassifierType  Accuracy  Precision    Recall  \\\n",
            "0  Baseline with Class Weights  0.611272   0.551179  0.569021   \n",
            "\n",
            "   BalancedAccuracy  F1-score  \n",
            "0          0.569021  0.526474  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "disabled-shopping"
      },
      "source": [
        "### 2. Imputation Balanced GAN"
      ],
      "id": "disabled-shopping"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noble-update"
      },
      "source": [
        "def define_classifier(ts_shape=(max_sequence_length, n_features),\n",
        "                           hint_shape = (max_sequence_length, n_features)):\n",
        "    # time series input\n",
        "    in_ts = Input(shape=ts_shape)\n",
        "    in_hint = Input(shape= hint_shape)\n",
        "    filter1 = n_features * 2\n",
        "    fmaps = n_classes*2\n",
        "    fe = Conv1D(filter1, kernel_size=k, padding='same', input_shape = (max_sequence_length, n_features))(in_ts)\n",
        "    fe = BatchNormalization()(fe)\n",
        "    fe = MaxPooling1D(pool_size=(2))(fe)\n",
        "    fe = Conv1D(1, kernel_size=k, padding='same', input_shape = (max_sequence_length, n_features))(fe)\n",
        "    fe = BatchNormalization()(fe)\n",
        "    fe = MaxPooling1D(pool_size=(2))(fe)\n",
        "    #Fully connected layers\n",
        "    fe = Flatten()(fe)\n",
        "    fe = Dense(fmaps)(fe)\n",
        "    out_labels = Dense(n_classes, activation='softmax', name=\"labels_output\")(fe)\n",
        "    model = Model(inputs=[in_ts, in_hint], outputs=out_labels, name=\"Labels_Classifier\")\n",
        "    opt = Adam()\n",
        "    losses = {\"labels_output\":'categorical_crossentropy'}\n",
        "    model.compile(loss=losses, optimizer=opt, metrics = ['categorical_accuracy']) \n",
        "    model.summary() #prints out layers of model\n",
        "    return model\n"
      ],
      "id": "noble-update",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "subsequent-estate"
      },
      "source": [
        "def define_discriminator(ts_shape=(max_sequence_length, n_features), \n",
        "                           hint_shape = (max_sequence_length, n_features)):\n",
        "    # time series input\n",
        "    in_ts = Input(shape=ts_shape)\n",
        "    in_hint = Input(shape= hint_shape)\n",
        "    in_merged = Concatenate(axis=2)([in_ts, in_hint])\n",
        "    filter1 = n_features * 2\n",
        "    fe = Conv1D(filter1, kernel_size=k, padding=\"same\", activation='relu', \n",
        "                input_shape = (max_sequence_length, n_features))(in_merged)\n",
        "    fe = Conv1D(n_features, kernel_size=k, padding=\"same\",\n",
        "                input_shape = (max_sequence_length, n_features))(fe)\n",
        "    # real/fake output\n",
        "    out_realfake = Activation('sigmoid', name=\"realfake_output\")(fe)\n",
        "    model = Model(inputs=[in_ts, in_hint], outputs=out_realfake, name=\"RealFake_Discriminator\")\n",
        "    # compile model\n",
        "    opt = Adam()\n",
        "    losses = {\"realfake_output\":'binary_crossentropy'}\n",
        "    model.compile(loss=losses, optimizer=opt, metrics = ['binary_accuracy']) \n",
        "    model.summary() #prints out layers of model\n",
        "    return model"
      ],
      "id": "subsequent-estate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "returning-arctic"
      },
      "source": [
        "def define_generator_impute(missing_shape = (max_sequence_length, n_features),\n",
        "                            mask_shape = (max_sequence_length, n_features),\n",
        "                            labels_shape = (n_classes, ),\n",
        "                            hint_shape = (max_sequence_length, n_features)):\n",
        "    # time series input\n",
        "    in_ts = Input(shape=missing_shape)\n",
        "    in_hint = Input(shape= hint_shape)\n",
        "\n",
        "    #Other inputs\n",
        "    in_labels = Input(shape = labels_shape)\n",
        "    labels_tile = RepeatVector(max_sequence_length)(in_labels)\n",
        "    in_mask = Input(shape=mask_shape)\n",
        "    in_merged = Concatenate(axis=2)([in_ts, in_hint, labels_tile])\n",
        "    \n",
        "    filter1 = n_features * 2\n",
        "    fe = Conv1D(filter1, kernel_size=k, padding=\"same\", activation='relu', \n",
        "                input_shape = (max_sequence_length, n_features))(in_merged)\n",
        "    fe = Conv1D(n_features, kernel_size=k, padding=\"same\",\n",
        "                input_shape = (max_sequence_length, n_features))(fe)\n",
        "    fake_ts = fe \n",
        "    Masked_Data = Multiply()([in_ts, in_mask])\n",
        "    Reversed_Mask = Lambda(lambda x: 1. - x)(in_mask)\n",
        "    Imputed_Vals = Multiply()([fake_ts, Reversed_Mask])\n",
        "    imputed_ts = Add(name = \"fakets_output\")([Masked_Data, Imputed_Vals]) #Note that we replaced noise with values from our imputed copy\n",
        "    # define model\n",
        "    model = Model(inputs = [in_ts, in_hint, in_mask, in_labels], outputs = [imputed_ts, in_hint], name=\"Generator\") \n",
        "    model.summary()\n",
        "    return model\n",
        "    "
      ],
      "id": "returning-arctic",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alpine-relay"
      },
      "source": [
        "def define_gan(g_model, d_model, c_model):\n",
        "    # make weights in the discriminator AND classifier not trainable\n",
        "    for layer in d_model.layers:\n",
        "        if not isinstance(layer, BatchNormalization):\n",
        "            layer.trainable = False  \n",
        "    discriminator_output = d_model(g_model.output)\n",
        "    classifier_output = c_model(g_model.output)\n",
        "    model = Model(inputs = g_model.input, output = [classifier_output, discriminator_output])\n",
        "    # compile model\n",
        "    opt = Adam()\n",
        "    twolosses = {\"Labels_Classifier\":'binary_crossentropy', \"RealFake_Discriminator\":'binary_crossentropy'}\n",
        "    twolossesweights = {\"Labels_Classifier\": 1, \"RealFake_Discriminator\": 1}\n",
        "    model.compile(loss=twolosses, optimizer=opt, loss_weights = twolossesweights)\n",
        "    model.summary()    \n",
        "    return model\n"
      ],
      "id": "alpine-relay",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "downtown-fantasy"
      },
      "source": [
        "def sample_toImpute(x_train, y_train, p_missing, n_samples=n_samples, \n",
        "                             n_features = n_features, max_sequence_length=max_sequence_length):\n",
        "    #1. Reweighted Sampling\n",
        "    uniq_classes, uniq_counts = np.unique(y_train, return_counts=True)\n",
        "    class_probs = uniq_counts/len(y_train)\n",
        "    inv_probs = (1/class_probs)/(sum(1/class_probs))\n",
        "    sample_counts = np.round(inv_probs * n_samples)\n",
        "    sample_counts[len(sample_counts) - 1] = n_samples - sum(sample_counts[0:(len(sample_counts) - 1)])                           \n",
        "    ix_bal = []\n",
        "    for i in uniq_classes:\n",
        "        index_class = [k for k in range(len(y_train)) if y_train[k] == i]\n",
        "        index_sample = random.choices(index_class, k=int(sample_counts[int(i)]))\n",
        "        ix_bal.append(index_sample)\n",
        "    \n",
        "    ix_bal = list(chain(*ix_bal))\n",
        "    \n",
        "    impute_y_label = y_train[ix_bal]\n",
        "    impute_y_label = tf.keras.utils.to_categorical(impute_y_label, num_classes=n_classes)\n",
        "    \n",
        "    #2. Random Masking under MCAR scheme\n",
        "    #Create mask matrix: M_{ij} = 1 mean data is real\n",
        "    impute_mask = np.random.rand(max_sequence_length * n_features * n_samples) > p_missing\n",
        "    impute_mask = 1*impute_mask.reshape(n_samples, max_sequence_length, n_features)\n",
        "    \n",
        "    #Mask the true data with noise values \n",
        "    impute_noise = np.random.rand(max_sequence_length * n_features * n_samples)\n",
        "    impute_noise = impute_noise.reshape(n_samples, max_sequence_length, n_features)\n",
        "    impute_x_train = x_train[ix_bal]\n",
        "    impute_z_input = impute_x_train * (impute_mask) + impute_noise * (1-impute_mask)\n",
        "    \n",
        "    return impute_z_input, impute_mask, impute_y_label, impute_x_train"
      ],
      "id": "downtown-fantasy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inner-coast"
      },
      "source": [
        "def sample_hint_matrix(mask_matrix, p_hint=0.8):\n",
        "    hints = np.random.rand(mask_matrix.shape[0] * mask_matrix.shape[1] * mask_matrix.shape[2]) > p_missing\n",
        "    hints = 1*hints.reshape(mask_matrix.shape[0], mask_matrix.shape[1], mask_matrix.shape[2])\n",
        "    hint_matrix = hints*mask_matrix\n",
        "    return(hint_matrix)"
      ],
      "id": "inner-coast",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fancy-chess"
      },
      "source": [
        "def shuffle_minibatch(X_both, labels_both, hint_both):\n",
        "    p = np.random.permutation(X_both.shape[0])\n",
        "    X_both = X_both[p]\n",
        "    labels_both = labels_both[p]\n",
        "    hint_both = hint_both[p]\n",
        "    return X_both, labels_both, hint_both"
      ],
      "id": "fancy-chess",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "entitled-third",
        "outputId": "74144d25-c468-420a-ccd6-788049035356"
      },
      "source": [
        "%time\n",
        "def train(g_model, d_model, c_model, gan_model, \n",
        "          x_train, y_train, x_test, n_epochs, p_missing,\n",
        "          n_features = n_features, max_sequence_length = max_sequence_length, n_samples=n_samples):\n",
        "\n",
        "    n_steps = int((x_train.shape[0]/n_samples) * n_epochs)\n",
        "    n_burn = 10\n",
        "    # manually enumerate epochs\n",
        "    print(\"Number of steps total: \", n_steps)\n",
        "    for i in range(n_steps):  \n",
        "        \n",
        "        # Select a random batch of true data\n",
        "        idx = np.random.randint(0, x_train.shape[0], n_samples)\n",
        "        true_samples = x_train[idx]\n",
        "\n",
        "        true_labels = y_train[idx]\n",
        "        true_labels = tf.keras.utils.to_categorical(true_labels, num_classes=n_classes)\n",
        "        \n",
        "        #Select generator input:\n",
        "        z_input, z_mask, z_labels, z_train = sample_toImpute(x_train=x_train, y_train=y_train,\n",
        "                                                             p_missing = p_missing,n_samples=n_samples, \n",
        "                                                             n_features = n_features,\n",
        "                                                             max_sequence_length=max_sequence_length)\n",
        "        #Discriminator ground truths\n",
        "        true_M = np.ones((n_samples, max_sequence_length, n_features))\n",
        "        fake_M = z_mask\n",
        "        true_hint = sample_hint_matrix(true_M, p_hint=0.5)\n",
        "        z_hint = sample_hint_matrix(z_mask, p_hint=0.5)\n",
        "\n",
        "        #Get generator prediction\n",
        "        gen_samples, gen_hint = generator.predict([z_input, z_hint, z_mask, z_labels])\n",
        "        \n",
        "        # Update discriminator\n",
        "        d_loss_real = discriminator.train_on_batch([true_samples, true_hint], true_M)\n",
        "        d_loss_fake = discriminator.train_on_batch([gen_samples, gen_hint], fake_M)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        \n",
        "        # Train the classifier after burning the first n iterations\n",
        "        if i <= n_burn:\n",
        "            c_loss = classifier.train_on_batch([true_samples, true_hint], true_labels)\n",
        "        if i > n_burn:\n",
        "            samples_both = np.concatenate((true_samples, gen_samples), axis=0)\n",
        "            labels_both = np.concatenate((true_labels, z_labels), axis=0)\n",
        "            hint_both = np.concatenate((true_hint, z_hint), axis=0)\n",
        "            samples_both, labels_both, hint_both = shuffle_minibatch(samples_both, labels_both, hint_both)\n",
        "            c_loss = classifier.train_on_batch([samples_both, hint_both], labels_both)\n",
        "\n",
        "        # Train the generator\n",
        "        g_loss = gan_model.train_on_batch([z_input, z_hint, z_mask, z_labels], [z_labels, true_M])\n",
        "        \n",
        "        # State losses\n",
        "        print (\"%d [D loss: %f] [G loss: %f] [C loss: %f]\" % (i, d_loss[0], g_loss[0], c_loss[0]))\n",
        "\n",
        "        if i  == (n_steps -1):\n",
        "            test_M = np.ones((x_test.shape[0], max_sequence_length, n_features))\n",
        "            test_hint = sample_hint_matrix(test_M, p_hint = 0.5)\n",
        "            pred_realfake = d_model.predict([x_test, test_hint])\n",
        "            pred_labels = c_model.predict([x_test, test_hint])\n",
        "    return pred_realfake, pred_labels"
      ],
      "id": "entitled-third",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 19 µs, sys: 24 µs, total: 43 µs\n",
            "Wall time: 75.6 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spectacular-sustainability"
      },
      "source": [
        "#### Train IB-GAN for one iteration"
      ],
      "id": "spectacular-sustainability"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extra-moscow",
        "outputId": "412e647f-99e1-4fcb-f7bc-a572bcb7f61b"
      },
      "source": [
        "%%time\n",
        "tf.keras.backend.clear_session()\n",
        "random.seed(123)\n",
        "#Hyperparameters\n",
        "n_epochs = 100\n",
        "p_missing = 0.1\n",
        "\n",
        "#Loop\n",
        "classifier = define_classifier()\n",
        "discriminator = define_discriminator()\n",
        "generator = define_generator_impute()\n",
        "gan_model = define_gan(g_model = generator, d_model = discriminator, c_model = classifier)\n",
        "pred_realfake, pred_labels = train(g_model = generator, d_model = discriminator, c_model = classifier, \n",
        "                               gan_model=gan_model, p_missing=p_missing,\n",
        "                               x_train = x_train, y_train = y_train,\n",
        "                               x_test = x_test, n_epochs=n_epochs)\n",
        "\n",
        "rounded_labels= np.argmax(pred_labels, axis = 1)\n",
        "\n",
        "pred_metrics = pd.DataFrame({'ClassifierType': \"IB-GAN\",\n",
        "                    'Accuracy': accuracy_score(y_test, rounded_labels), \n",
        "                     'Precision': precision_score(y_test, rounded_labels, average=\"macro\"),\n",
        "                     'Recall': recall_score(y_test, rounded_labels, average=\"macro\"),\n",
        "                     'BalancedAccuracy': balanced_accuracy_score(y_true=y_test, y_pred = rounded_labels),\n",
        "                     'F1-score': f1_score(y_true=y_test, y_pred=rounded_labels, average=\"macro\")\n",
        "                    }, index = [0])\n"
      ],
      "id": "extra-moscow",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Labels_Classifier\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 93, 13)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 93, 26)            702       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 93, 26)            104       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 46, 26)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 46, 1)             53        \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 46, 1)             4         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 23, 1)             0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 23)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                480       \n",
            "_________________________________________________________________\n",
            "labels_output (Dense)        (None, 10)                210       \n",
            "=================================================================\n",
            "Total params: 1,553\n",
            "Trainable params: 1,499\n",
            "Non-trainable params: 54\n",
            "_________________________________________________________________\n",
            "Model: \"RealFake_Discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 93, 13)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 93, 13)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 93, 26)       0           input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 93, 26)       1378        concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 93, 13)       689         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "realfake_output (Activation)    (None, 93, 13)       0           conv1d_4[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,067\n",
            "Trainable params: 2,067\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"Generator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 93, 13)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 93, 13)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 93, 10)       0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 93, 36)       0           input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            (None, 93, 13)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 93, 26)       1898        concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 93, 13)       689         conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 93, 13)       0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 93, 13)       0           input_5[0][0]                    \n",
            "                                                                 input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 93, 13)       0           conv1d_6[0][0]                   \n",
            "                                                                 lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "fakets_output (Add)             (None, 93, 13)       0           multiply_1[0][0]                 \n",
            "                                                                 multiply_2[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 2,587\n",
            "Trainable params: 2,587\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 93, 13)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 93, 13)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 93, 10)       0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 93, 36)       0           input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            (None, 93, 13)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 93, 26)       1898        concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 93, 13)       689         conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 93, 13)       0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 93, 13)       0           input_5[0][0]                    \n",
            "                                                                 input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 93, 13)       0           conv1d_6[0][0]                   \n",
            "                                                                 lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "fakets_output (Add)             (None, 93, 13)       0           multiply_1[0][0]                 \n",
            "                                                                 multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "Labels_Classifier (Model)       (None, 10)           1553        fakets_output[0][0]              \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "RealFake_Discriminator (Model)  (None, 93, 13)       2067        fakets_output[0][0]              \n",
            "                                                                 input_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 6,207\n",
            "Trainable params: 4,086\n",
            "Non-trainable params: 2,121\n",
            "__________________________________________________________________________________________________\n",
            "Number of steps total:  500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.749327] [G loss: 1.129638] [C loss: 2.777773]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 [D loss: 0.720249] [G loss: 1.089097] [C loss: 2.769699]\n",
            "2 [D loss: 0.690112] [G loss: 1.038612] [C loss: 2.671327]\n",
            "3 [D loss: 0.660841] [G loss: 1.003426] [C loss: 2.617783]\n",
            "4 [D loss: 0.631245] [G loss: 0.969167] [C loss: 2.556081]\n",
            "5 [D loss: 0.602717] [G loss: 0.927545] [C loss: 2.584717]\n",
            "6 [D loss: 0.575001] [G loss: 0.885251] [C loss: 2.528491]\n",
            "7 [D loss: 0.547903] [G loss: 0.849937] [C loss: 2.411498]\n",
            "8 [D loss: 0.522560] [G loss: 0.817405] [C loss: 2.410905]\n",
            "9 [D loss: 0.496404] [G loss: 0.779333] [C loss: 2.355428]\n",
            "10 [D loss: 0.472498] [G loss: 0.756427] [C loss: 2.339181]\n",
            "11 [D loss: 0.448339] [G loss: 0.717516] [C loss: 2.331836]\n",
            "12 [D loss: 0.424922] [G loss: 0.679316] [C loss: 2.241491]\n",
            "13 [D loss: 0.402016] [G loss: 0.654098] [C loss: 2.241865]\n",
            "14 [D loss: 0.381279] [G loss: 0.616325] [C loss: 2.218970]\n",
            "15 [D loss: 0.360904] [G loss: 0.576536] [C loss: 2.143676]\n",
            "16 [D loss: 0.341720] [G loss: 0.557010] [C loss: 2.134474]\n",
            "17 [D loss: 0.324679] [G loss: 0.529212] [C loss: 2.112467]\n",
            "18 [D loss: 0.307246] [G loss: 0.505032] [C loss: 2.114999]\n",
            "19 [D loss: 0.292885] [G loss: 0.478755] [C loss: 2.062670]\n",
            "20 [D loss: 0.278579] [G loss: 0.457633] [C loss: 2.037954]\n",
            "21 [D loss: 0.267828] [G loss: 0.434000] [C loss: 1.999915]\n",
            "22 [D loss: 0.257481] [G loss: 0.424494] [C loss: 2.009230]\n",
            "23 [D loss: 0.249096] [G loss: 0.403486] [C loss: 2.011390]\n",
            "24 [D loss: 0.242336] [G loss: 0.381468] [C loss: 1.947167]\n",
            "25 [D loss: 0.235433] [G loss: 0.368143] [C loss: 1.944830]\n",
            "26 [D loss: 0.231108] [G loss: 0.363101] [C loss: 1.935854]\n",
            "27 [D loss: 0.225715] [G loss: 0.345234] [C loss: 1.887245]\n",
            "28 [D loss: 0.222508] [G loss: 0.338456] [C loss: 1.903852]\n",
            "29 [D loss: 0.221114] [G loss: 0.331138] [C loss: 1.868458]\n",
            "30 [D loss: 0.217754] [G loss: 0.322834] [C loss: 1.868093]\n",
            "31 [D loss: 0.216377] [G loss: 0.316819] [C loss: 1.859424]\n",
            "32 [D loss: 0.213773] [G loss: 0.318219] [C loss: 1.863580]\n",
            "33 [D loss: 0.212682] [G loss: 0.305853] [C loss: 1.820045]\n",
            "34 [D loss: 0.210636] [G loss: 0.304717] [C loss: 1.814980]\n",
            "35 [D loss: 0.209587] [G loss: 0.297806] [C loss: 1.776172]\n",
            "36 [D loss: 0.208574] [G loss: 0.300267] [C loss: 1.815082]\n",
            "37 [D loss: 0.207216] [G loss: 0.289736] [C loss: 1.730042]\n",
            "38 [D loss: 0.206983] [G loss: 0.287916] [C loss: 1.739667]\n",
            "39 [D loss: 0.206518] [G loss: 0.293822] [C loss: 1.736337]\n",
            "40 [D loss: 0.204750] [G loss: 0.283863] [C loss: 1.723723]\n",
            "41 [D loss: 0.203501] [G loss: 0.285734] [C loss: 1.705986]\n",
            "42 [D loss: 0.203489] [G loss: 0.285154] [C loss: 1.695617]\n",
            "43 [D loss: 0.202521] [G loss: 0.282103] [C loss: 1.680332]\n",
            "44 [D loss: 0.202112] [G loss: 0.277943] [C loss: 1.658547]\n",
            "45 [D loss: 0.201333] [G loss: 0.282180] [C loss: 1.649156]\n",
            "46 [D loss: 0.200757] [G loss: 0.276057] [C loss: 1.653194]\n",
            "47 [D loss: 0.199514] [G loss: 0.276913] [C loss: 1.613546]\n",
            "48 [D loss: 0.200910] [G loss: 0.275775] [C loss: 1.630388]\n",
            "49 [D loss: 0.200018] [G loss: 0.275978] [C loss: 1.603397]\n",
            "50 [D loss: 0.198472] [G loss: 0.270424] [C loss: 1.611222]\n",
            "51 [D loss: 0.198388] [G loss: 0.270316] [C loss: 1.556144]\n",
            "52 [D loss: 0.198766] [G loss: 0.268963] [C loss: 1.551993]\n",
            "53 [D loss: 0.197588] [G loss: 0.268475] [C loss: 1.544537]\n",
            "54 [D loss: 0.197139] [G loss: 0.258229] [C loss: 1.509210]\n",
            "55 [D loss: 0.197361] [G loss: 0.261479] [C loss: 1.516686]\n",
            "56 [D loss: 0.196439] [G loss: 0.260000] [C loss: 1.504520]\n",
            "57 [D loss: 0.195577] [G loss: 0.258256] [C loss: 1.493366]\n",
            "58 [D loss: 0.195555] [G loss: 0.256613] [C loss: 1.492111]\n",
            "59 [D loss: 0.195045] [G loss: 0.254774] [C loss: 1.474097]\n",
            "60 [D loss: 0.194692] [G loss: 0.256616] [C loss: 1.478250]\n",
            "61 [D loss: 0.194618] [G loss: 0.258478] [C loss: 1.466148]\n",
            "62 [D loss: 0.194118] [G loss: 0.253488] [C loss: 1.448416]\n",
            "63 [D loss: 0.193687] [G loss: 0.244559] [C loss: 1.404549]\n",
            "64 [D loss: 0.193370] [G loss: 0.249792] [C loss: 1.414852]\n",
            "65 [D loss: 0.193701] [G loss: 0.251338] [C loss: 1.417920]\n",
            "66 [D loss: 0.192501] [G loss: 0.253564] [C loss: 1.433859]\n",
            "67 [D loss: 0.191889] [G loss: 0.240371] [C loss: 1.374201]\n",
            "68 [D loss: 0.191507] [G loss: 0.239940] [C loss: 1.369113]\n",
            "69 [D loss: 0.190796] [G loss: 0.239310] [C loss: 1.329606]\n",
            "70 [D loss: 0.190741] [G loss: 0.236677] [C loss: 1.346017]\n",
            "71 [D loss: 0.190515] [G loss: 0.243922] [C loss: 1.362133]\n",
            "72 [D loss: 0.190956] [G loss: 0.235142] [C loss: 1.294998]\n",
            "73 [D loss: 0.190042] [G loss: 0.235809] [C loss: 1.311256]\n",
            "74 [D loss: 0.190668] [G loss: 0.236417] [C loss: 1.319718]\n",
            "75 [D loss: 0.189437] [G loss: 0.227448] [C loss: 1.275292]\n",
            "76 [D loss: 0.189201] [G loss: 0.235158] [C loss: 1.300816]\n",
            "77 [D loss: 0.188780] [G loss: 0.224383] [C loss: 1.261249]\n",
            "78 [D loss: 0.189230] [G loss: 0.228397] [C loss: 1.238369]\n",
            "79 [D loss: 0.188568] [G loss: 0.223384] [C loss: 1.277677]\n",
            "80 [D loss: 0.188610] [G loss: 0.223068] [C loss: 1.224334]\n",
            "81 [D loss: 0.188099] [G loss: 0.224878] [C loss: 1.224978]\n",
            "82 [D loss: 0.188005] [G loss: 0.219105] [C loss: 1.201961]\n",
            "83 [D loss: 0.187015] [G loss: 0.221905] [C loss: 1.207389]\n",
            "84 [D loss: 0.187879] [G loss: 0.216867] [C loss: 1.193099]\n",
            "85 [D loss: 0.187639] [G loss: 0.211514] [C loss: 1.199582]\n",
            "86 [D loss: 0.188307] [G loss: 0.213598] [C loss: 1.184957]\n",
            "87 [D loss: 0.187375] [G loss: 0.215322] [C loss: 1.184178]\n",
            "88 [D loss: 0.187803] [G loss: 0.205990] [C loss: 1.149968]\n",
            "89 [D loss: 0.187782] [G loss: 0.205912] [C loss: 1.145260]\n",
            "90 [D loss: 0.188160] [G loss: 0.205480] [C loss: 1.107517]\n",
            "91 [D loss: 0.188179] [G loss: 0.207477] [C loss: 1.148033]\n",
            "92 [D loss: 0.188207] [G loss: 0.204754] [C loss: 1.103195]\n",
            "93 [D loss: 0.187460] [G loss: 0.201503] [C loss: 1.112269]\n",
            "94 [D loss: 0.187418] [G loss: 0.201977] [C loss: 1.136270]\n",
            "95 [D loss: 0.187247] [G loss: 0.198843] [C loss: 1.085439]\n",
            "96 [D loss: 0.187584] [G loss: 0.196530] [C loss: 1.089748]\n",
            "97 [D loss: 0.187942] [G loss: 0.191932] [C loss: 1.057891]\n",
            "98 [D loss: 0.188555] [G loss: 0.189566] [C loss: 1.048963]\n",
            "99 [D loss: 0.187706] [G loss: 0.196668] [C loss: 1.101174]\n",
            "100 [D loss: 0.187978] [G loss: 0.189792] [C loss: 1.053389]\n",
            "101 [D loss: 0.187376] [G loss: 0.192675] [C loss: 1.059909]\n",
            "102 [D loss: 0.187213] [G loss: 0.182419] [C loss: 1.043453]\n",
            "103 [D loss: 0.187534] [G loss: 0.184844] [C loss: 1.047774]\n",
            "104 [D loss: 0.188554] [G loss: 0.182961] [C loss: 1.007977]\n",
            "105 [D loss: 0.188434] [G loss: 0.178532] [C loss: 1.011802]\n",
            "106 [D loss: 0.188236] [G loss: 0.174179] [C loss: 0.994199]\n",
            "107 [D loss: 0.188896] [G loss: 0.167845] [C loss: 0.998164]\n",
            "108 [D loss: 0.188262] [G loss: 0.174879] [C loss: 1.006457]\n",
            "109 [D loss: 0.187663] [G loss: 0.169672] [C loss: 0.986542]\n",
            "110 [D loss: 0.188665] [G loss: 0.167697] [C loss: 0.943795]\n",
            "111 [D loss: 0.188472] [G loss: 0.168516] [C loss: 0.978926]\n",
            "112 [D loss: 0.189972] [G loss: 0.166114] [C loss: 0.974665]\n",
            "113 [D loss: 0.188701] [G loss: 0.166084] [C loss: 0.988418]\n",
            "114 [D loss: 0.188258] [G loss: 0.163376] [C loss: 0.972594]\n",
            "115 [D loss: 0.189142] [G loss: 0.160573] [C loss: 0.928403]\n",
            "116 [D loss: 0.190171] [G loss: 0.160569] [C loss: 0.921766]\n",
            "117 [D loss: 0.189218] [G loss: 0.159124] [C loss: 0.910404]\n",
            "118 [D loss: 0.189239] [G loss: 0.164907] [C loss: 0.930686]\n",
            "119 [D loss: 0.189460] [G loss: 0.157567] [C loss: 0.902844]\n",
            "120 [D loss: 0.190352] [G loss: 0.155778] [C loss: 0.887605]\n",
            "121 [D loss: 0.189775] [G loss: 0.157427] [C loss: 0.894252]\n",
            "122 [D loss: 0.189581] [G loss: 0.155331] [C loss: 0.904550]\n",
            "123 [D loss: 0.190022] [G loss: 0.154392] [C loss: 0.876235]\n",
            "124 [D loss: 0.190415] [G loss: 0.155928] [C loss: 0.898498]\n",
            "125 [D loss: 0.189801] [G loss: 0.147810] [C loss: 0.870938]\n",
            "126 [D loss: 0.189706] [G loss: 0.155093] [C loss: 0.879436]\n",
            "127 [D loss: 0.190147] [G loss: 0.151423] [C loss: 0.863437]\n",
            "128 [D loss: 0.190410] [G loss: 0.155851] [C loss: 0.874880]\n",
            "129 [D loss: 0.189568] [G loss: 0.144700] [C loss: 0.833816]\n",
            "130 [D loss: 0.189850] [G loss: 0.150813] [C loss: 0.854269]\n",
            "131 [D loss: 0.189825] [G loss: 0.146867] [C loss: 0.837469]\n",
            "132 [D loss: 0.190471] [G loss: 0.145119] [C loss: 0.849404]\n",
            "133 [D loss: 0.189159] [G loss: 0.144788] [C loss: 0.839446]\n",
            "134 [D loss: 0.190292] [G loss: 0.151032] [C loss: 0.867841]\n",
            "135 [D loss: 0.189693] [G loss: 0.148917] [C loss: 0.846240]\n",
            "136 [D loss: 0.189940] [G loss: 0.146359] [C loss: 0.840395]\n",
            "137 [D loss: 0.189956] [G loss: 0.140663] [C loss: 0.818056]\n",
            "138 [D loss: 0.191043] [G loss: 0.140104] [C loss: 0.829961]\n",
            "139 [D loss: 0.190471] [G loss: 0.141022] [C loss: 0.800281]\n",
            "140 [D loss: 0.190068] [G loss: 0.142010] [C loss: 0.819918]\n",
            "141 [D loss: 0.190432] [G loss: 0.138582] [C loss: 0.837952]\n",
            "142 [D loss: 0.190341] [G loss: 0.144595] [C loss: 0.818272]\n",
            "143 [D loss: 0.190178] [G loss: 0.142641] [C loss: 0.826052]\n",
            "144 [D loss: 0.189712] [G loss: 0.140976] [C loss: 0.822500]\n",
            "145 [D loss: 0.190864] [G loss: 0.138680] [C loss: 0.785453]\n",
            "146 [D loss: 0.189815] [G loss: 0.141570] [C loss: 0.818171]\n",
            "147 [D loss: 0.189105] [G loss: 0.137547] [C loss: 0.791673]\n",
            "148 [D loss: 0.190535] [G loss: 0.135684] [C loss: 0.782488]\n",
            "149 [D loss: 0.189468] [G loss: 0.131019] [C loss: 0.768561]\n",
            "150 [D loss: 0.189700] [G loss: 0.135064] [C loss: 0.766658]\n",
            "151 [D loss: 0.189303] [G loss: 0.137994] [C loss: 0.822597]\n",
            "152 [D loss: 0.189696] [G loss: 0.135511] [C loss: 0.765854]\n",
            "153 [D loss: 0.190166] [G loss: 0.130555] [C loss: 0.775559]\n",
            "154 [D loss: 0.189904] [G loss: 0.132436] [C loss: 0.767445]\n",
            "155 [D loss: 0.189984] [G loss: 0.132304] [C loss: 0.755319]\n",
            "156 [D loss: 0.189745] [G loss: 0.131268] [C loss: 0.736877]\n",
            "157 [D loss: 0.189578] [G loss: 0.133568] [C loss: 0.749112]\n",
            "158 [D loss: 0.189517] [G loss: 0.129510] [C loss: 0.725624]\n",
            "159 [D loss: 0.188901] [G loss: 0.129964] [C loss: 0.742927]\n",
            "160 [D loss: 0.188870] [G loss: 0.132104] [C loss: 0.754242]\n",
            "161 [D loss: 0.189636] [G loss: 0.131365] [C loss: 0.763597]\n",
            "162 [D loss: 0.189704] [G loss: 0.128541] [C loss: 0.730011]\n",
            "163 [D loss: 0.189065] [G loss: 0.130451] [C loss: 0.741403]\n",
            "164 [D loss: 0.189699] [G loss: 0.128242] [C loss: 0.723165]\n",
            "165 [D loss: 0.190341] [G loss: 0.126899] [C loss: 0.716735]\n",
            "166 [D loss: 0.189821] [G loss: 0.133053] [C loss: 0.748354]\n",
            "167 [D loss: 0.189345] [G loss: 0.129830] [C loss: 0.747211]\n",
            "168 [D loss: 0.188740] [G loss: 0.121490] [C loss: 0.690312]\n",
            "169 [D loss: 0.189344] [G loss: 0.129835] [C loss: 0.723116]\n",
            "170 [D loss: 0.189596] [G loss: 0.127435] [C loss: 0.684601]\n",
            "171 [D loss: 0.189645] [G loss: 0.123050] [C loss: 0.672275]\n",
            "172 [D loss: 0.188526] [G loss: 0.119352] [C loss: 0.689862]\n",
            "173 [D loss: 0.189653] [G loss: 0.127672] [C loss: 0.705590]\n",
            "174 [D loss: 0.189063] [G loss: 0.123721] [C loss: 0.692223]\n",
            "175 [D loss: 0.190709] [G loss: 0.120269] [C loss: 0.687451]\n",
            "176 [D loss: 0.190386] [G loss: 0.119618] [C loss: 0.640057]\n",
            "177 [D loss: 0.190192] [G loss: 0.120406] [C loss: 0.665483]\n",
            "178 [D loss: 0.189837] [G loss: 0.115319] [C loss: 0.676449]\n",
            "179 [D loss: 0.189818] [G loss: 0.120840] [C loss: 0.663007]\n",
            "180 [D loss: 0.190409] [G loss: 0.119929] [C loss: 0.697066]\n",
            "181 [D loss: 0.189957] [G loss: 0.119767] [C loss: 0.691621]\n",
            "182 [D loss: 0.189276] [G loss: 0.118833] [C loss: 0.665510]\n",
            "183 [D loss: 0.189384] [G loss: 0.119700] [C loss: 0.648255]\n",
            "184 [D loss: 0.189206] [G loss: 0.119288] [C loss: 0.641305]\n",
            "185 [D loss: 0.189288] [G loss: 0.113077] [C loss: 0.611664]\n",
            "186 [D loss: 0.189837] [G loss: 0.113337] [C loss: 0.625377]\n",
            "187 [D loss: 0.189703] [G loss: 0.118084] [C loss: 0.626410]\n",
            "188 [D loss: 0.189547] [G loss: 0.116688] [C loss: 0.669524]\n",
            "189 [D loss: 0.188950] [G loss: 0.118755] [C loss: 0.656049]\n",
            "190 [D loss: 0.189128] [G loss: 0.112012] [C loss: 0.636111]\n",
            "191 [D loss: 0.188941] [G loss: 0.114251] [C loss: 0.627707]\n",
            "192 [D loss: 0.188706] [G loss: 0.121042] [C loss: 0.635100]\n",
            "193 [D loss: 0.189257] [G loss: 0.110984] [C loss: 0.636592]\n",
            "194 [D loss: 0.190004] [G loss: 0.111471] [C loss: 0.657880]\n",
            "195 [D loss: 0.189541] [G loss: 0.112370] [C loss: 0.610472]\n",
            "196 [D loss: 0.188796] [G loss: 0.113321] [C loss: 0.616306]\n",
            "197 [D loss: 0.189327] [G loss: 0.111545] [C loss: 0.610455]\n",
            "198 [D loss: 0.189344] [G loss: 0.115240] [C loss: 0.656751]\n",
            "199 [D loss: 0.188878] [G loss: 0.107125] [C loss: 0.582739]\n",
            "200 [D loss: 0.188079] [G loss: 0.107367] [C loss: 0.582306]\n",
            "201 [D loss: 0.189311] [G loss: 0.105574] [C loss: 0.609886]\n",
            "202 [D loss: 0.189012] [G loss: 0.107205] [C loss: 0.568305]\n",
            "203 [D loss: 0.188852] [G loss: 0.108296] [C loss: 0.590383]\n",
            "204 [D loss: 0.188542] [G loss: 0.104024] [C loss: 0.583732]\n",
            "205 [D loss: 0.188365] [G loss: 0.105453] [C loss: 0.577678]\n",
            "206 [D loss: 0.188416] [G loss: 0.106927] [C loss: 0.563071]\n",
            "207 [D loss: 0.188095] [G loss: 0.106550] [C loss: 0.601272]\n",
            "208 [D loss: 0.187582] [G loss: 0.103316] [C loss: 0.581120]\n",
            "209 [D loss: 0.188760] [G loss: 0.107740] [C loss: 0.583579]\n",
            "210 [D loss: 0.188367] [G loss: 0.106607] [C loss: 0.568322]\n",
            "211 [D loss: 0.186571] [G loss: 0.102558] [C loss: 0.604537]\n",
            "212 [D loss: 0.187530] [G loss: 0.104645] [C loss: 0.547086]\n",
            "213 [D loss: 0.188284] [G loss: 0.109353] [C loss: 0.592118]\n",
            "214 [D loss: 0.187395] [G loss: 0.105336] [C loss: 0.594651]\n",
            "215 [D loss: 0.187789] [G loss: 0.099711] [C loss: 0.555195]\n",
            "216 [D loss: 0.187619] [G loss: 0.106359] [C loss: 0.593313]\n",
            "217 [D loss: 0.186817] [G loss: 0.099390] [C loss: 0.518677]\n",
            "218 [D loss: 0.187266] [G loss: 0.102119] [C loss: 0.577565]\n",
            "219 [D loss: 0.187388] [G loss: 0.096310] [C loss: 0.556707]\n",
            "220 [D loss: 0.187625] [G loss: 0.101591] [C loss: 0.552850]\n",
            "221 [D loss: 0.188112] [G loss: 0.096840] [C loss: 0.540643]\n",
            "222 [D loss: 0.187388] [G loss: 0.098283] [C loss: 0.558628]\n",
            "223 [D loss: 0.186680] [G loss: 0.097248] [C loss: 0.527946]\n",
            "224 [D loss: 0.186904] [G loss: 0.095698] [C loss: 0.528011]\n",
            "225 [D loss: 0.186763] [G loss: 0.096374] [C loss: 0.536803]\n",
            "226 [D loss: 0.186380] [G loss: 0.095679] [C loss: 0.505188]\n",
            "227 [D loss: 0.186713] [G loss: 0.098365] [C loss: 0.585701]\n",
            "228 [D loss: 0.186740] [G loss: 0.099598] [C loss: 0.536114]\n",
            "229 [D loss: 0.186189] [G loss: 0.100966] [C loss: 0.527466]\n",
            "230 [D loss: 0.186123] [G loss: 0.099936] [C loss: 0.534005]\n",
            "231 [D loss: 0.186546] [G loss: 0.097942] [C loss: 0.540361]\n",
            "232 [D loss: 0.186183] [G loss: 0.095407] [C loss: 0.513742]\n",
            "233 [D loss: 0.186040] [G loss: 0.096570] [C loss: 0.535058]\n",
            "234 [D loss: 0.185501] [G loss: 0.097344] [C loss: 0.513272]\n",
            "235 [D loss: 0.185275] [G loss: 0.096667] [C loss: 0.504335]\n",
            "236 [D loss: 0.185490] [G loss: 0.102605] [C loss: 0.573577]\n",
            "237 [D loss: 0.184702] [G loss: 0.093545] [C loss: 0.530693]\n",
            "238 [D loss: 0.185706] [G loss: 0.097134] [C loss: 0.511946]\n",
            "239 [D loss: 0.184783] [G loss: 0.088538] [C loss: 0.479599]\n",
            "240 [D loss: 0.185036] [G loss: 0.089988] [C loss: 0.522652]\n",
            "241 [D loss: 0.184727] [G loss: 0.088807] [C loss: 0.495544]\n",
            "242 [D loss: 0.185017] [G loss: 0.093046] [C loss: 0.489541]\n",
            "243 [D loss: 0.185459] [G loss: 0.098259] [C loss: 0.539317]\n",
            "244 [D loss: 0.184437] [G loss: 0.091243] [C loss: 0.498046]\n",
            "245 [D loss: 0.184326] [G loss: 0.096218] [C loss: 0.516845]\n",
            "246 [D loss: 0.184710] [G loss: 0.095434] [C loss: 0.479978]\n",
            "247 [D loss: 0.184932] [G loss: 0.094334] [C loss: 0.552445]\n",
            "248 [D loss: 0.184178] [G loss: 0.090753] [C loss: 0.516333]\n",
            "249 [D loss: 0.184386] [G loss: 0.092406] [C loss: 0.482722]\n",
            "250 [D loss: 0.184211] [G loss: 0.091728] [C loss: 0.484554]\n",
            "251 [D loss: 0.184139] [G loss: 0.092998] [C loss: 0.478623]\n",
            "252 [D loss: 0.183693] [G loss: 0.093197] [C loss: 0.481894]\n",
            "253 [D loss: 0.183430] [G loss: 0.089454] [C loss: 0.493684]\n",
            "254 [D loss: 0.183379] [G loss: 0.095805] [C loss: 0.492080]\n",
            "255 [D loss: 0.182683] [G loss: 0.093291] [C loss: 0.489603]\n",
            "256 [D loss: 0.183974] [G loss: 0.089686] [C loss: 0.530910]\n",
            "257 [D loss: 0.183451] [G loss: 0.092994] [C loss: 0.498855]\n",
            "258 [D loss: 0.183245] [G loss: 0.090754] [C loss: 0.485031]\n",
            "259 [D loss: 0.182635] [G loss: 0.091330] [C loss: 0.489378]\n",
            "260 [D loss: 0.183072] [G loss: 0.089560] [C loss: 0.493287]\n",
            "261 [D loss: 0.182754] [G loss: 0.089003] [C loss: 0.523040]\n",
            "262 [D loss: 0.183120] [G loss: 0.093233] [C loss: 0.503576]\n",
            "263 [D loss: 0.183024] [G loss: 0.084514] [C loss: 0.448282]\n",
            "264 [D loss: 0.182630] [G loss: 0.092355] [C loss: 0.473696]\n",
            "265 [D loss: 0.182317] [G loss: 0.090031] [C loss: 0.456275]\n",
            "266 [D loss: 0.182628] [G loss: 0.091435] [C loss: 0.496624]\n",
            "267 [D loss: 0.182446] [G loss: 0.084710] [C loss: 0.499839]\n",
            "268 [D loss: 0.182528] [G loss: 0.086846] [C loss: 0.481495]\n",
            "269 [D loss: 0.182473] [G loss: 0.087000] [C loss: 0.474916]\n",
            "270 [D loss: 0.181741] [G loss: 0.083227] [C loss: 0.450936]\n",
            "271 [D loss: 0.181635] [G loss: 0.088260] [C loss: 0.471143]\n",
            "272 [D loss: 0.182309] [G loss: 0.088116] [C loss: 0.480334]\n",
            "273 [D loss: 0.181681] [G loss: 0.085003] [C loss: 0.465501]\n",
            "274 [D loss: 0.181422] [G loss: 0.084521] [C loss: 0.449328]\n",
            "275 [D loss: 0.181523] [G loss: 0.087709] [C loss: 0.475901]\n",
            "276 [D loss: 0.182551] [G loss: 0.092162] [C loss: 0.451089]\n",
            "277 [D loss: 0.181632] [G loss: 0.084639] [C loss: 0.392874]\n",
            "278 [D loss: 0.181509] [G loss: 0.093642] [C loss: 0.530939]\n",
            "279 [D loss: 0.181078] [G loss: 0.085689] [C loss: 0.451596]\n",
            "280 [D loss: 0.181250] [G loss: 0.085191] [C loss: 0.446519]\n",
            "281 [D loss: 0.182251] [G loss: 0.085529] [C loss: 0.432249]\n",
            "282 [D loss: 0.181450] [G loss: 0.085058] [C loss: 0.476050]\n",
            "283 [D loss: 0.182544] [G loss: 0.085552] [C loss: 0.459547]\n",
            "284 [D loss: 0.181222] [G loss: 0.082055] [C loss: 0.454415]\n",
            "285 [D loss: 0.181244] [G loss: 0.086740] [C loss: 0.475545]\n",
            "286 [D loss: 0.181778] [G loss: 0.085168] [C loss: 0.455420]\n",
            "287 [D loss: 0.181173] [G loss: 0.091824] [C loss: 0.506262]\n",
            "288 [D loss: 0.182003] [G loss: 0.086263] [C loss: 0.457206]\n",
            "289 [D loss: 0.181250] [G loss: 0.085314] [C loss: 0.452211]\n",
            "290 [D loss: 0.181548] [G loss: 0.086399] [C loss: 0.473715]\n",
            "291 [D loss: 0.181048] [G loss: 0.085779] [C loss: 0.420115]\n",
            "292 [D loss: 0.180547] [G loss: 0.083827] [C loss: 0.463928]\n",
            "293 [D loss: 0.180912] [G loss: 0.082134] [C loss: 0.449334]\n",
            "294 [D loss: 0.180508] [G loss: 0.084000] [C loss: 0.428660]\n",
            "295 [D loss: 0.181065] [G loss: 0.082324] [C loss: 0.437317]\n",
            "296 [D loss: 0.180611] [G loss: 0.084180] [C loss: 0.456202]\n",
            "297 [D loss: 0.180458] [G loss: 0.086400] [C loss: 0.462535]\n",
            "298 [D loss: 0.180839] [G loss: 0.080897] [C loss: 0.421683]\n",
            "299 [D loss: 0.180504] [G loss: 0.084072] [C loss: 0.423636]\n",
            "300 [D loss: 0.179406] [G loss: 0.086189] [C loss: 0.469129]\n",
            "301 [D loss: 0.180817] [G loss: 0.083670] [C loss: 0.436794]\n",
            "302 [D loss: 0.180045] [G loss: 0.083927] [C loss: 0.432077]\n",
            "303 [D loss: 0.179473] [G loss: 0.082180] [C loss: 0.405267]\n",
            "304 [D loss: 0.179275] [G loss: 0.081082] [C loss: 0.419867]\n",
            "305 [D loss: 0.179689] [G loss: 0.084440] [C loss: 0.426828]\n",
            "306 [D loss: 0.179304] [G loss: 0.086977] [C loss: 0.447237]\n",
            "307 [D loss: 0.178968] [G loss: 0.080972] [C loss: 0.423559]\n",
            "308 [D loss: 0.179976] [G loss: 0.080928] [C loss: 0.425762]\n",
            "309 [D loss: 0.178772] [G loss: 0.079228] [C loss: 0.401176]\n",
            "310 [D loss: 0.178735] [G loss: 0.085682] [C loss: 0.418884]\n",
            "311 [D loss: 0.179138] [G loss: 0.085644] [C loss: 0.439179]\n",
            "312 [D loss: 0.178722] [G loss: 0.087590] [C loss: 0.418476]\n",
            "313 [D loss: 0.178277] [G loss: 0.084100] [C loss: 0.421863]\n",
            "314 [D loss: 0.178225] [G loss: 0.079459] [C loss: 0.394315]\n",
            "315 [D loss: 0.178937] [G loss: 0.080800] [C loss: 0.378678]\n",
            "316 [D loss: 0.177995] [G loss: 0.081933] [C loss: 0.404567]\n",
            "317 [D loss: 0.177997] [G loss: 0.082942] [C loss: 0.443657]\n",
            "318 [D loss: 0.177496] [G loss: 0.083458] [C loss: 0.393309]\n",
            "319 [D loss: 0.178154] [G loss: 0.080801] [C loss: 0.419041]\n",
            "320 [D loss: 0.177368] [G loss: 0.079560] [C loss: 0.432365]\n",
            "321 [D loss: 0.177778] [G loss: 0.081670] [C loss: 0.407023]\n",
            "322 [D loss: 0.177308] [G loss: 0.079959] [C loss: 0.394223]\n",
            "323 [D loss: 0.177287] [G loss: 0.081252] [C loss: 0.427974]\n",
            "324 [D loss: 0.177533] [G loss: 0.079819] [C loss: 0.416767]\n",
            "325 [D loss: 0.177289] [G loss: 0.079459] [C loss: 0.421669]\n",
            "326 [D loss: 0.177490] [G loss: 0.079425] [C loss: 0.428611]\n",
            "327 [D loss: 0.176860] [G loss: 0.079349] [C loss: 0.389616]\n",
            "328 [D loss: 0.177609] [G loss: 0.079029] [C loss: 0.401853]\n",
            "329 [D loss: 0.177228] [G loss: 0.079474] [C loss: 0.395999]\n",
            "330 [D loss: 0.178004] [G loss: 0.076857] [C loss: 0.377094]\n",
            "331 [D loss: 0.176765] [G loss: 0.082396] [C loss: 0.431629]\n",
            "332 [D loss: 0.176739] [G loss: 0.083438] [C loss: 0.402297]\n",
            "333 [D loss: 0.177153] [G loss: 0.080903] [C loss: 0.396817]\n",
            "334 [D loss: 0.176232] [G loss: 0.078608] [C loss: 0.402440]\n",
            "335 [D loss: 0.176233] [G loss: 0.079873] [C loss: 0.404969]\n",
            "336 [D loss: 0.176404] [G loss: 0.077496] [C loss: 0.399836]\n",
            "337 [D loss: 0.176373] [G loss: 0.078658] [C loss: 0.402053]\n",
            "338 [D loss: 0.176350] [G loss: 0.078749] [C loss: 0.419956]\n",
            "339 [D loss: 0.175987] [G loss: 0.076396] [C loss: 0.386325]\n",
            "340 [D loss: 0.175481] [G loss: 0.079446] [C loss: 0.376162]\n",
            "341 [D loss: 0.176074] [G loss: 0.078903] [C loss: 0.420268]\n",
            "342 [D loss: 0.175681] [G loss: 0.079908] [C loss: 0.412884]\n",
            "343 [D loss: 0.175965] [G loss: 0.079168] [C loss: 0.388616]\n",
            "344 [D loss: 0.175482] [G loss: 0.075121] [C loss: 0.390178]\n",
            "345 [D loss: 0.175467] [G loss: 0.077658] [C loss: 0.410121]\n",
            "346 [D loss: 0.174420] [G loss: 0.078016] [C loss: 0.439537]\n",
            "347 [D loss: 0.175698] [G loss: 0.079772] [C loss: 0.393921]\n",
            "348 [D loss: 0.175259] [G loss: 0.077276] [C loss: 0.373591]\n",
            "349 [D loss: 0.175241] [G loss: 0.079218] [C loss: 0.406540]\n",
            "350 [D loss: 0.175170] [G loss: 0.078408] [C loss: 0.385699]\n",
            "351 [D loss: 0.175207] [G loss: 0.077355] [C loss: 0.384381]\n",
            "352 [D loss: 0.174561] [G loss: 0.081080] [C loss: 0.367443]\n",
            "353 [D loss: 0.174472] [G loss: 0.078798] [C loss: 0.386548]\n",
            "354 [D loss: 0.174371] [G loss: 0.073977] [C loss: 0.357993]\n",
            "355 [D loss: 0.174496] [G loss: 0.077523] [C loss: 0.372205]\n",
            "356 [D loss: 0.174125] [G loss: 0.075511] [C loss: 0.375037]\n",
            "357 [D loss: 0.174502] [G loss: 0.077203] [C loss: 0.413750]\n",
            "358 [D loss: 0.174584] [G loss: 0.075804] [C loss: 0.385672]\n",
            "359 [D loss: 0.174213] [G loss: 0.076364] [C loss: 0.415080]\n",
            "360 [D loss: 0.173890] [G loss: 0.077949] [C loss: 0.392121]\n",
            "361 [D loss: 0.174442] [G loss: 0.078341] [C loss: 0.379327]\n",
            "362 [D loss: 0.174348] [G loss: 0.074211] [C loss: 0.355551]\n",
            "363 [D loss: 0.173769] [G loss: 0.075701] [C loss: 0.381827]\n",
            "364 [D loss: 0.173682] [G loss: 0.078431] [C loss: 0.369195]\n",
            "365 [D loss: 0.174534] [G loss: 0.080744] [C loss: 0.425177]\n",
            "366 [D loss: 0.173852] [G loss: 0.074631] [C loss: 0.355212]\n",
            "367 [D loss: 0.173680] [G loss: 0.074556] [C loss: 0.391744]\n",
            "368 [D loss: 0.174843] [G loss: 0.073973] [C loss: 0.353270]\n",
            "369 [D loss: 0.174856] [G loss: 0.078563] [C loss: 0.398414]\n",
            "370 [D loss: 0.175097] [G loss: 0.073814] [C loss: 0.336489]\n",
            "371 [D loss: 0.174137] [G loss: 0.074920] [C loss: 0.355131]\n",
            "372 [D loss: 0.174489] [G loss: 0.076643] [C loss: 0.384958]\n",
            "373 [D loss: 0.174522] [G loss: 0.076764] [C loss: 0.354151]\n",
            "374 [D loss: 0.174603] [G loss: 0.075559] [C loss: 0.394111]\n",
            "375 [D loss: 0.174920] [G loss: 0.075809] [C loss: 0.381950]\n",
            "376 [D loss: 0.174534] [G loss: 0.078459] [C loss: 0.381465]\n",
            "377 [D loss: 0.174085] [G loss: 0.073786] [C loss: 0.344230]\n",
            "378 [D loss: 0.173826] [G loss: 0.073713] [C loss: 0.314063]\n",
            "379 [D loss: 0.174123] [G loss: 0.077014] [C loss: 0.335590]\n",
            "380 [D loss: 0.174315] [G loss: 0.071961] [C loss: 0.360266]\n",
            "381 [D loss: 0.173392] [G loss: 0.074566] [C loss: 0.355155]\n",
            "382 [D loss: 0.174449] [G loss: 0.074364] [C loss: 0.365765]\n",
            "383 [D loss: 0.174080] [G loss: 0.074051] [C loss: 0.416198]\n",
            "384 [D loss: 0.175101] [G loss: 0.075136] [C loss: 0.383475]\n",
            "385 [D loss: 0.174870] [G loss: 0.076725] [C loss: 0.346693]\n",
            "386 [D loss: 0.175050] [G loss: 0.073697] [C loss: 0.346929]\n",
            "387 [D loss: 0.174197] [G loss: 0.072318] [C loss: 0.316158]\n",
            "388 [D loss: 0.174349] [G loss: 0.070788] [C loss: 0.385362]\n",
            "389 [D loss: 0.174414] [G loss: 0.073700] [C loss: 0.344597]\n",
            "390 [D loss: 0.174519] [G loss: 0.075661] [C loss: 0.392263]\n",
            "391 [D loss: 0.173650] [G loss: 0.075373] [C loss: 0.357889]\n",
            "392 [D loss: 0.174279] [G loss: 0.077721] [C loss: 0.371113]\n",
            "393 [D loss: 0.173721] [G loss: 0.076188] [C loss: 0.363464]\n",
            "394 [D loss: 0.173445] [G loss: 0.077426] [C loss: 0.362647]\n",
            "395 [D loss: 0.173422] [G loss: 0.074772] [C loss: 0.340444]\n",
            "396 [D loss: 0.174135] [G loss: 0.073272] [C loss: 0.364521]\n",
            "397 [D loss: 0.173729] [G loss: 0.073837] [C loss: 0.361567]\n",
            "398 [D loss: 0.173193] [G loss: 0.075687] [C loss: 0.354061]\n",
            "399 [D loss: 0.173480] [G loss: 0.076454] [C loss: 0.374491]\n",
            "400 [D loss: 0.173373] [G loss: 0.076703] [C loss: 0.388041]\n",
            "401 [D loss: 0.173107] [G loss: 0.071056] [C loss: 0.329484]\n",
            "402 [D loss: 0.172524] [G loss: 0.073317] [C loss: 0.353641]\n",
            "403 [D loss: 0.172715] [G loss: 0.073908] [C loss: 0.353367]\n",
            "404 [D loss: 0.172390] [G loss: 0.074970] [C loss: 0.348692]\n",
            "405 [D loss: 0.172273] [G loss: 0.075542] [C loss: 0.373615]\n",
            "406 [D loss: 0.172356] [G loss: 0.075094] [C loss: 0.354225]\n",
            "407 [D loss: 0.172390] [G loss: 0.074365] [C loss: 0.415981]\n",
            "408 [D loss: 0.171856] [G loss: 0.073250] [C loss: 0.378660]\n",
            "409 [D loss: 0.171598] [G loss: 0.072120] [C loss: 0.394552]\n",
            "410 [D loss: 0.172110] [G loss: 0.076324] [C loss: 0.369782]\n",
            "411 [D loss: 0.172248] [G loss: 0.072761] [C loss: 0.361046]\n",
            "412 [D loss: 0.171429] [G loss: 0.076485] [C loss: 0.385235]\n",
            "413 [D loss: 0.171640] [G loss: 0.072372] [C loss: 0.371540]\n",
            "414 [D loss: 0.171242] [G loss: 0.073693] [C loss: 0.354438]\n",
            "415 [D loss: 0.170958] [G loss: 0.073126] [C loss: 0.339338]\n",
            "416 [D loss: 0.171161] [G loss: 0.073521] [C loss: 0.352261]\n",
            "417 [D loss: 0.170730] [G loss: 0.072055] [C loss: 0.389275]\n",
            "418 [D loss: 0.170674] [G loss: 0.075330] [C loss: 0.375233]\n",
            "419 [D loss: 0.170040] [G loss: 0.072649] [C loss: 0.372328]\n",
            "420 [D loss: 0.170209] [G loss: 0.075173] [C loss: 0.375111]\n",
            "421 [D loss: 0.169781] [G loss: 0.074770] [C loss: 0.338378]\n",
            "422 [D loss: 0.169982] [G loss: 0.073382] [C loss: 0.355080]\n",
            "423 [D loss: 0.169669] [G loss: 0.072048] [C loss: 0.344400]\n",
            "424 [D loss: 0.169873] [G loss: 0.073524] [C loss: 0.342142]\n",
            "425 [D loss: 0.169575] [G loss: 0.073232] [C loss: 0.352826]\n",
            "426 [D loss: 0.169448] [G loss: 0.076564] [C loss: 0.369116]\n",
            "427 [D loss: 0.170181] [G loss: 0.069759] [C loss: 0.343557]\n",
            "428 [D loss: 0.169177] [G loss: 0.073208] [C loss: 0.342691]\n",
            "429 [D loss: 0.169286] [G loss: 0.069422] [C loss: 0.328733]\n",
            "430 [D loss: 0.168769] [G loss: 0.071724] [C loss: 0.344845]\n",
            "431 [D loss: 0.169231] [G loss: 0.071782] [C loss: 0.346077]\n",
            "432 [D loss: 0.169672] [G loss: 0.077268] [C loss: 0.383921]\n",
            "433 [D loss: 0.168675] [G loss: 0.075222] [C loss: 0.337821]\n",
            "434 [D loss: 0.168507] [G loss: 0.071464] [C loss: 0.332333]\n",
            "435 [D loss: 0.168988] [G loss: 0.076259] [C loss: 0.374424]\n",
            "436 [D loss: 0.168689] [G loss: 0.074725] [C loss: 0.331870]\n",
            "437 [D loss: 0.168864] [G loss: 0.071452] [C loss: 0.371795]\n",
            "438 [D loss: 0.168649] [G loss: 0.072234] [C loss: 0.333239]\n",
            "439 [D loss: 0.168501] [G loss: 0.070865] [C loss: 0.363915]\n",
            "440 [D loss: 0.168363] [G loss: 0.074749] [C loss: 0.348841]\n",
            "441 [D loss: 0.168498] [G loss: 0.068905] [C loss: 0.365379]\n",
            "442 [D loss: 0.168643] [G loss: 0.073672] [C loss: 0.359328]\n",
            "443 [D loss: 0.168581] [G loss: 0.072147] [C loss: 0.384040]\n",
            "444 [D loss: 0.167836] [G loss: 0.069502] [C loss: 0.332500]\n",
            "445 [D loss: 0.167765] [G loss: 0.075266] [C loss: 0.358694]\n",
            "446 [D loss: 0.168221] [G loss: 0.074417] [C loss: 0.338698]\n",
            "447 [D loss: 0.168521] [G loss: 0.074653] [C loss: 0.366618]\n",
            "448 [D loss: 0.168063] [G loss: 0.072887] [C loss: 0.368452]\n",
            "449 [D loss: 0.168335] [G loss: 0.070758] [C loss: 0.362847]\n",
            "450 [D loss: 0.168037] [G loss: 0.074533] [C loss: 0.345579]\n",
            "451 [D loss: 0.167068] [G loss: 0.071745] [C loss: 0.348921]\n",
            "452 [D loss: 0.167106] [G loss: 0.073673] [C loss: 0.393515]\n",
            "453 [D loss: 0.167455] [G loss: 0.072313] [C loss: 0.373396]\n",
            "454 [D loss: 0.166759] [G loss: 0.073416] [C loss: 0.357117]\n",
            "455 [D loss: 0.166030] [G loss: 0.069976] [C loss: 0.313780]\n",
            "456 [D loss: 0.166786] [G loss: 0.073379] [C loss: 0.356903]\n",
            "457 [D loss: 0.166610] [G loss: 0.073602] [C loss: 0.355963]\n",
            "458 [D loss: 0.166046] [G loss: 0.074990] [C loss: 0.367084]\n",
            "459 [D loss: 0.166584] [G loss: 0.073524] [C loss: 0.350392]\n",
            "460 [D loss: 0.166233] [G loss: 0.072613] [C loss: 0.332348]\n",
            "461 [D loss: 0.166137] [G loss: 0.073788] [C loss: 0.308177]\n",
            "462 [D loss: 0.165986] [G loss: 0.074544] [C loss: 0.382607]\n",
            "463 [D loss: 0.166657] [G loss: 0.073164] [C loss: 0.327384]\n",
            "464 [D loss: 0.165837] [G loss: 0.069900] [C loss: 0.334960]\n",
            "465 [D loss: 0.165958] [G loss: 0.075053] [C loss: 0.365072]\n",
            "466 [D loss: 0.165749] [G loss: 0.075291] [C loss: 0.339135]\n",
            "467 [D loss: 0.166482] [G loss: 0.073694] [C loss: 0.362740]\n",
            "468 [D loss: 0.165961] [G loss: 0.073396] [C loss: 0.331908]\n",
            "469 [D loss: 0.165380] [G loss: 0.069732] [C loss: 0.350508]\n",
            "470 [D loss: 0.165695] [G loss: 0.073521] [C loss: 0.332681]\n",
            "471 [D loss: 0.166030] [G loss: 0.072915] [C loss: 0.365747]\n",
            "472 [D loss: 0.165365] [G loss: 0.070607] [C loss: 0.328361]\n",
            "473 [D loss: 0.165747] [G loss: 0.070518] [C loss: 0.321322]\n",
            "474 [D loss: 0.165701] [G loss: 0.072794] [C loss: 0.343529]\n",
            "475 [D loss: 0.165053] [G loss: 0.071131] [C loss: 0.362083]\n",
            "476 [D loss: 0.165817] [G loss: 0.071395] [C loss: 0.325185]\n",
            "477 [D loss: 0.165455] [G loss: 0.072440] [C loss: 0.403336]\n",
            "478 [D loss: 0.165644] [G loss: 0.074782] [C loss: 0.340585]\n",
            "479 [D loss: 0.166170] [G loss: 0.074367] [C loss: 0.337612]\n",
            "480 [D loss: 0.164933] [G loss: 0.072199] [C loss: 0.342712]\n",
            "481 [D loss: 0.165165] [G loss: 0.071154] [C loss: 0.350773]\n",
            "482 [D loss: 0.165483] [G loss: 0.075415] [C loss: 0.338582]\n",
            "483 [D loss: 0.165464] [G loss: 0.075353] [C loss: 0.375730]\n",
            "484 [D loss: 0.165241] [G loss: 0.072369] [C loss: 0.344653]\n",
            "485 [D loss: 0.165224] [G loss: 0.072539] [C loss: 0.333988]\n",
            "486 [D loss: 0.164437] [G loss: 0.072095] [C loss: 0.351642]\n",
            "487 [D loss: 0.165399] [G loss: 0.075725] [C loss: 0.361019]\n",
            "488 [D loss: 0.164495] [G loss: 0.072530] [C loss: 0.334005]\n",
            "489 [D loss: 0.166167] [G loss: 0.072329] [C loss: 0.315515]\n",
            "490 [D loss: 0.165266] [G loss: 0.073789] [C loss: 0.368205]\n",
            "491 [D loss: 0.165327] [G loss: 0.072770] [C loss: 0.333906]\n",
            "492 [D loss: 0.165380] [G loss: 0.070023] [C loss: 0.317185]\n",
            "493 [D loss: 0.165383] [G loss: 0.073094] [C loss: 0.306797]\n",
            "494 [D loss: 0.165492] [G loss: 0.072774] [C loss: 0.354218]\n",
            "495 [D loss: 0.165770] [G loss: 0.073910] [C loss: 0.363828]\n",
            "496 [D loss: 0.166555] [G loss: 0.072083] [C loss: 0.344010]\n",
            "497 [D loss: 0.165077] [G loss: 0.075856] [C loss: 0.343040]\n",
            "498 [D loss: 0.166500] [G loss: 0.073019] [C loss: 0.324429]\n",
            "499 [D loss: 0.165997] [G loss: 0.075498] [C loss: 0.333844]\n",
            "CPU times: user 1h 9min 2s, sys: 55min 36s, total: 2h 4min 39s\n",
            "Wall time: 3min 3s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "respective-diagram",
        "outputId": "a2fd0287-75e5-4020-da86-c2e0ddb357dd"
      },
      "source": [
        "pred_metrics"
      ],
      "id": "respective-diagram",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ClassifierType</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>BalancedAccuracy</th>\n",
              "      <th>F1-score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IB-GAN</td>\n",
              "      <td>0.790462</td>\n",
              "      <td>0.693231</td>\n",
              "      <td>0.672434</td>\n",
              "      <td>0.672434</td>\n",
              "      <td>0.65461</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  ClassifierType  Accuracy  Precision    Recall  BalancedAccuracy  F1-score\n",
              "0         IB-GAN  0.790462   0.693231  0.672434          0.672434   0.65461"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cordless-armor"
      },
      "source": [
        ""
      ],
      "id": "cordless-armor",
      "execution_count": null,
      "outputs": []
    }
  ]
}
